<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="OpenMP loop parallelism #  With a parallel region and identification of individual threads, we can actually parallelise loops &ldquo;by hand&rdquo;.
Suppose we wish to divide a loop approximately equally between all the threads, by assigning consecutive blocks of the loop iterations to consecutive threads.
 Distribution of 16 loop iterations across five threads.
  Notice here that the number of iterations is not evenly divisible by the number of threads, so we&rsquo;ve assigned one extra iteration to thread0."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Loop parallelism"><meta property="og:description" content="OpenMP loop parallelism #  With a parallel region and identification of individual threads, we can actually parallelise loops &ldquo;by hand&rdquo;.
Suppose we wish to divide a loop approximately equally between all the threads, by assigning consecutive blocks of the loop iterations to consecutive threads.
 Distribution of 16 loop iterations across five threads.
  Notice here that the number of iterations is not evenly divisible by the number of threads, so we&rsquo;ve assigned one extra iteration to thread0."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/"><meta property="article:modified_time" content="2022-04-07T18:13:40+01:00"><title>Loop parallelism | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/ class=active>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Loop parallelism</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#worksharing-constructs>Worksharing constructs</a><ul><li><a href=#data-races>When is loop parallelisation possible?</a></li><li><a href=#synchronisation>Synchronisation</a></li><li><a href=#loop-schedules>Doling out iterations</a></li><li><a href=#controlling-execution-in-a-parallel-region>Controlling execution in a parallel region</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=openmp-loop-parallelism>OpenMP loop parallelism
<a class=anchor href=#openmp-loop-parallelism>#</a></h1><p>With a parallel region and identification of individual threads, we
can actually parallelise loops &ldquo;by hand&rdquo;.</p><p>Suppose we wish to divide a loop approximately equally between all the
threads, by assigning consecutive blocks of the loop iterations to
consecutive threads.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/loop-chunking.svg alt="Distribution of 16 loop iterations across five threads."><figcaption><p>Distribution of 16 loop iterations across five threads.</p></figcaption></figure><p>Notice here that the number of iterations is not evenly divisible by
the number of threads, so we&rsquo;ve assigned one extra iteration to
thread0.</p><p>The code that distributes a loop in this way looks something like the below.</p><div class=book-include><div class=book-include-heading><tt>openmp-snippets/hand-loop.c</tt></div><div class=book-include-download><a href=https://teaching.wence.uk/phys52015/code/openmp-snippets/hand-loop.c>Download</a></div><div class=book-include-content><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;omp.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdlib.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span><span style=color:#00a8c8>static</span> <span style=color:#00a8c8>inline</span> <span style=color:#00a8c8>int</span> <span style=color:#75af00>min</span><span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>a</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>b</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>return</span> <span style=color:#111>a</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>b</span> <span style=color:#f92672>?</span> <span style=color:#111>a</span> <span style=color:#111>:</span> <span style=color:#111>b</span><span style=color:#111>;</span>
<span style=color:#111>}</span>

<span style=color:#00a8c8>int</span> <span style=color:#75af00>main</span><span style=color:#111>(</span><span style=color:#00a8c8>void</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>const</span> <span style=color:#00a8c8>int</span> <span style=color:#111>N</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>N</span><span style=color:#111>];</span>
  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#75715e>/* Sentinel for unhandled value */</span>
    <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>;</span>
  <span style=color:#111>}</span>
<span style=color:#75715e>#pragma omp parallel default(none) shared(N, a)
</span><span style=color:#75715e></span>  <span style=color:#111>{</span>
    <span style=color:#00a8c8>int</span> <span style=color:#111>tid</span> <span style=color:#f92672>=</span> <span style=color:#111>omp_get_thread_num</span><span style=color:#111>();</span>
    <span style=color:#00a8c8>int</span> <span style=color:#111>nthread</span> <span style=color:#f92672>=</span> <span style=color:#111>omp_get_num_threads</span><span style=color:#111>();</span>

    <span style=color:#00a8c8>int</span> <span style=color:#111>chunk</span> <span style=color:#f92672>=</span> <span style=color:#111>N</span> <span style=color:#f92672>/</span> <span style=color:#111>nthread</span> <span style=color:#f92672>+</span> <span style=color:#111>((</span><span style=color:#111>N</span> <span style=color:#f92672>%</span> <span style=color:#111>nthread</span><span style=color:#111>)</span> <span style=color:#f92672>&gt;</span> <span style=color:#111>tid</span><span style=color:#111>);</span>
    <span style=color:#00a8c8>int</span> <span style=color:#111>start</span> <span style=color:#f92672>=</span> <span style=color:#111>tid</span> <span style=color:#f92672>*</span> <span style=color:#111>(</span><span style=color:#111>N</span> <span style=color:#f92672>/</span> <span style=color:#111>nthread</span><span style=color:#111>)</span> <span style=color:#f92672>+</span> <span style=color:#111>min</span><span style=color:#111>(</span><span style=color:#111>tid</span><span style=color:#111>,</span> <span style=color:#111>N</span> <span style=color:#f92672>%</span> <span style=color:#111>nthread</span><span style=color:#111>);</span>
    <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#111>start</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>start</span> <span style=color:#f92672>+</span> <span style=color:#111>chunk</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
      <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>tid</span><span style=color:#111>;</span>
    <span style=color:#111>}</span>
  <span style=color:#111>}</span>
  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;a[%2d] = %2d</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>i</span><span style=color:#111>,</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]);</span>
  <span style=color:#111>}</span>
  <span style=color:#00a8c8>return</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
</code></pre></div></div></div><blockquote class=exercise><h3>Exercise</h3>Run this code for a number of threads between 1 and 8. Convince
yourself that it correctly allocates all the loop iterations!</blockquote><p>UGH!</p><p>Fortunately, there is a better way<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><h2 id=worksharing-constructs>Worksharing constructs
<a class=anchor href=#worksharing-constructs>#</a></h2><p>Suppose we are in a parallel region, to distribute the work in a loop
amongst the thread team, we use the <a href=https://hpc.llnl.gov/tuts/openMP/#DO><code>#pragma omp for</code></a> directive.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>void</span> <span style=color:#75af00>foo</span><span style=color:#111>(</span><span style=color:#75715e>/* Some arguments */</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
<span style=color:#75715e>#pragma omp parallel default(none) ...
</span><span style=color:#75715e></span>  <span style=color:#111>{</span>
    <span style=color:#111>...;</span>

    <span style=color:#75715e>/* Loop to parallelise */</span>
    <span style=color:#75715e>#pragma omp for
</span><span style=color:#75715e></span>    <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
      <span style=color:#111>...;</span>
    <span style=color:#111>}</span>
  <span style=color:#111>}</span>
<span style=color:#111>}</span>

</code></pre></div><p>This directive does the job of dividing the loop iterations between
the currently active threads. Without the directive, all threads in
the team would execute all iterations.</p><details><summary>Shorthand</summary><div class=markdown-inner><p>This pattern of parallel region + loop is so common that there is a
separate directive that merges the two</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp parallel for ...
</span><span style=color:#75715e></span><span style=color:#00a8c8>for</span> <span style=color:#111>(...)</span>
   <span style=color:#111>;</span>
</code></pre></div><p>Is equivalent to</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp parallel ...
</span><span style=color:#75715e></span><span style=color:#111>{</span>
  <span style=color:#75715e>#pragma omp for
</span><span style=color:#75715e></span>  <span style=color:#00a8c8>for</span> <span style=color:#111>(...)</span>
     <span style=color:#111>;</span>
<span style=color:#111>}</span>
</code></pre></div></div></details><h3 id=data-races>When is loop parallelisation possible?
<a class=anchor href=#data-races>#</a></h3><p>For loop parallelism to be possible, these loops must obey a number of
constraints, both in the form of the loop construct, and also in what
the loop body contains. Formally, we need the loop to be of the form.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#111>var</span> <span style=color:#f92672>=</span> <span style=color:#111>init</span><span style=color:#111>;</span> <span style=color:#111>var</span> <span style=color:#111>logical_op</span> <span style=color:#111>end</span><span style=color:#111>;</span> <span style=color:#111>incr_expr</span><span style=color:#111>)</span>
  <span style=color:#111>...</span>
</code></pre></div><p>Where the <code>logical_op</code> is one of <code>&lt;</code>, <code>&lt;=</code>, <code>></code>, or <code>>=</code> and
<code>incr_expr</code> is an increment expression like <code>var = var + incr</code> (or
similar). We are also not allowed to modify <code>var</code> in the loop body.</p><p>The major constraint on the loop <em>body</em> is that we cannot have
<em>dependencies</em> between loop iterations. Conceptually, a parallel loop
runs multiple iterations of the loop body statements <em>in parallel</em>.
For this to be valid, we must be able to execute the statements in any
order we like.</p><p>For example, this loop cannot be straightforwardly parallelised</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#111>size_t</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span>
  <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>]</span> <span style=color:#f92672>+</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
</code></pre></div><p>Since the $i$th iteration depends on the result of the
$i-1$th iteration.</p><blockquote class=exercise><h3>Exercise</h3>Write out the unrolled loop (unrolling by 4) and convince yourself
that you can&rsquo;t reorder the statements in the loop body while
maintaining the same semantics.</blockquote><p>This particular loop exhibits a <em>read-after-write</em> dependency, some
times called a <em>flow</em> dependency. These are &ldquo;true&rdquo; dependencies and
really inhibit parallelisation. There are also a number of other
types, <em>write-after-read</em> (also called anti-dependencies),
<em>write-after-write</em> (also called output dependencies), and
<em>read-after-read</em> (not really dependencies). As usual,
<a href=https://en.wikipedia.org/wiki/Data_dependency>wikipedia</a> has a good
summary. For our purposes, <em>read-after-write</em> are the difficult ones
to handle. The others can usually be refactored by introducing some
temporary variables (as discussed in the linked wikipedia article).
Typically, they then might reveal a read-after-write dependency.</p><blockquote class="book-hint warning">The compiler will complain if your looping construct has the wrong
form, however, it will <em>not</em> complain if your loop body is not
suitable for parallelisation (e.g. it has data dependencies).</blockquote><p><blockquote class=exercise><h3>Exercise</h3><p>Try compiling and running the bad loop in the code below</p><p>Do you always get the same results on different numbers of threads?
What is wrong with the parallel loop?</p><details><summary>Solution</summary><div class=markdown-inner><p>We don&rsquo;t see the same values printed independent of the number of
threads, indicating that we did something wrong.</p><p>The reason is that the loop body modifies the iteration variable <code>i</code>.
This is explicitly not allowed by the OpenMP standard (although the
compiler does not complain), because if we modify the iteration
variable, the compiler cannot figure out how to hand out the loops
between threads.</p></div></details></blockquote><div class=book-include><div class=book-include-heading><tt>openmp-snippets/bad-loop.c</tt></div><div class=book-include-download><a href=https://teaching.wence.uk/phys52015/code/openmp-snippets/bad-loop.c>Download</a></div><div class=book-include-content><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;omp.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>main</span><span style=color:#111>(</span><span style=color:#00a8c8>void</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>N</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>N</span><span style=color:#111>];</span>
  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#75715e>/* Sentinel for unhandled value */</span>
    <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>;</span>
  <span style=color:#111>}</span>
<span style=color:#75715e>#pragma omp parallel for default(none) shared(a, N)
</span><span style=color:#75715e></span>  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>i</span><span style=color:#111>;</span>
    <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>;</span>
  <span style=color:#111>}</span>

  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;a[%2d] = %2d</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>i</span><span style=color:#111>,</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]);</span>
  <span style=color:#111>}</span>
  <span style=color:#00a8c8>return</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
</code></pre></div></div></div></p><h3 id=synchronisation>Synchronisation
<a class=anchor href=#synchronisation>#</a></h3><p>There is no synchronisation at the start of a <code>for</code> work-sharing
construct. By default, however, there <em>is</em> a synchronisation at the
end.</p><p>To remove the synchronisation at the end, we can use an additional
<code>nowait</code> clause.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp parallel
</span><span style=color:#75715e></span><span style=color:#111>{</span>
<span style=color:#75715e>#pragma omp for
</span><span style=color:#75715e></span>  <span style=color:#00a8c8>for</span> <span style=color:#111>(...)</span> <span style=color:#111>{</span>
    <span style=color:#111>...;</span>
  <span style=color:#111>}</span> <span style=color:#75715e>/* All threads synchronise here */</span>

<span style=color:#75715e>#pragma omp for nowait
</span><span style=color:#75715e></span>  <span style=color:#00a8c8>for</span> <span style=color:#111>(...)</span> <span style=color:#111>{</span>
    <span style=color:#111>...;</span>
  <span style=color:#111>}</span> <span style=color:#75715e>/* No synchronisation in this case */</span>
<span style=color:#111>}</span>
</code></pre></div><h3 id=loop-schedules>Doling out iterations
<a class=anchor href=#loop-schedules>#</a></h3><p>The <code>for</code> directive takes a number of additional clauses that allow us
to control <em>how</em> the loop iterations are divided between threads.</p><p>We do this with</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp for schedule(KIND[, CHUNKSIZE])
</span></code></pre></div><p>Where <code>KIND</code> is one of</p><ol><li><code>static</code></li><li><code>dynamic</code></li><li><code>guided</code></li><li><code>auto</code></li><li><code>runtime</code></li></ol><p>and <code>CHUNKSIZE</code> is an expression returning a positive integer.</p><p>If you don&rsquo;t specify anything, this is equivalent to</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp parallel for schedule(static)
</span></code></pre></div><p>In most cases, the static schedule is the most useful. We&rsquo;ll list the
properties of all of the choices below.</p><h4 id=static-schedules><code>static</code> schedules
<a class=anchor href=#static-schedules>#</a></h4><p><code>schedule(static)</code> is a block schedule. We divide the total iterations
into (approximately) equal chunks, one for each thread in the team,
and assign the chunks in order to threads.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/static-schedule.svg alt="Iteration allocation for a static schedule with default chunksize."><figcaption><p>Iteration allocation for a static schedule with default chunksize.</p></figcaption></figure><details><summary>What if there are leftover iterations?</summary><div class=markdown-inner>If the number of iterations is not evenly divisible by the number of
threads, the OpenMP runtime is supposed to allocate the extra
iterations &ldquo;approximately evenly&rdquo;, but it is not specified how this is
to be done. If you need a <em>specific</em> distribution you need to write
code by hand.</div></details><p>If specifying a chunk size (for example <code>schedule(static, 3)</code>) then
we have a block cyclic schedule. Iterations are doled out to threads
in order <code>chunksize</code> iterations at a time.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/static-schedule-chunksize.svg alt="Iteration allocation for a static schedule with chunksize of three."><figcaption><p>Iteration allocation for a static schedule with chunksize of three.</p></figcaption></figure><p><code>static</code> schedules deterministically allocate loop iterations to
threads. Two loops of the same length with the same static schedule
will dole out iterations in the same way.</p><h4 id=dynamic-schedules><code>dynamic</code> schedules
<a class=anchor href=#dynamic-schedules>#</a></h4><p><code>schedule(dynamic)</code> is a block cyclic schedule with a block size of
one. Iterations of the loop are doled out on a first-come-first-served
basis. That is as a thread becomes available, it is given the next
iteration.</p><p><code>schedule(dynamic, chunksize)</code> is the same, only now the block size is
specified by <code>chunksize</code>.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/dynamic-schedule-chunksize.svg alt="Example iteration allocation for a dynamic schedule with chunksize of three."><figcaption><p>Example iteration allocation for a dynamic schedule with chunksize of three.</p></figcaption></figure><p><code>dynamic</code> schedules <strong>do not</strong> provide deterministic allocation of
loop iterations to threads loop iterations to
threads. Two loops of the same length with the same dynamic schedule
will <strong>not necessarily</strong> dole out iterations in the same way.</p><h4 id=guided-schedules><code>guided</code> schedules
<a class=anchor href=#guided-schedules>#</a></h4><p><code>schedule(guided)</code> is a block cyclic schedule where the blocks start
out large and get exponentially smaller, until they reach a minimum
size of one iteration. The size of a block is proportional to the
number of <em>remaining</em> iterations divided by the number of threads.</p><p><code>schedule(guided, chunksize)</code> is the same, only now the minimum size
of the chunks is specified by <code>chunksize</code>.</p><p>Chunks are doled out to threads on a first-come-first-served basis
(like the <code>dynamic</code> schedule).</p><h4 id=auto-and-runtime-schedules><code>auto</code> and <code>runtime</code> schedules
<a class=anchor href=#auto-and-runtime-schedules>#</a></h4><p>The <code>auto</code> schedule leaves it up to the OpenMP runtime to decide how
to dole out iterations. It may implement some auto-tuning framework
where it measures performance of the loop with a given schedule and
then adjusts that in some clever way. Probably, the implementation
does not do anything clever.</p><p>The <code>runtime</code> schedule allows you to specify the schedule to use at
runtime (rather than compile time) by setting the <code>OMP_SCHEDULE</code>
environment variable to a valid schedule string. For example</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp for schedule(runtime)
</span><span style=color:#75715e></span><span style=color:#111>...</span>
</code></pre></div><pre><code>$ gcc -fopenmp -o code code.c
$ export OMP_SCHEDULE=&quot;static, 5&quot;
$ OMP_NUM_THREADS=4 ./code # uses &quot;static, 5&quot; as the schedule
</code></pre><p>This is kind of pointless except for some small experiments because
there is only one value of the <code>OMP_SCHEDULE</code> variable, so <em>all</em>
<code>schedule(runtime)</code> loops must use the same value.</p><blockquote class=exercise><h3>Exercise</h3>You now know enough (probably more than enough) to attempt the OpenMP
<a href=https://teaching.wence.uk/phys52015/exercises/openmp-loop/>loop exercise</a>.</blockquote><h3 id=controlling-execution-in-a-parallel-region>Controlling execution in a parallel region
<a class=anchor href=#controlling-execution-in-a-parallel-region>#</a></h3><p>Sometimes, we might want to serialise some part of the code in a
parallel region. OpenMP offers us two ways of doing this. If we don&rsquo;t
care about which thread executes some code, we can use <a href=https://hpc.llnl.gov/tuts/openMP/#SINGLE><code>#pragma omp single</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp parallel
</span><span style=color:#75715e></span><span style=color:#111>{</span>
  <span style=color:#111>...;</span> <span style=color:#75715e>/* Some stuff in parallel */</span>
<span style=color:#75715e>#pragma omp single
</span><span style=color:#75715e></span>  <span style=color:#111>{</span>
    <span style=color:#111>...;</span> <span style=color:#75715e>/* Only one thread should do this */</span>
  <span style=color:#111>}</span> <span style=color:#75715e>/* All threads synchronise here unless nowait is specified. */</span>
<span style=color:#111>}</span>
</code></pre></div><p>For example, we could use this construct to read in an input file
after some parallel setup on a single thread.</p><p>Alternatively, if we want thread0 to execute something, we can use
<a href=https://hpc.llnl.gov/tuts/openMP/#MASTER><code>#pragma omp master</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp parallel
</span><span style=color:#75715e></span><span style=color:#111>{</span>
  <span style=color:#111>...;</span> <span style=color:#75715e>/* Some stuff in parallel */</span>
<span style=color:#75715e>#pragma omp master
</span><span style=color:#75715e></span>  <span style=color:#111>{</span>
    <span style=color:#111>...;</span> <span style=color:#75715e>/* Only thread0 does this */</span>
  <span style=color:#111>}</span> <span style=color:#75715e>/* No synchronisation */</span>
<span style=color:#111>}</span>

<span style=color:#75715e>/* Kind of pointless, because it&#39;s equivalent to */</span>
<span style=color:#75715e>#pragma omp parallel
</span><span style=color:#75715e></span><span style=color:#111>{</span>
  <span style=color:#111>...;</span>
  <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>omp_get_thread_num</span><span style=color:#111>()</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>...;</span>
  <span style=color:#111>}</span>
<span style=color:#111>}</span>
</code></pre></div><h2 id=summary>Summary
<a class=anchor href=#summary>#</a></h2><p>OpenMP provides a number of constructs for distributing work (in the
form of loop iterations) between threads in a team. This is achieved
with the <code>#pragma omp for</code> directive (inside an existing parallel
region).</p><p>Completely general for-loops are not allowed, because the runtime must
be able to determine the number of iterations (and how to get from one
iteration to the next) without executing the loop body.</p><p>There are a few ways of controlling how the iterations of the loop are
allocated to threads. These are called <code>schedule</code>s. Generally a
<code>static</code> schedule is best.</p><p><a href=https://teaching.wence.uk/phys52015/notes/openmp/collectives/>Next</a> we&rsquo;ll look at how threads can
communicate with one another, so that we can do slightly more
complicated things than just share loop iterations.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Counterpoint, if you write large libraries using OpenMP you will
probably end up managing the loop parallelism and distribution by
hand in this way anyway. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/notes/openmp/loop-parallelism.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#worksharing-constructs>Worksharing constructs</a><ul><li><a href=#data-races>When is loop parallelisation possible?</a></li><li><a href=#synchronisation>Synchronisation</a></li><li><a href=#loop-schedules>Doling out iterations</a></li><li><a href=#controlling-execution-in-a-parallel-region>Controlling execution in a parallel region</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></aside></main></body></html>