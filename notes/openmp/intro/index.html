<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="What is OpenMP #  OpenMP is a standardised API for programming shared memory computers (and more recently GPUs) using threading as the programming paradigm. It supports both data-parallel shared memory programming (typically for parallelising loops) and task parallelism. We&rsquo;ll see some examples later.
In recent years, it has also gained support for some vector-based parallelism.
Using OpenMP #  OpenMP is implemented as a set of extensions for C, C++, and Fortran."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="What is OpenMP?"><meta property="og:description" content="What is OpenMP #  OpenMP is a standardised API for programming shared memory computers (and more recently GPUs) using threading as the programming paradigm. It supports both data-parallel shared memory programming (typically for parallelising loops) and task parallelism. We&rsquo;ll see some examples later.
In recent years, it has also gained support for some vector-based parallelism.
Using OpenMP #  OpenMP is implemented as a set of extensions for C, C++, and Fortran."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/notes/openmp/intro/"><meta property="article:modified_time" content="2022-04-07T18:13:40+01:00"><title>What is OpenMP? | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/ class=active>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>What is OpenMP?</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#using-openmp>Using OpenMP</a><ul><li><a href=#directives>Directives</a></li><li><a href=#library-routines>Library routines</a></li><li><a href=#environment-variables>Environment variables</a></li></ul></li><li><a href=#parallel-constructs>Parallel constructs.</a></li><li><a href=#data-scoping-shared-and-private>Data scoping: shared and private</a><ul><li><a href=#default-data-scoping>Default data scoping</a></li><li><a href=#values-on-entry>Values on entry</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=what-is-openmp>What is OpenMP
<a class=anchor href=#what-is-openmp>#</a></h1><p><a href=https://www.openmp.org>OpenMP</a> is a
<a href=https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5.0.pdf>standardised</a>
API for programming shared memory computers (and more recently
<a href=https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units>GPUs</a>)
using threading as the programming paradigm. It supports both
data-parallel shared memory programming (typically for parallelising
loops) and task parallelism. We&rsquo;ll see some examples later.</p><p>In recent years, it has also gained support for some vector-based
parallelism.</p><h2 id=using-openmp>Using OpenMP
<a class=anchor href=#using-openmp>#</a></h2><p>OpenMP is implemented as a set of extensions for C, C++, and
Fortran. These extensions come in three parts</p><ol><li><code>#pragma</code>-based <em>directives</em>;</li><li>runtime library routines;</li><li>environment variables for controlling runtime behaviour.</li></ol><p>OpenMP is an <em>explicit</em> model of parallel programming. It is your job,
as the programmer, to decide where and how to employ parallelism.</p><h3 id=directives>Directives
<a class=anchor href=#directives>#</a></h3><p>We already saw some directives when discussing vectorisation. In that
case, we saw compiler-specific directives. In the case of OpenMP,
since it is a standard, the meaning of the directive is the same
independent of the compiler choice<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>All OpenMP directives start with <code>#pragma omp </code>. They are therefore
ignored if the code is compiled without adding some special compiler
flags.</p><div class=book-tabs><input type=radio class=toggle name="tabs-Compiler flags" id="tabs-Compiler flags-0" checked>
<label for="tabs-Compiler flags-0">Intel</label><div class="book-tabs-content markdown-inner"><code>-qopenmp</code></div><input type=radio class=toggle name="tabs-Compiler flags" id="tabs-Compiler flags-1">
<label for="tabs-Compiler flags-1">GCC/Clang</label><div class="book-tabs-content markdown-inner"><code>-fopenmp</code></div></div><p>We can <a href=https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/>parallelise a loop</a> like so</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp parallel for
</span><span style=color:#75715e></span><span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span>
  <span style=color:#111>...</span>
</code></pre></div><h3 id=library-routines>Library routines
<a class=anchor href=#library-routines>#</a></h3><p>In addition to the directives, which are used to enable parallelism
through annotation, the OpenMP standard also provides for a number of
runtime API calls. These allow threads to inspect the state of the
program (for example to ask which thread they are). To do this, we
must include a C header file <code>omp.h</code>. All OpenMP API calls are
prefixed with <code>omp_</code>.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;omp.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#111>...</span>
<span style=color:#75715e>#pragma omp parallel
</span><span style=color:#75715e></span><span style=color:#111>{</span>
  <span style=color:#75715e>/* Which thread am I? */</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>threadid</span> <span style=color:#f92672>=</span> <span style=color:#111>omp_get_thread_num</span><span style=color:#111>();</span>
  <span style=color:#75715e>/* How many threads are currently executing */</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>nthread</span> <span style=color:#f92672>=</span> <span style=color:#111>omp_get_num_threads</span><span style=color:#111>();</span>
<span style=color:#111>}</span>
</code></pre></div><p>Unlike the pragmas, these runtime calls are only available when
compiling with OpenMP enabled<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><h3 id=environment-variables>Environment variables
<a class=anchor href=#environment-variables>#</a></h3><p>The primary variable is <code>OMP_NUM_THREADS</code> which specifies the number of
threads that should be available to the program when running.</p><p>This is the number of threads created when entering a
parallel region.</p><div class=book-include><div class=book-include-heading><tt>hello/openmp.c</tt></div><div class=book-include-download><a href=https://teaching.wence.uk/phys52015/code/hello/openmp.c>Download</a></div><div class=book-include-content><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;omp.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>main</span><span style=color:#111>(</span><span style=color:#00a8c8>void</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>nthread</span> <span style=color:#f92672>=</span> <span style=color:#111>omp_get_max_threads</span><span style=color:#111>();</span>
  <span style=color:#00a8c8>int</span> <span style=color:#00a8c8>thread</span><span style=color:#111>;</span>
<span style=color:#75715e>#pragma omp parallel private(thread) shared(nthread)
</span><span style=color:#75715e></span>  <span style=color:#111>{</span>
    <span style=color:#00a8c8>thread</span> <span style=color:#f92672>=</span> <span style=color:#111>omp_get_thread_num</span><span style=color:#111>();</span>
    <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;Hello, World! I am thread %d of %d</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#00a8c8>thread</span><span style=color:#111>,</span> <span style=color:#111>nthread</span><span style=color:#111>);</span>
  <span style=color:#111>}</span>
  <span style=color:#00a8c8>return</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
</code></pre></div></div></div><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>$ icc -qopenmp -o hello openmp.c
$ <span style=color:#111>export</span> <span style=color:#111>OMP_NUM_THREADS</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
$ ./hello
Hello, World! I am thread <span style=color:#ae81ff>0</span> of <span style=color:#ae81ff>1</span>
$ <span style=color:#111>export</span> <span style=color:#111>OMP_NUM_THREADS</span><span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>
$ ./hello
Hello, World! I am thread <span style=color:#ae81ff>5</span> of <span style=color:#ae81ff>8</span>
Hello, World! I am thread <span style=color:#ae81ff>3</span> of <span style=color:#ae81ff>8</span>
Hello, World! I am thread <span style=color:#ae81ff>6</span> of <span style=color:#ae81ff>8</span>
Hello, World! I am thread <span style=color:#ae81ff>1</span> of <span style=color:#ae81ff>8</span>
Hello, World! I am thread <span style=color:#ae81ff>0</span> of <span style=color:#ae81ff>8</span>
Hello, World! I am thread <span style=color:#ae81ff>2</span> of <span style=color:#ae81ff>8</span>
Hello, World! I am thread <span style=color:#ae81ff>4</span> of <span style=color:#ae81ff>8</span>
Hello, World! I am thread <span style=color:#ae81ff>7</span> of <span style=color:#ae81ff>8</span>
</code></pre></div><blockquote class=exercise><h3>Exercise</h3><p>Try this yourself. For instructions you can also see the <a href=https://teaching.wence.uk/phys52015/exercises/hello/>Hello
World</a> exercise.</p><p>What do you observe if you never set <code>OMP_NUM_THREADS</code>?</p><details><summary>Hint</summary><div class=markdown-inner>Use <code>unset OMP_NUM_THREADS</code> to ensure there is no existing value of
the variable.</div></details></blockquote><blockquote class="book-hint warning"><p>The number of threads used by an OpenMP program if you do not set
<code>OMP_NUM_THREADS</code> explicitly is up to the particular implementation.</p><p>I therefore recommend that you <em>always</em> set <code>OMP_NUM_THREADS</code>
explicitly.</p></blockquote><h2 id=parallel-constructs>Parallel constructs.
<a class=anchor href=#parallel-constructs>#</a></h2><p>The basic parallel construct is a <em>parallel region</em>. This is
introduced with <a href=https://hpc.llnl.gov/tuts/openMP/#ParallelRegion><code>#pragma omp parallel</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>/* Some serial code on thread0 */</span>
<span style=color:#75715e>#pragma omp parallel </span><span style=color:#75715e>/* Extra threads created */</span><span style=color:#75715e>
</span><span style=color:#75715e></span><span style=color:#111>{</span>
  <span style=color:#75715e>/* This code is executed in parallel by all threads. */</span>
  <span style=color:#111>...;</span>
<span style=color:#111>}</span> <span style=color:#75715e>/* Synchronisation waiting for all threads */</span>
<span style=color:#75715e>/* More serial code on thread0 */</span>
</code></pre></div><p>The program begins by executing using a single thread (commonly termed
the master thread, though we&rsquo;ll use the phrase &ldquo;thread0&rdquo;). When a
parallel region is encounter, a number of additional threads (called a
&ldquo;team&rdquo;) are created. These threads all execute the code inside the
parallel region, there is then a synchronisation point at the end of
the region, after which thread0 continues execution of the next
(serial) statements.</p><p>This is a <a href=https://en.wikipedia.org/wiki/Fork%e2%80%93join_model>fork/join</a>
programming model, and is therefore best analysed using the <a href=https://en.wikipedia.org/wiki/Bulk_synchronous_parallel>bulk
synchronous parallel
(BSP)</a>
abstract model.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/fork-join-schematic.svg alt="Schematic of fork-join parallelism. Single-threaded execution above (with two regions of parallel tasks). Parallel execution with fork-join points marked below."><figcaption><p>Schematic of fork-join parallelism. Single-threaded execution above (with two regions of parallel tasks). Parallel execution with fork-join points marked below.</p></figcaption></figure><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>void</span> <span style=color:#75af00>foo</span><span style=color:#111>(...)</span>
<span style=color:#111>{</span>
  <span style=color:#111>...;</span>
  <span style=color:#75715e>#pragma omp parallel
</span><span style=color:#75715e></span>  <span style=color:#111>{</span>
    <span style=color:#111>parallel_code</span><span style=color:#111>;</span>
  <span style=color:#111>}</span> <span style=color:#75715e>/* Synchronisation here */</span>
  <span style=color:#111>serial_code</span><span style=color:#111>;</span>
  <span style=color:#75715e>#pragma omp parallel
</span><span style=color:#75715e></span>  <span style=color:#111>{</span>
    <span style=color:#111>more_parallel_code</span><span style=color:#111>;</span>
  <span style=color:#111>}</span> <span style=color:#75715e>/* Synchronisation here */</span>
<span style=color:#111>}</span>
</code></pre></div><h2 id=data-scoping-shared-and-private>Data scoping: shared and private
<a class=anchor href=#data-scoping-shared-and-private>#</a></h2><p>Inside a parallel region, any variables that are in scope can either
be <em>shared</em> or <em>private</em>. All threads see the same (single) copy of
any shared variables and can read and write to them. Private variables
are individual to each thread: there is one copy of the variable per
thread. These variables are not visible to any other threads in the
region, and can thus only be read and written by their own thread.</p><p>We declare the visibility of variables by providing extra clauses to
the <code>parallel</code> directive</p><div class=book-include><div class=book-include-heading><tt>openmp-snippets/parallel-region.c</tt></div><div class=book-include-download><a href=https://teaching.wence.uk/phys52015/code/openmp-snippets/parallel-region.c>Download</a></div><div class=book-include-content><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;omp.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;unistd.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span><span style=color:#00a8c8>void</span> <span style=color:#75af00>foo</span><span style=color:#111>(</span><span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#111>a</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>N</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>i</span><span style=color:#111>;</span>
<span style=color:#75715e>#pragma omp parallel shared(a, N) private(i)
</span><span style=color:#75715e></span>  <span style=color:#111>{</span>
    <span style=color:#00a8c8>int</span> <span style=color:#111>j</span><span style=color:#111>;</span> <span style=color:#75715e>/* This variable is local to the block (and hence private) */</span>

    <span style=color:#75715e>/* Each thread has its own copy of i. */</span>
    <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#111>omp_get_thread_num</span><span style=color:#111>();</span>
    <span style=color:#111>j</span> <span style=color:#f92672>=</span> <span style=color:#111>i</span><span style=color:#111>;</span>
    <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>i</span><span style=color:#f92672>%</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
      <span style=color:#75715e>/* Fake &#34;delay&#34; of some threads. */</span>
      <span style=color:#111>usleep</span><span style=color:#111>(</span><span style=color:#ae81ff>10</span><span style=color:#111>);</span>
    <span style=color:#111>}</span>
    <span style=color:#75715e>/* All threads write to the same a and read the same N. */</span>
    <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>j</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>)</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>j</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>i</span><span style=color:#111>;</span>
  <span style=color:#111>}</span>
<span style=color:#111>}</span>

<span style=color:#00a8c8>int</span> <span style=color:#75af00>main</span><span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>argc</span><span style=color:#111>,</span> <span style=color:#00a8c8>char</span> <span style=color:#f92672>**</span><span style=color:#111>argv</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>N</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>double</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#ae81ff>32</span><span style=color:#111>];</span>

  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>;</span>
  <span style=color:#111>}</span>

  <span style=color:#111>foo</span><span style=color:#111>(</span><span style=color:#111>a</span><span style=color:#111>,</span> <span style=color:#111>N</span><span style=color:#111>);</span>

  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>&gt;</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#111>{</span>
      <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;a[%2d] = %g</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>i</span><span style=color:#111>,</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]);</span>
    <span style=color:#111>}</span>
  <span style=color:#111>}</span>
<span style=color:#111>}</span>
</code></pre></div></div></div><blockquote class=exercise><h3>Exercise</h3><p>Compile and run this code with a number of different threads.</p><p>Convince yourself you understand what is going on.</p></blockquote><h3 id=default-data-scoping>Default data scoping
<a class=anchor href=#default-data-scoping>#</a></h3><p>If we have a lot of variables that we want to access in the parallel
region, it is slightly tedious to explicitly list them.</p><p>Consider, for example, the following snippet</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#111>a</span><span style=color:#111>;</span>
<span style=color:#00a8c8>int</span> <span style=color:#111>N</span><span style=color:#111>;</span>
<span style=color:#00a8c8>int</span> <span style=color:#111>i</span><span style=color:#111>,</span> <span style=color:#111>j</span><span style=color:#111>;</span>
<span style=color:#75715e>#pragma omp parallel shared(a) private(i)
</span><span style=color:#75715e></span><span style=color:#111>{</span>
  <span style=color:#75715e>/* a is shared, i is private (explicitly) */</span>
  <span style=color:#75715e>/* N and j are also in scope, so they must be either shared or private. */</span>
<span style=color:#111>}</span>
</code></pre></div><p>The OpenMP design board took the decision that there should be a
default scoping rule for variables.</p><p>This was a <strong>terrible</strong> decision.</p><p>Neither choice is particularly satisfying, but the default, in the
absence of specification, is that variables are <em>shared</em>.</p><p>This is bad, because if we write seemingly innocuous code, it can
behave badly when run in parallel.</p><p>Try compiling and running this code, very similar to the
parallel-region example we saw above</p><div class=book-include><div class=book-include-heading><tt>openmp-snippets/parallel-region-bad.c</tt></div><div class=book-include-download><a href=https://teaching.wence.uk/phys52015/code/openmp-snippets/parallel-region-bad.c>Download</a></div><div class=book-include-content><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;omp.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;unistd.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span><span style=color:#00a8c8>void</span> <span style=color:#75af00>foo</span><span style=color:#111>(</span><span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#111>a</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>N</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>i</span><span style=color:#111>;</span>
<span style=color:#75715e>#pragma omp parallel shared(a, N)
</span><span style=color:#75715e></span>  <span style=color:#111>{</span>
    <span style=color:#00a8c8>int</span> <span style=color:#111>j</span><span style=color:#111>;</span> <span style=color:#75715e>/* This variable is local to the block (and hence private) */</span>
    <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#111>omp_get_thread_num</span><span style=color:#111>();</span>
    <span style=color:#111>j</span> <span style=color:#f92672>=</span> <span style=color:#111>i</span><span style=color:#111>;</span>
    <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>i</span><span style=color:#f92672>%</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
      <span style=color:#75715e>/* Fake &#34;delay&#34; of some threads. */</span>
      <span style=color:#111>usleep</span><span style=color:#111>(</span><span style=color:#ae81ff>10</span><span style=color:#111>);</span>
    <span style=color:#111>}</span>
    <span style=color:#75715e>/* All threads write to the same a and read the same N. */</span>
    <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>j</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>)</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>j</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>i</span><span style=color:#111>;</span>
  <span style=color:#111>}</span>
<span style=color:#111>}</span>

<span style=color:#00a8c8>int</span> <span style=color:#75af00>main</span><span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>argc</span><span style=color:#111>,</span> <span style=color:#00a8c8>char</span> <span style=color:#f92672>**</span><span style=color:#111>argv</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>N</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>double</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#ae81ff>32</span><span style=color:#111>];</span>

  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>;</span>
  <span style=color:#111>}</span>

  <span style=color:#111>foo</span><span style=color:#111>(</span><span style=color:#111>a</span><span style=color:#111>,</span> <span style=color:#111>N</span><span style=color:#111>);</span>

  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>&gt;</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#111>{</span>
      <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;a[%2d] = %g</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>i</span><span style=color:#111>,</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]);</span>
    <span style=color:#111>}</span>
  <span style=color:#111>}</span>
<span style=color:#111>}</span>
</code></pre></div></div></div><blockquote class=exercise><h3>Exercise</h3><p>Compile and run the code using 8 threads a number of times. What do
you observe about the output?</p><p>Can you explain what is happening?</p><details><summary>Hint</summary><div class=markdown-inner>Think about the potential <a href=https://teaching.wence.uk/phys52015/notes/openmp/#sync-data-race>data races</a>.</div></details><details><summary>Solution</summary><div class=markdown-inner><p>If I run this code on eight processes, I see:</p><pre><code>$ OMP_NUM_THREADS=8 ./bad-region
a[ 0] = 7
a[ 1] = 1
a[ 2] = 7
a[ 3] = 3
a[ 4] = 7
a[ 5] = 5
a[ 6] = 6
a[ 7] = 7
</code></pre><p>Although sometimes the values change.</p><p>What is happening is that the <code>i</code> variable which records the thread
number in the parallel region is <em>shared</em> (rather than being private).
So by the time we get to the point where we write it into the output
array, it is probably overwritten by a value from another thread.</p></div></details></blockquote><p>Fortunately, there is a way to negate this bad design decision. The
<code>default</code> clause. If we write</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp parallel default(none)
</span><span style=color:#75715e></span><span style=color:#111>{</span>
<span style=color:#111>}</span>
</code></pre></div><p>Then we must explictly provide the scoping rules for all variables we
use in the parallel region. This forces us to think about what the
right scope should be.</p><blockquote class="book-hint warning">My recommendation is to <strong>always</strong> use <code>default(none)</code> in parallel
directives. It might start out tedious, but it will save you many
subtle bugs!</blockquote><h3 id=values-on-entry>Values on entry
<a class=anchor href=#values-on-entry>#</a></h3><p>Shared variables take inherit the value outside the parallel region
inside. Private variables are uninitialised.</p><div class=book-include><div class=book-include-heading><tt>openmp-snippets/uninitialised.c</tt></div><div class=book-include-download><a href=https://teaching.wence.uk/phys52015/code/openmp-snippets/uninitialised.c>Download</a></div><div class=book-include-content><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;omp.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdlib.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span><span style=color:#00a8c8>int</span> <span style=color:#75af00>main</span><span style=color:#111>(</span><span style=color:#00a8c8>void</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>b</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>42</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#111>a</span> <span style=color:#f92672>=</span> <span style=color:#111>malloc</span><span style=color:#111>(</span><span style=color:#ae81ff>100</span><span style=color:#f92672>*</span><span style=color:#00a8c8>sizeof</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>a</span><span style=color:#111>));</span>

<span style=color:#75715e>#pragma omp parallel default(none) shared(a) private(b)
</span><span style=color:#75715e></span>  <span style=color:#111>{</span>
    <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>omp_get_thread_num</span><span style=color:#111>()]</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span><span style=color:#111>;</span>

    <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;Thread=%d; b=%d</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>omp_get_thread_num</span><span style=color:#111>(),</span> <span style=color:#111>b</span><span style=color:#111>);</span>
  <span style=color:#111>}</span>
  <span style=color:#111>free</span><span style=color:#111>(</span><span style=color:#111>a</span><span style=color:#111>);</span>
  <span style=color:#00a8c8>return</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
</code></pre></div></div></div><p>At least some compilers will warn about this.</p><pre><code>$ gcc-10 -Wall -Wextra -o uninitialised uninitialised.c -fopenmp
uninitialised.c: In function 'main._omp_fn.0':
uninitialised.c:13:5: warning: 'b' is used uninitialized in this function [-Wuninitialize]
   13 |     printf(&quot;%d %d\n&quot;, omp_get_thread_num(), b);
      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
uninitialised.c:6:7: note: 'b' was declared here
    6 |   int b = 42;
      |       ^
</code></pre><p>Running the code produces some (possibly surprising) results.</p><pre><code>$ OMP_NUM_THREADS=8 ./uninitialised
4 32642
1 32642
3 32642
2 32642
0 0
6 32642
7 32642
5 32642
</code></pre><blockquote class=exercise><h3>Exercise</h3><p>If you do this, do you always see the same nonsense values? Does it
depend on the compiler?</p><details><summary>Solution</summary><div class=markdown-inner>I, at least, don&rsquo;t always see the same values. Although it seems for
me, thread0 always gets initialised to zero.</div></details></blockquote><p>If you <em>really</em> need a private variable that takes its initial value
from the surrounding scope you can use the <code>firstprivate</code> clause. But
it is rare that this is necessary.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#111>b</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>23</span><span style=color:#111>;</span>
<span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#111>a</span> <span style=color:#f92672>=</span> <span style=color:#111>malloc</span><span style=color:#111>(</span><span style=color:#ae81ff>4</span><span style=color:#f92672>*</span><span style=color:#00a8c8>sizeof</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>a</span><span style=color:#111>));</span>
<span style=color:#75715e>#pragma omp parallel default(none) firstprivate(b) shared(a)
</span><span style=color:#75715e></span><span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#111>omp_get_thread_num</span><span style=color:#111>();</span>
  <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>4</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>b</span> <span style=color:#f92672>+</span> <span style=color:#111>i</span><span style=color:#111>;</span>
  <span style=color:#111>}</span>
<span style=color:#111>}</span>
</code></pre></div><h2 id=summary>Summary
<a class=anchor href=#summary>#</a></h2><p>OpenMP provides a directives + runtime library approach to shared
memory parallelism using threads. It uses the fork/join model of
parallel execution. Threads can diverge in control-flow, and can
either share variables, or have their own copies.</p><p>With the ability to create threads and have a unique identifier for
each thread, it is possible to program a lot of parallel patterns &ldquo;by
hand&rdquo;. For example, dividing loop iterations up between threads.</p><p>This is rather hard work, and OpenMP provides a number of additional
parallelisation directives which we will look at next that help with
<a href=https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/>parallelisation of loops</a></p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Modulo bugs in the compiler&rsquo;s implementation. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>The Intel compiler supports a &ldquo;stub&rdquo; library that you can use
with <code>-qopemp-stubs</code> if you want to compile in serial. <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/notes/openmp/intro.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#using-openmp>Using OpenMP</a><ul><li><a href=#directives>Directives</a></li><li><a href=#library-routines>Library routines</a></li><li><a href=#environment-variables>Environment variables</a></li></ul></li><li><a href=#parallel-constructs>Parallel constructs.</a></li><li><a href=#data-scoping-shared-and-private>Data scoping: shared and private</a><ul><li><a href=#default-data-scoping>Default data scoping</a></li><li><a href=#values-on-entry>Values on entry</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></aside></main></body></html>