<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="OpenMP #  Recall that one of the levels of parallelism offered by computer hardware is shared memory parallelism. In this style of computer architecture, there are multiple CPU cores that share main memory.
 Sketch of a shared memory chip with four sockets.
  In this picture, any CPU can access any of the memory.
Communication, and therefore coordination, between CPUs happens by reading and writing to this shared memory."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="OpenMP"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://teaching.wence.uk/phys52015/notes/openmp/"><title>OpenMP | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/ class=active>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>OpenMP</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#sync-data-race>Synchronisation and data races</a></li><li><a href=#programming-with-shared-memory>Programming with shared memory</a></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=openmp>OpenMP
<a class=anchor href=#openmp>#</a></h1><p>Recall that one of the levels of parallelism offered by computer
hardware is <a href=https://teaching.wence.uk/phys52015/notes/theory/hardware-parallelism/#shared-memory>shared memory</a> parallelism. In this
style of computer architecture, there are <em>multiple</em> CPU cores that
share main memory.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/numa-socket-sketch.svg alt="Sketch of a shared memory chip with four sockets."><figcaption><p>Sketch of a shared memory chip with four sockets.</p></figcaption></figure><p>In this picture, any CPU can access any of the memory.</p><p>Communication, and therefore coordination, between CPUs happens by
reading and writing to this shared memory.</p><p>Equally, division of work between CPUs can be arranged by each CPU
deciding which part of the data to work on.</p><p>Let us consider a cartoon of how this might work with two CPUs
cooperating to add up all the entries in a vector. The serial code
looks like this</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>double</span> <span style=color:#75af00>sum_vector</span><span style=color:#111>(</span><span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#111>a</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>N</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>double</span> <span style=color:#111>sum</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span>
    <span style=color:#111>sum</span> <span style=color:#f92672>=</span> <span style=color:#111>sum</span> <span style=color:#f92672>+</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
  <span style=color:#00a8c8>return</span> <span style=color:#111>sum</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
</code></pre></div><p>In parallel, the obvious thing to do is to divide up the array into
two halves</p><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner"><p>CPU 0</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#f92672>/</span><span style=color:#ae81ff>2</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span>
  <span style=color:#111>sum0</span> <span style=color:#f92672>=</span> <span style=color:#111>sum0</span> <span style=color:#f92672>+</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
</code></pre></div></div><div class="flex-even markdown-inner"><p>CPU 1</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#111>N</span><span style=color:#f92672>/</span><span style=color:#ae81ff>2</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span>
  <span style=color:#111>sum1</span> <span style=color:#f92672>=</span> <span style=color:#111>sum1</span> <span style=color:#f92672>+</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
</code></pre></div></div></div><p>These two <em>partial</em> sums must then be merged into a single result.</p><h2 id=sync-data-race>Synchronisation and data races
<a class=anchor href=#sync-data-race>#</a></h2><p>Conceptually this seems very straightforward, however there are some
subtleties that we should be aware of. The major one is to note that
the partial sums write to separate variables (and hence separate
places in memory).</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/SM-vector-sum.svg alt="Two CPUs coordinating to produce partial sums of an array."><figcaption><p>Two CPUs coordinating to produce partial sums of an array.</p></figcaption></figure><p>So far, everything is great. When we come to produce the final result,
however, we have a (potential) problem.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/SM-vector-sum-write-contention.svg alt="Two CPUs might race on writing the final answer."><figcaption><p>Two CPUs might race on writing the final answer.</p></figcaption></figure><p>Since we haven&rsquo;t asked for any <em>synchronisation</em> between the CPUs, if
we naively get the individual CPUs to increment the global summation
result by their partial sum, we are not guaranteed to get the correct
answer (namely <code>sum0 + sum1</code>). The three possible outcomes for <code>sum</code>
are:</p><ol><li><code>sum = sum0 + sum1</code>, the correct answer, great!</li><li><code>sum = sum0</code>, oh no.</li><li><code>sum = sum1</code>, also wrong.</li></ol><p>The reason is that to perform the addition both CPUs first load the
current value of <code>sum</code> from memory, increment it, and then store their
answer back to main memory. If <code>sum</code> is initially zero, then they
might both load zero, add their part, and then write back. Whichever
is &ldquo;slower&rdquo; then wins.</p><p>This is an example of a <em>data race</em>, and is the cause of most bugs in
shared memory programming.</p><blockquote class="book-hint info">A <strong>data race</strong> occurs whenever two or more threads access <em>the same</em>
memory location, and <em>at least one</em> of those accesses is a write.</blockquote><h2 id=programming-with-shared-memory>Programming with shared memory
<a class=anchor href=#programming-with-shared-memory>#</a></h2><p>The operating system level object that facilitates shared memory
programming is a <em>thread</em>. Threads are like processes (e.g. your text
editor and web browser are two different processes), but they
additionally can share memory (as well as having private,
thread-specific, memory). Threads logically all belong to the same
program, but they can follow different control-flow through the
program (since they each have their own, private, program counter).</p><p>Usually, though not always, we use one thread per physical CPU core.
However, this is not required.</p><p>The other concept is that of a <em>task</em>, which we can describe as some
piece of computation that can be executed independently of, and hence
potentially in parallel with, other tasks.</p><p>As with most parallel programming, the major idea is to figure out how
to distribute the computational work across the available resources.
<a href=https://teaching.wence.uk/phys52015/notes/openmp/intro/>OpenMP</a> provides facilities for doing this
for <a href=https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/>loop-based computations</a>.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/notes/openmp/_index.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#sync-data-race>Synchronisation and data races</a></li><li><a href=#programming-with-shared-memory>Programming with shared memory</a></li></ul></nav></aside></main></body></html>