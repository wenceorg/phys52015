<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>OpenMP on PHYS52015 – Introduction to HPC</title><link>https://teaching.wence.uk/phys52015/notes/openmp/</link><description>Recent content in OpenMP on PHYS52015 – Introduction to HPC</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://teaching.wence.uk/phys52015/notes/openmp/index.xml" rel="self" type="application/rss+xml"/><item><title>What is OpenMP?</title><link>https://teaching.wence.uk/phys52015/notes/openmp/intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/openmp/intro/</guid><description>What is OpenMP # OpenMP is a standardised API for programming shared memory computers (and more recently GPUs) using threading as the programming paradigm. It supports both data-parallel shared memory programming (typically for parallelising loops) and task parallelism. We&amp;rsquo;ll see some examples later.
In recent years, it has also gained support for some vector-based parallelism.
Using OpenMP # OpenMP is implemented as a set of extensions for C, C++, and Fortran.</description></item><item><title>Loop parallelism</title><link>https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/</guid><description>OpenMP loop parallelism # With a parallel region and identification of individual threads, we can actually parallelise loops &amp;ldquo;by hand&amp;rdquo;.
Suppose we wish to divide a loop approximately equally between all the threads, by assigning consecutive blocks of the loop iterations to consecutive threads.
Distribution of 16 loop iterations across five threads.
Notice here that the number of iterations is not evenly divisible by the number of threads, so we&amp;rsquo;ve assigned one extra iteration to thread0.</description></item><item><title>Collectives</title><link>https://teaching.wence.uk/phys52015/notes/openmp/collectives/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/openmp/collectives/</guid><description>OpenMP collectives # So far we&amp;rsquo;ve seen how we can create thread teams using #pragma omp parallel and distribute work in loops between members of the team by using #pragma omp for.
Now we&amp;rsquo;ll look at what we need to do if we need to communicate between threads.
Reductions # Remember that the OpenMP programming model allows communication between threads by using shared memory. If some piece of memory is shared in a parallel region then every thread in the team can both read and write to it.</description></item></channel></rss>