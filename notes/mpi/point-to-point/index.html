<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Pairwise message exchange #  The simplest form of communication in MPI is a pairwise exchange of a message between two processes.
In MPI, communication via messages is two-sided1. That is, for every message one process sends, there must be a matching receive call by another process.
 Cartoon of sending a message between two processes
  We need to fill in some details
 How will we describe &ldquo;data&rdquo; How will we identify processes How will the receiver know which message to put where?"><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Point-to-point messaging in MPI"><meta property="og:description" content="Pairwise message exchange #  The simplest form of communication in MPI is a pairwise exchange of a message between two processes.
In MPI, communication via messages is two-sided1. That is, for every message one process sends, there must be a matching receive call by another process.
 Cartoon of sending a message between two processes
  We need to fill in some details
 How will we describe &ldquo;data&rdquo; How will we identify processes How will the receiver know which message to put where?"><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/"><meta property="article:modified_time" content="2022-04-07T18:13:40+01:00"><title>Point-to-point messaging in MPI | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/ class=active>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Point-to-point messaging in MPI</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#describing-the-data>Describing the data</a></li><li><a href=#identification-of-processes-and-distinguishing-messages>Identification of processes and distinguishing messages</a></li><li><a href=#when-are-sends-receives-complete>When are sends (receives) complete?</a></li><li><a href=#different-types-of-send-calls>Different types of send calls</a><ul><li><a href=#synchronous-send-mpi_ssend>Synchronous send: <code>MPI_Ssend</code></a></li><li><a href=#buffered-send-mpi_bsend>Buffered send <code>MPI_Bsend</code></a></li><li><a href=#i-dont-want-to-decide-mpi_send>I don&rsquo;t want to decide: <code>MPI_Send</code></a></li><li><a href=#deadlock-exercise>A concrete example</a></li></ul></li><li><a href=#avoiding-deadlocks>Avoiding deadlocks</a><ul><li><a href=#pairwise-communication-mpi_sendrecv>Pairwise communication: <code>MPI_Sendrecv</code></a></li><li><a href=#non-blocking-communication>Non-blocking communication</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=pairwise-message-exchange>Pairwise message exchange
<a class=anchor href=#pairwise-message-exchange>#</a></h1><p>The simplest form of communication in MPI is a pairwise exchange of a
message between two processes.</p><p>In MPI, communication via messages is <em>two-sided</em><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. That is, for every
message one process sends, there must be a matching receive call by
another process.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-send-recv-cartoon.svg alt="Cartoon of sending a message between two processes"><figcaption><p>Cartoon of sending a message between two processes</p></figcaption></figure><p>We need to fill in some details</p><ol><li>How will we describe &ldquo;data&rdquo;</li><li>How will we identify processes</li><li>How will the receiver know which message to put where?</li><li>What does it mean for a send (or receive) to be complete?</li></ol><p>The C function signatures for basic, blocking send and receive are</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Send</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>dtype</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Recv</span><span style=color:#111>(</span><span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>dtype</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>src</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#111>MPI_Status</span> <span style=color:#f92672>*</span><span style=color:#111>status</span><span style=color:#111>);</span>
</code></pre></div><p>We first note a few things about the interface, and then describe the
details. All input and output variables are as arguments to the
functions.</p><blockquote class="book-hint info"><p>The return value is always an error code. As with normal C
conventions, the return value <code>0</code> means that no error occurred. you
can set an error handler in MPI, the default is to abort the program
when an error is encountered.</p><p>Sometimes you will encounter a programming library that does not do
this, so they can handle and provide better error messages. In those
circumstances you should always check the error code.</p></blockquote><p>Let&rsquo;s look at how this works in more detail.</p><h2 id=describing-the-data>Describing the data
<a class=anchor href=#describing-the-data>#</a></h2><p>To provide the data, we pass a pointer to the start of a buffer we
want to send from (receive into). It&rsquo;s a <code>void *</code> so that we can pass
any type. We describe how much data to send (receive) by providing a
<code>count</code> and a datatype. MPI datatypes are quite flexible, we will
start off only using builtin datatypes (for describing the basic
variable types that C supports). We show a list of the more common
ones below, see the section <a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node459.htm#Node459>Named Predefined Datatypes C
types</a>
in the MPI standard for the full list.</p><table><thead><tr><th align=left>C type</th><th align=left>MPI_Datatype</th></tr></thead><tbody><tr><td align=left><code>char</code></td><td align=left>MPI_CHAR</td></tr><tr><td align=left><code>int</code></td><td align=left>MPI_INT</td></tr><tr><td align=left><code>float</code></td><td align=left>MPI_FLOAT</td></tr><tr><td align=left><code>double</code></td><td align=left>MPI_DOUBLE</td></tr></tbody></table><p>For example, to send a single double we would write:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>double</span> <span style=color:#111>value</span> <span style=color:#f92672>=</span> <span style=color:#111>...;</span>
<span style=color:#111>MPI_Send</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>value</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_DOUBLE</span><span style=color:#111>,</span> <span style=color:#111>...);</span>
</code></pre></div><p>To send the second and third integers from an array of <code>ints</code></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#111>numbers</span><span style=color:#111>[</span><span style=color:#ae81ff>3</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>...;</span>
<span style=color:#75715e>/* &amp;(numbers[1]) is the address of the second entry in the array. */</span>
<span style=color:#111>MPI_Send</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>(</span><span style=color:#111>numbers</span><span style=color:#111>[</span><span style=color:#ae81ff>1</span><span style=color:#111>]),</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#111>...);</span>
</code></pre></div><p>Receiving works analogously, so to receive the two integers, this time
into the first two entries of a buffer</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#111>numbers</span><span style=color:#111>[</span><span style=color:#ae81ff>3</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>...;</span>
<span style=color:#75715e>/* We could also have written numbers here, since &amp;(numbers[0]) == numbers */</span>
<span style=color:#111>MPI_Recv</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>(</span><span style=color:#111>numbers</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>]),</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#111>...);</span>
</code></pre></div><h2 id=identification-of-processes-and-distinguishing-messages>Identification of processes and distinguishing messages
<a class=anchor href=#identification-of-processes-and-distinguishing-messages>#</a></h2><p>The concept that groups together processes in an MPI program is a
<em>communicator</em>. To identify processes in a send (receive) we use their
<code>rank</code> in a particular communicator. As we saw previously, MPI starts
up and provides a communicator that contains all processes, namely
<code>MPI_COMM_WORLD</code>.</p><p>Suppose I further (for my application) want to distinguish messages
with the same datatype/count arguments. I can use the <em>tags</em> to do so.
A message sent with tag <code>N</code> will only be matched by a receive that
also has tag <code>N</code>. Often it doesn&rsquo;t matter that much what we use as a
tag, because we arrange our code so that they are not important.</p><p>So if I want to send to rank 1 in <code>MPI_COMM_WORLD</code>, I write</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#111>MPI_Send</span><span style=color:#111>(...,</span> <span style=color:#ae81ff>1</span> <span style=color:#75715e>/* dest */</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span> <span style=color:#75715e>/* tag */</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>);</span>
</code></pre></div><p>Rank 1 can receive this message with:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#111>MPI_Recv</span><span style=color:#111>(...,</span> <span style=color:#ae81ff>0</span> <span style=color:#75715e>/* src */</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#75715e>/* tag */</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#111>...);</span>
</code></pre></div><blockquote class="book-hint warning">The count and datatype are <strong>not used</strong> when matching up sends and
receives, it is only the source/destination pair and the tag.</blockquote><p>To decide on the order in which messages are processed, MPI has a rule
that messages with the same source and tag do not &ldquo;overtake&rdquo;. So if I
have</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Send</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>vala</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_DOUBLE</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>);</span>
  <span style=color:#111>MPI_Send</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>valb</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_DOUBLE</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>);</span>
<span style=color:#111>}</span> <span style=color:#00a8c8>else</span> <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Recv</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>],</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_DOUBLE</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUS_IGNORE</span><span style=color:#111>);</span>
  <span style=color:#111>MPI_Recv</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#ae81ff>1</span><span style=color:#111>],</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_DOUBLE</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUS_IGNORE</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div><p>Then on rank 1, <code>a[0]</code> will always contain <code>vala</code> and <code>a[1]</code> will
always contain <code>valb</code>.</p><p>Let&rsquo;s look at an example. Suppose we have two processes, and we want
to send a message from rank 0 to rank 1.</p><div class=book-include><div class=book-include-heading><tt>mpi-snippets/send-message.c</tt></div><div class=book-include-download><a href=https://teaching.wence.uk/phys52015/code/mpi-snippets/send-message.c>Download</a></div><div class=book-include-content><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;mpi.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>main</span><span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>argc</span><span style=color:#111>,</span> <span style=color:#00a8c8>char</span> <span style=color:#f92672>**</span><span style=color:#111>argv</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>rank</span><span style=color:#111>,</span> <span style=color:#111>size</span><span style=color:#111>;</span>

  <span style=color:#00a8c8>double</span> <span style=color:#111>value</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#111>;</span>

  <span style=color:#111>MPI_Init</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>argc</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>argv</span><span style=color:#111>);</span>

  <span style=color:#111>MPI_Comm_rank</span><span style=color:#111>(</span><span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>rank</span><span style=color:#111>);</span>
  <span style=color:#111>MPI_Comm_size</span><span style=color:#111>(</span><span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>size</span><span style=color:#111>);</span>

  <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>value</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span><span style=color:#111>;</span>
  <span style=color:#111>}</span>

  <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>MPI_Ssend</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>value</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_DOUBLE</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>);</span>
  <span style=color:#111>}</span> <span style=color:#00a8c8>else</span> <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;[%d]: before receiving, my value is %g</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>rank</span><span style=color:#111>,</span> <span style=color:#111>value</span><span style=color:#111>);</span>
    <span style=color:#111>MPI_Recv</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>value</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_DOUBLE</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUS_IGNORE</span><span style=color:#111>);</span>
  <span style=color:#111>}</span>
  <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;[%d]: my value is %g</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>rank</span><span style=color:#111>,</span> <span style=color:#111>value</span><span style=color:#111>);</span>
  <span style=color:#111>MPI_Finalize</span><span style=color:#111>();</span>
  <span style=color:#00a8c8>return</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
</code></pre></div></div></div><p>Notice how we used <code>MPI_STATUS_IGNORE</code> in the status field in
<code>MPI_Recv</code>. The other option is to provide an <code>MPI_Status</code> object.
This can be used to find out a little more information about the
message.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#111>MPI_Status</span> <span style=color:#111>status</span><span style=color:#111>;</span>
<span style=color:#111>...;</span>
<span style=color:#111>MPI_Recv</span><span style=color:#111>(...,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>status</span><span style=color:#111>);</span>
</code></pre></div><p>We will revisit this when we look at <a href=https://teaching.wence.uk/phys52015/notes/mpi/point-to-point-nb/#wildcards>wildcard matching</a>.</p><blockquote class=exercise><h3>Exercise</h3><p>The code above sends a message from rank 0 to rank 1. Modify it so
that it sends the message from rank 0 to ranks $[1..N]$ when run on
$N$ processes.</p><details><summary>Solution</summary><div class=markdown-inner><p>We just need to turn the <code>else if (rank == 1)</code> into an <code>else</code> clause
and send <code>size-1</code> messages.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c>  <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>size</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
      <span style=color:#75715e>/* Send to every rank other than myself */</span>
      <span style=color:#111>MPI_Ssend</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>value</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_DOUBLE</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>);</span>
    <span style=color:#111>}</span>
  <span style=color:#111>}</span> <span style=color:#00a8c8>else</span> <span style=color:#111>{</span>
    <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;[%d]: before receiving, my value is %g</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>rank</span><span style=color:#111>,</span> <span style=color:#111>value</span><span style=color:#111>);</span>
    <span style=color:#111>MPI_Recv</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>value</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_DOUBLE</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUS_IGNORE</span><span style=color:#111>);</span>
  <span style=color:#111>}</span>
</code></pre></div><p>This is actually a <a href=https://teaching.wence.uk/phys52015/notes/mpi/collectives/#bcast>broadcast</a>,
for which the right MPI function is
<a href=https://rookiehpc.com/mpi/docs/mpi_bcast.php><code>MPI_Bcast</code></a>.</p></div></details></blockquote><h2 id=when-are-sends-receives-complete>When are sends (receives) complete?
<a class=anchor href=#when-are-sends-receives-complete>#</a></h2><p>Let us think about how MPI might implement sending a message over a
network. One option is that MPI copies the user data to be sent into a
buffer, sends it over the network into another buffer, and then copies
it out into the user-level receive buffer. This is shown in the figure
below.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-send-recv-with-buffers.svg alt="Send-receive pair with MPI-provided buffers."><figcaption><p>Send-receive pair with MPI-provided buffers.</p></figcaption></figure><p>To avoid this copy, we would like to directly send through the network</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-send-recv-no-buffer.svg alt="Send-receive with no buffers."><figcaption><p>Send-receive with no buffers.</p></figcaption></figure><p>For this to be possible, the send has to wait for the receive to be
available. MPI provides us with sending modes that support both of
these mechanisms.</p><h2 id=different-types-of-send-calls>Different types of send calls
<a class=anchor href=#different-types-of-send-calls>#</a></h2><h3 id=synchronous-send-mpi_ssend>Synchronous send: <code>MPI_Ssend</code>
<a class=anchor href=#synchronous-send-mpi_ssend>#</a></h3><p>This send mode covers the case with no buffers. The program will wait
inside the <a href=https://rookiehpc.com/mpi/docs/mpi_ssend.php><code>MPI_Ssend</code></a> call until the matching receive is ready. The
figure below shows a timeline on two processes.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-ssend-cartoon.svg alt="Sketch of synchronous send between two processes."><figcaption><p>Sketch of synchronous send between two processes.</p></figcaption></figure><h3 id=buffered-send-mpi_bsend>Buffered send <code>MPI_Bsend</code>
<a class=anchor href=#buffered-send-mpi_bsend>#</a></h3><p>This send mode allows the user to provide a buffer for MPI to copy
into. The call to
<a href=https://rookiehpc.com/mpi/docs/mpi_bsend.php><code>MPI_Bsend</code></a> will
return as soon as the data are copied into the buffer. If the buffer
is too small, an error occurs.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-bsend-cartoon.svg alt="Sketch of buffered send between two processes."><figcaption><p>Sketch of buffered send between two processes.</p></figcaption></figure><blockquote class="book-hint info"><h4 id=points-to-note>Points to note</h4><p>The receive <code>MPI_Recv</code> is always synchronous: it waits until the
buffer is filled up with the complete received message.</p><p>In the <code>Bsend</code> case, it the receive is issued on process 1 before
process 0 starts the send, then process 1 waits in the <code>MPI_Recv</code>
call.</p></blockquote><h3 id=i-dont-want-to-decide-mpi_send>I don&rsquo;t want to decide: <code>MPI_Send</code>
<a class=anchor href=#i-dont-want-to-decide-mpi_send>#</a></h3><p>Managing send buffers by hand for <code>Bsend</code> is somewhat tedious, so MPI
provides a get-out option:
<a href=https://rookiehpc.com/mpi/docs/mpi_send.php><code>MPI_Send</code></a>.</p><p>In <code>MPI_Send</code>, the buffer space is provided by the MPI implementation.
If enough buffer space is available for the message, it is used (so
the send behaves like <code>Bsend</code> and returns as soon as the copy is
complete). If the buffer is full, then <code>MPI_Send</code> turns into
<code>MPI_Ssend</code>.</p><blockquote class="book-hint warning">You can&rsquo;t rely on any particular size of buffer from the MPI
implementation, so you should really treat <code>MPI_Send</code> like <code>MPI_Ssend</code>.</blockquote><blockquote class="book-hint info"><h4 id=recommendation>Recommendation</h4><p><code>MPI_Bsend</code> is really an optimisation that you should apply once you
really want to squeeze the last little bit out of your implementation.</p><p>Therefore, I would only worry about <code>MPI_Send</code> and <code>MPI_Ssend</code>.
<code>MPI_Ssend</code> is <em>less forgiving</em> of incorrect code, so I recommend
<code>MPI_Ssend</code> to catch any deadlock errors.</p></blockquote><h3 id=deadlock-exercise>A concrete example
<a class=anchor href=#deadlock-exercise>#</a></h3><p>Let us look at the difference in behaviour between <code>MPI_Ssend</code> and
<code>MPI_Send</code> to observe how <code>MPI_Send</code> can hide deadlocks in some
circumstances.</p><p>Remember that <code>MPI_Send</code> returns immediately if there is enough buffer
space available, but turns into <code>MPI_Ssend</code> when the buffer space runs
out.</p><p>Here is a short snippet that illustrates the kind of problematic code.
Rank 0 will send a message to rank 1, and then receive a message from
rank 1. At the same time, rank 1 first sends a message to rank 0.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Send</span><span style=color:#111>(</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>...);</span>
  <span style=color:#111>MPI_Recv</span><span style=color:#111>(</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>...);</span>
<span style=color:#111>}</span> <span style=color:#00a8c8>else</span> <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Send</span><span style=color:#111>(</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>...);</span>
  <span style=color:#111>MPI_Recv</span><span style=color:#111>(</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>...);</span>
<span style=color:#111>}</span>
</code></pre></div><blockquote class=exercise><h3>Exercise</h3><p>The code <a href=https://teaching.wence.uk/phys52015/code/mpi-snippets/ptp-deadlock.c><code>mpi-snippets/ptp-deadlock.c</code></a> implements this message passing
deadlock.</p><p>It takes one argument, which is the size of message to send. Compile
with</p><pre><code>mpicc -o ptp-deadlock deadlock.c
</code></pre><details><summary>Hint</summary><div class=markdown-inner>Don&rsquo;t forget to load the <a href=https://teaching.wence.uk/phys52015/exercises/hello/#mpi>relevant MPI module</a>.</div></details><p>Run it on two processes.</p><p>How big can you make this message before you observe a
deadlock?</p><details><summary>Cancelling the process</summary><div class=markdown-inner><p>If you launched the run interactively, type <code>Control-c</code> to quite the
hanging process.</p><p>If you used the batch system you can use <code>scancel</code> followed by the ID
of the job to cancel the job (or set a short timeout in your slurm script).</p></div></details><p>Try changing the <code>MPI_Send</code> calls to <code>MPI_Ssend</code>, is there now any
value of the buffer size that completes successfully?</p><details><summary>Solution</summary><div class=markdown-inner><p>The MPI implementation I have access to completes with <code>16356</code> and
deadlocks with <code>16357</code>. Since each integer is 4 bytes, this is very
slightly less than to $2^16$ bytes.</p><p>When I replace <code>MPI_Send</code> with <code>MPI_Ssend</code>, as expected, no size of
message is sent successfully. This is because both processes are
waiting in the <code>MPI_Ssend</code> until a receive appears.</p></div></details></blockquote><h2 id=avoiding-deadlocks>Avoiding deadlocks
<a class=anchor href=#avoiding-deadlocks>#</a></h2><h3 id=pairwise-communication-mpi_sendrecv>Pairwise communication: <code>MPI_Sendrecv</code>
<a class=anchor href=#pairwise-communication-mpi_sendrecv>#</a></h3><p>For simple pairwise communication, like our example of exchanging
messages, MPI offers an function that does the equivalent of executing
a send and a receive <em>simultaneously</em> (avoiding the deadlock problem
of sends coming before receives).</p><p><a href=https://rookiehpc.com/mpi/docs/mpi_sendrecv.php><code>MPI_Sendrecv</code></a>
pairs up a send and a receive in one call.</p><blockquote class=exercise><h3>Exercise</h3><p>Rewrite the code of <a href=https://teaching.wence.uk/phys52015/code/mpi-snippets/ptp-deadlock.c><code>mpi-snippets/ptp-deadlock.c</code></a> to use <code>MPI_Sendrecv</code>.</p><details><summary>Solution</summary><div class=markdown-inner><p>This exercise previously inadvertantly said to rewrite
<code>send-message.c</code>, but we need both sides. Let&rsquo;s reproduce the relevant
bit</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c>  <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>MPI_Send</span><span style=color:#111>(</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>);</span>
    <span style=color:#111>MPI_Recv</span><span style=color:#111>(</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUS_IGNORE</span><span style=color:#111>);</span>
  <span style=color:#111>}</span> <span style=color:#00a8c8>else</span> <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#111>MPI_Send</span><span style=color:#111>(</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>);</span>
    <span style=color:#111>MPI_Recv</span><span style=color:#111>(</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUS_IGNORE</span><span style=color:#111>);</span>
  <span style=color:#111>}</span>
</code></pre></div><p>We basically just copy-and-paste the arguments from the <code>MPI_Send</code> and
<code>MPI_Recv</code> calls together:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Sendrecv</span><span style=color:#111>(</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#75715e>/* Send parameters */</span>
               <span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#75715e>/* Recv parameters */</span>
               <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUS_IGNORE</span><span style=color:#111>);</span>
<span style=color:#111>}</span> <span style=color:#00a8c8>else</span> <span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Sendrecv</span><span style=color:#111>(</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#75715e>/* Send parameters */</span>
               <span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#111>nentries</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#75715e>/* Recv parameters */</span>
               <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUS_IGNORE</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div><p>The usual mistake when using <code>MPI_Sendrecv</code> (especially if
communicating with multiple neighbours) is to match up the sends and
receives incorrectly.</p></div></details></blockquote><h3 id=non-blocking-communication>Non-blocking communication
<a class=anchor href=#non-blocking-communication>#</a></h3><p>The pairwise send-receive is useful. but not general enough to cover
all point-to-point communication patterns we might encounter. MPI
therefore offers &ldquo;non-blocking&rdquo; communication modes that return
immediately and allow us to later test if the message has been
sent/received.</p><p>This page is already long enough, so they&rsquo;re described in detail
<a href=https://teaching.wence.uk/phys52015/notes/mpi/point-to-point-nb/>separately</a>.</p><h2 id=summary>Summary
<a class=anchor href=#summary>#</a></h2><p>MPI has flexible point-to-point messaging. The message contents are
described by a pointer to a buffer (to send from/receive into) along
with a count and datatype.</p><p>The source or destination of a message is specified by providing the
communicator and a rank.</p><p>Messages can be distinguished by tags. Often don&rsquo;t need them for
simple processes, but can be used in advanced usage, or to make sure
that messages don&rsquo;t accidentally match.</p><p>You should now know enough to attempt the exercise <a href=https://teaching.wence.uk/phys52015/exercises/mpi-ring/>sending messages
in a ring</a>.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>MPI does have some facility for one-sided message passing, but
we won&rsquo;t cover it in this course. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/notes/mpi/point-to-point.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#describing-the-data>Describing the data</a></li><li><a href=#identification-of-processes-and-distinguishing-messages>Identification of processes and distinguishing messages</a></li><li><a href=#when-are-sends-receives-complete>When are sends (receives) complete?</a></li><li><a href=#different-types-of-send-calls>Different types of send calls</a><ul><li><a href=#synchronous-send-mpi_ssend>Synchronous send: <code>MPI_Ssend</code></a></li><li><a href=#buffered-send-mpi_bsend>Buffered send <code>MPI_Bsend</code></a></li><li><a href=#i-dont-want-to-decide-mpi_send>I don&rsquo;t want to decide: <code>MPI_Send</code></a></li><li><a href=#deadlock-exercise>A concrete example</a></li></ul></li><li><a href=#avoiding-deadlocks>Avoiding deadlocks</a><ul><li><a href=#pairwise-communication-mpi_sendrecv>Pairwise communication: <code>MPI_Sendrecv</code></a></li><li><a href=#non-blocking-communication>Non-blocking communication</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></aside></main></body></html>