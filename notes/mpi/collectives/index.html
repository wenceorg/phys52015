<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Collective communication #  Point-to-point messages are sufficient to write all the parallel algorithms we might want. However, they might not necessarily be the most efficient.
As motivation, let&rsquo;s think about the time we would expect the ring reduction you implemented to take as a function of the number of processes.
Recall from the ping-pong exercise that our model for the length of time it takes to send a message with $B$ bytes is"><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Collectives"><meta property="og:description" content="Collective communication #  Point-to-point messages are sufficient to write all the parallel algorithms we might want. However, they might not necessarily be the most efficient.
As motivation, let&rsquo;s think about the time we would expect the ring reduction you implemented to take as a function of the number of processes.
Recall from the ping-pong exercise that our model for the length of time it takes to send a message with $B$ bytes is"><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/notes/mpi/collectives/"><meta property="article:modified_time" content="2022-04-07T18:13:40+01:00"><title>Collectives | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/ class=active>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Collectives</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#what-about-implementation>What about implementation?</a></li><li><a href=#types-of-collectives>Types of collectives</a><ul><li><a href=#barrier-mpi_barrier>Barrier: <code>MPI_Barrier</code></a></li><li><a href=#reductions>Reductions</a></li><li><a href=#bcast>One to all: scatters and broadcasts</a></li><li><a href=#all-to-one-gathers>All to one: gathers</a></li><li><a href=#all-to-all-everyone-talks-to-everyone>All to all: everyone talks to everyone</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=collective-communication>Collective communication
<a class=anchor href=#collective-communication>#</a></h1><p><a href=https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/>Point-to-point</a> messages are
sufficient to write all the parallel algorithms we might want.
However, they might not necessarily be the most efficient.</p><p>As motivation, let&rsquo;s think about the time we would expect the <a href=https://teaching.wence.uk/phys52015/exercises/mpi-ring/>ring
reduction</a> you implemented to take as a
function of the number of processes.</p><p>Recall from the <a href=https://teaching.wence.uk/phys52015/exercises/mpi-ping-pong/>ping-pong exercise</a>
that our model for the length of time it takes to send a message with
$B$ bytes is</p><p>$$
T(B) = \alpha + \beta B
$$</p><p>To rotate each item all the way around a ring of $P$ processes
requires us to pass it from to each process in turn. To get all the
way around, the item must be sent $P-1$ times. The longest &ldquo;chain&rdquo; of
messages that must be sent has $P-1$ hops in it. Hence, supposing we
reduce $B$ bytes, the total time to perform a reduction using this
message passing scheme is:
$$
T_\text{ring}(B) = (P-1)(\alpha + \beta B).
$$
So as we add more processes, the time scales linearly.</p><p>An alternative is to combine the partial reductions pairwise in a
tree, as shown in the figure below<figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-tree-reduce.svg alt="Tree reduction across 16 processes. At each level processes combine pairwise."><figcaption><p>Tree reduction across 16 processes. At each level processes combine pairwise.</p></figcaption></figure>This approach can be generalised to problems where the number of
processes is not a power of two.</p><p>To get the result back to all processes, we can just send the combined
final value back up the tree in the same manner. For $P$ processes,
the tree has depth $\log_2 P$ (we divide the number of processes by two each
time). The longest chain of messages from the leaves of the tree to
the root is now only $\log_2 P$, so to reduce to the root and then
send back to the leaves has a longest chain of $2 \log_2 P$ messages.
The total time to perform a reduction using this tree-based message
passing scheme is therefore:
$$
T_{\text{tree}}(B) = 2\log_2 P (\alpha + \beta B)
$$
As long as $2\log_2 P &lt; (P-1)$ this takes less overall time. We can
solve this numerically to find it should be preferable as long as $P >
6$.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-python data-lang=python><span style=color:#f92672>import</span> <span style=color:#111>numpy</span>
<span style=color:#f92672>import</span> <span style=color:#111>scipy.optimize</span>


<span style=color:#75715e># Do Newton to find P such that 2log_2 P - (P-1) = 0</span>
<span style=color:#75715e># We know that P = 1 is a solution, so we deflate that away</span>
<span style=color:#00a8c8>def</span> <span style=color:#75af00>f</span><span style=color:#111>(</span><span style=color:#111>p</span><span style=color:#111>):</span>
    <span style=color:#00a8c8>return</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>/</span><span style=color:#111>abs</span><span style=color:#111>(</span><span style=color:#111>p</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#f92672>*</span> <span style=color:#111>(</span><span style=color:#ae81ff>2</span><span style=color:#f92672>*</span><span style=color:#111>numpy</span><span style=color:#f92672>.</span><span style=color:#111>log2</span><span style=color:#111>(</span><span style=color:#111>p</span><span style=color:#111>)</span> <span style=color:#f92672>-</span> <span style=color:#111>(</span><span style=color:#111>p</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span><span style=color:#111>))</span>


<span style=color:#111>root</span> <span style=color:#f92672>=</span> <span style=color:#111>scipy</span><span style=color:#f92672>.</span><span style=color:#111>optimize</span><span style=color:#f92672>.</span><span style=color:#111>newton</span><span style=color:#111>(</span><span style=color:#111>f</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>)</span>

<span style=color:#00a8c8>print</span><span style=color:#111>(</span><span style=color:#111>root</span><span style=color:#111>)</span>
<span style=color:#f92672>=&gt;</span> <span style=color:#ae81ff>6.319722355838366</span>
</code></pre></div><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/auto/log-tree-minimum-p.svg alt="Modelled time to compute a reduction using a 1D ring and a tree, as soon as $P &amp;gt; 6$, the tree is faster"><figcaption><p>Modelled time to compute a reduction using a 1D ring and a tree, as soon as $P > 6$, the tree is faster</p></figcaption></figure><p>We conclude that some kind of tree-based reduction is superior in
almost all circumstances.</p><p>This reduction is an example of a <em>collective</em> operation. Rather than
sending messages pairwise, all processes in a communicator
work together (or collectively) to compute some result.</p><h2 id=what-about-implementation>What about implementation?
<a class=anchor href=#what-about-implementation>#</a></h2><p>Writing code for a tree-reduction in MPI by hand is actually not too
hard. However, writing it generically to handle all datatypes is
slightly fiddly, and moreover, depending on the topology of the
underlying network, other approaches might be more efficient. For
example,
<a href=https://en.wikipedia.org/wiki/Hypercube_%28communication_pattern%29>hypercubes</a>
often offer a superior communication pattern.</p><p>Our conclusion then, is that we probably don&rsquo;t want to implement this
stuff ourselves.</p><p>Fortunately, MPI offers a broad range of builtin collective
operations that cover most of the communication patterns we might want
to use in a parallel program. It now falls to the implementor of the
MPI library (i.e. not us) to come up with an efficient implementation.</p><blockquote class="book-hint info">This is perhaps a general pattern that we observe when writing MPI
programs. It is best to see if our communication pattern fits with
something the MPI library implements before rolling our own.</blockquote><h2 id=types-of-collectives>Types of collectives
<a class=anchor href=#types-of-collectives>#</a></h2><p>One thing to note with all of these collectives is that none of them
require a <em>tag</em>. MPI therefore just matches collectives in the order
that they are called. So we need to be careful that we call
collectives in the same order on all processes.</p><blockquote class="book-hint info"><p>All of the collective operations we introduce in this section are
blocking: they only return once all processes in the communicator have
called them, at which point the input and output buffers are safe to
reuse.</p><p>MPI-3 (standardised in around 2014) introduce <em>non-blocking</em> versions
of all the collectives (with names starting with <code>MPI_I</code> like the
non-blocking point-to-point functions). These have the same semantics,
but we get an <code>MPI_Request</code> object that we must wait on before we can
look at the results.</p><p>The rationale for the introduction of non-blocking collectives was
similar to that for non-blocking point-to-point: they allow some
overlap of communication latency with computation, potentially
enabling more scalable code.</p></blockquote><h3 id=barrier-mpi_barrier>Barrier: <code>MPI_Barrier</code>
<a class=anchor href=#barrier-mpi_barrier>#</a></h3><p>We already saw a barrier in OpenMP, MPI similarly has one, named
<a href=https://rookiehpc.com/mpi/docs/mpi_barrier.php><code>MPI_Barrier</code></a>.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Barrier</span><span style=color:#111>(</span><span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
</code></pre></div><p>In a barrier, no process may exit the barrier until all have entered.
So we can use this to provide a synchronisation point in a program.
Note that there are no guarantees that all processes leave at the same
time.</p><blockquote class="book-hint warning"><p>There are actually very few reasons that a correct MPI code needs a
barrier.</p><p>The usual reason to add them is if we want to measure the parallel
performance of some part of the code, and don&rsquo;t want bad load balance
from another part to pollute our measurements.</p></blockquote><h3 id=reductions>Reductions
<a class=anchor href=#reductions>#</a></h3><p>These <em>are</em> useful, there are few related ones.</p><p><a href=https://rookiehpc.com/mpi/docs/mpi_reduce.php><code>MPI_Reduce</code></a> combines
messages with a provided operation and accumulates the final result on
a specified <code>root</code> rank.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Reduce</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>datatype</span><span style=color:#111>,</span>
               <span style=color:#111>MPI_Op</span> <span style=color:#111>op</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>root</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
</code></pre></div><p>Every process provides a value in <code>sendbuf</code>, the root process (often
rank 0, but not necessarily) provides an output buffer in <code>recvbuf</code>.
The count and datatype arguments are the same as for point-to-point
messaging, they describe how the send and receive buffers are to be
interpreted by MPI. The <code>op</code> argument tells MPI how to combine values
from different processes.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-reduce.svg alt="MPI_Reduce combines values with a specified operation and places the result on the root process."><figcaption><p><code>MPI_Reduce</code> combines values with a specified operation and places the result on the <code>root</code> process.</p></figcaption></figure><p>Closely related is
<a href=https://rookiehpc.com/mpi/docs/mpi_allreduce.php><code>MPI_Allreduce</code></a>
which does the same thing as <code>MPI_Reduce</code> but puts the final result on
all processes (so there is no need for a <code>root</code> argument.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Allreduce</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>datatype</span><span style=color:#111>,</span>
                  <span style=color:#111>MPI_Op</span> <span style=color:#111>op</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
</code></pre></div><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-allreduce.svg alt="MPI_Allreduce combines values with a specified operation and places the result on all processes."><figcaption><p><code>MPI_Allreduce</code> combines values with a specified operation and places the result on all processes.</p></figcaption></figure><p><code>MPI_Allreduce</code> is generally more useful than <code>MPI_Reduce</code>, since
often we need to make a collective decision based on the result of the
reduction (for example when checking for convergence of a numerical
algorithm). Although it is possible to implement an allreduce using an
<code>MPI_Reduce</code> followed by an
<a href=https://rookiehpc.com/mpi/docs/mpi_bcast.php><code>MPI_Bcast</code></a>, more
efficient algorithms exist.</p><p>There are also &ldquo;prefix reduction&rdquo; versions,
<a href=https://rookiehpc.com/mpi/docs/mpi_scan.php><code>MPI_Scan</code></a> and
<a href=https://rookiehpc.com/mpi/docs/mpi_exscan.php><code>MPI_Exscan</code></a> (for
inclusive and exclusive &ldquo;scans&rdquo; of the input respectively).</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Scan</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>datatype</span><span style=color:#111>,</span>
             <span style=color:#111>MPI_Op</span> <span style=color:#111>op</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Exscan</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>datatype</span><span style=color:#111>,</span>
               <span style=color:#111>MPI_Op</span> <span style=color:#111>op</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
</code></pre></div><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner"><figure style=width:100%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-scan.svg alt="MPI_Scan computes an inclusive prefix reduction."><figcaption><p><code>MPI_Scan</code> computes an inclusive prefix reduction.</p></figcaption></figure></div><div class="flex-even markdown-inner"><figure style=width:100%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-exscan.svg alt="MPI_Exscan computes an exclusive prefix reduction, on rank 0 the value in the output buffer is undefined."><figcaption><p><code>MPI_Exscan</code> computes an exclusive prefix reduction, on rank 0 the value in the output buffer is undefined.</p></figcaption></figure></div></div><p>The final missing piece is what this magic <code>op</code> argument is. For
combining simple types, we usually can use one of a number of builtin
options given to us by MPI.</p><table><thead><tr><th>Description</th><th>Operator</th><th>Identity element</th></tr></thead><tbody><tr><td>Addition</td><td><a href=https://rookiehpc.com/mpi/docs/mpi_sum.php><code>MPI_SUM</code></a></td><td>0</td></tr><tr><td>Multiplication</td><td><a href=https://rookiehpc.com/mpi/docs/mpi_prod.php><code>MPI_PROD</code></a></td><td>1</td></tr><tr><td>Minimum</td><td><a href=https://rookiehpc.com/mpi/docs/mpi_min.php><code>MPI_MIN</code></a></td><td>Most positive number of given type</td></tr><tr><td>Maximum</td><td><a href=https://rookiehpc.com/mpi/docs/mpi_max.php><code>MPI_MAX</code></a></td><td>Most negative number of given type</td></tr><tr><td>Logical and</td><td><a href=https://rookiehpc.com/mpi/docs/mpi_land.php><code>MPI_LAND</code></a></td><td>True</td></tr><tr><td>Logical or</td><td><a href=https://rookiehpc.com/mpi/docs/mpi_lor.php><code>MPI_LOR</code></a></td><td>False</td></tr><tr><td>Logical xor</td><td><a href=https://rookiehpc.com/mpi/docs/mpi_lxor.php><code>MPI_LXOR</code></a></td><td>False</td></tr><tr><td>Bitwise and</td><td><a href=https://rookiehpc.com/mpi/docs/mpi_band.php><code>MPI_BAND</code></a></td><td>All bits 1</td></tr><tr><td>Bitwise or</td><td><a href=https://rookiehpc.com/mpi/docs/mpi_bor.php><code>MPI_BOR</code></a></td><td>All bits 0</td></tr><tr><td>Bitwise xor</td><td><a href=https://rookiehpc.com/mpi/docs/mpi_bxor.php><code>MPI_BXOR</code></a></td><td>All bits 0</td></tr></tbody></table><p>If we pass a <code>count</code> of more than 1, then the operation is applied to
each entry in turn. So</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#111>send</span><span style=color:#111>[</span><span style=color:#ae81ff>2</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>...;</span>
<span style=color:#00a8c8>int</span> <span style=color:#111>recv</span><span style=color:#111>[</span><span style=color:#ae81ff>2</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>...;</span>
<span style=color:#111>MPI_Allreduce</span><span style=color:#111>(</span><span style=color:#111>send</span><span style=color:#111>,</span> <span style=color:#111>recv</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#111>MPI_SUM</span><span style=color:#111>,</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
</code></pre></div><p>Produces the sum over all processes of <code>send[0]</code> in <code>recv[0]</code> and that
of <code>send[1]</code> in <code>recv[1]</code>.</p><p>When computing a minimum or maximum value, we might also want to know
on which process the value was found. This can be done using the
combining operations
<a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node114.htm#Node114><code>MPI_MINLOC</code></a> and
<a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node114.htm#Node114><code>MPI_MAXLOC</code></a>. These
also show an example of using more complicated datatypes.</p><p>Suppose we have a pair of values</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>struct</span> <span style=color:#111>ValAndLoc</span> <span style=color:#111>{</span>
  <span style=color:#00a8c8>double</span> <span style=color:#111>x</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>loc</span><span style=color:#111>;</span>
<span style=color:#111>};</span>
</code></pre></div><p>We put the value we care about in the first slot, and the rank of the
current process (say) in the second. Now combining with <code>MPI_MAXLOC</code>
produces the <code>MAX</code> over the first slot, and just copies the second
slot over.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>struct</span> <span style=color:#111>ValAndLoc</span> <span style=color:#111>local</span><span style=color:#111>;</span>
<span style=color:#111>local</span><span style=color:#111>.</span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>...;</span>
<span style=color:#111>local</span><span style=color:#111>.</span><span style=color:#111>loc</span> <span style=color:#f92672>=</span> <span style=color:#111>rank</span><span style=color:#111>;</span>

<span style=color:#00a8c8>struct</span> <span style=color:#111>ValueAndLoc</span> <span style=color:#111>global</span><span style=color:#111>;</span>
<span style=color:#111>MPI_Allreduce</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>local</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>global</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_DOUBLE_INT</span><span style=color:#111>,</span> <span style=color:#111>MPI_MAXLOC</span><span style=color:#111>,</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
</code></pre></div><h3 id=bcast>One to all: scatters and broadcasts
<a class=anchor href=#bcast>#</a></h3><p>When starting up a simulation, we might want to read some
configuration file to provide parameters. This typically should only
be done by a single process<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, however, all processes will need to
know the values so that they can configure the simulation
appropriately.</p><p>To do this we can broadcast data from a single rank using
<a href=https://rookiehpc.com/mpi/docs/mpi_bcast.php><code>MPI_Bcast</code></a>.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Bcast</span><span style=color:#111>(</span><span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>datatype</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>root</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
</code></pre></div><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-bcast.svg alt="MPI_Bcast sends data from a root rank to all ranks in the communicator."><figcaption><p><code>MPI_Bcast</code> sends data from a root rank to all ranks in the communicator.</p></figcaption></figure><p>Note that the value in the buffer for input purposes only matters on
the root process, but all processes must provide enough space. For
example, to send 10 integers from rank 0</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#f92672>*</span><span style=color:#111>buffer</span> <span style=color:#f92672>=</span> <span style=color:#111>malloc</span><span style=color:#111>(</span><span style=color:#ae81ff>10</span><span style=color:#f92672>*</span><span style=color:#00a8c8>sizeof</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>buffer</span><span style=color:#111>));</span>
<span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
   <span style=color:#75715e>/* Initialise values from somewhere (e.g. read from file) */</span>
   <span style=color:#111>...</span>
<span style=color:#111>}</span>
<span style=color:#111>MPI_Bcast</span><span style=color:#111>(</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#ae81ff>10</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
</code></pre></div><p>A more general broadcast mechanism, where each rank receives different
data, is provided by
<a href=https://rookiehpc.com/mpi/docs/mpi_scatter.php><code>MPI_Scatter</code></a>, which
takes some data on a single rank, and splits it out to all processes.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Scatter</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>sendcount</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>sendtype</span><span style=color:#111>,</span>
                <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>recvcount</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>recvtype</span><span style=color:#111>,</span>
                <span style=color:#00a8c8>int</span> <span style=color:#111>root</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
</code></pre></div><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-scatter.svg alt="MPI_Scatter splits and sends data on the root rank to all ranks in the communicator."><figcaption><p><code>MPI_Scatter</code> splits and sends data on the root rank to all ranks in the communicator.</p></figcaption></figure><p>Note that the <code>sendcount</code> argument is the number of values to send to
each process (not the total number of values in the send buffer). On
ranks other than the root rank, the send parameters are ignored (so
<code>sendbuf</code> can be NULL).</p><p>For example, consider again a situation where rank zero reads a
configuration file and then determines some allocation of work
(perhaps loop iteration counts) for each process. We need to
communicate a pair of &ldquo;start&rdquo; and &ldquo;end&rdquo; values to each process.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#111>rank</span><span style=color:#111>;</span>
<span style=color:#111>MPI_Comm_rank</span><span style=color:#111>(</span><span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>rank</span><span style=color:#111>);</span>
<span style=color:#00a8c8>int</span> <span style=color:#f92672>*</span><span style=color:#111>recvbuf</span> <span style=color:#f92672>=</span> <span style=color:#111>malloc</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span><span style=color:#f92672>*</span><span style=color:#00a8c8>sizeof</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>recvbuf</span><span style=color:#111>));</span>

<span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>size</span><span style=color:#111>;</span>
  <span style=color:#111>MPI_Comm_size</span><span style=color:#111>(</span><span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>size</span><span style=color:#111>);</span>
  <span style=color:#00a8c8>int</span> <span style=color:#f92672>*</span><span style=color:#111>sendbuf</span> <span style=color:#f92672>=</span> <span style=color:#111>malloc</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span><span style=color:#f92672>*</span><span style=color:#111>size</span><span style=color:#f92672>*</span><span style=color:#00a8c8>sizeof</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>sendbuf</span><span style=color:#111>));</span>
  <span style=color:#75715e>/* Populate sendbuf somehow */</span>
  <span style=color:#111>MPI_Scatter</span><span style=color:#111>(</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
<span style=color:#111>}</span> <span style=color:#00a8c8>else</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Scatter</span><span style=color:#111>(</span><span style=color:#111>NULL</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>MPI_DATATYPE_NULL</span><span style=color:#111>,</span> <span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div><p>This sends entries 0 and 1 in sendbuf to rank 0; entries 2 and 3 to
rank 1; entries 4 and 5 to rank 2; and so on.</p><p>There is also a more general
<a href=https://rookiehpc.com/mpi/docs/mpi_scatterv.php><code>MPI_Scatterv</code></a>,
in which we can specify how many values we send to each process.</p><h3 id=all-to-one-gathers>All to one: gathers
<a class=anchor href=#all-to-one-gathers>#</a></h3><p>The dual to
<a href=https://rookiehpc.com/mpi/docs/mpi_scatter.php><code>MPI_Scatter</code></a>, in
which we gather all values to a single process is (perhaps
unsurprisingly) called
<a href=https://rookiehpc.com/mpi/docs/mpi_gather.php><code>MPI_Gather</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Gather</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>sendcount</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>sendtype</span><span style=color:#111>,</span>
               <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>recvcount</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>recvtype</span><span style=color:#111>,</span>
               <span style=color:#00a8c8>int</span> <span style=color:#111>root</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
</code></pre></div><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-gather.svg alt="MPI_Gather gathers data from all ranks to a root rank."><figcaption><p><code>MPI_Gather</code> gathers data from all ranks to a root rank.</p></figcaption></figure><p>We might use this, for example, to collect output data before writing
it to a file<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><p>Just like <code>MPI_Scatter</code>, there is a more general
<a href=https://rookiehpc.com/mpi/docs/mpi_gatherv.php><code>MPI_Gatherv</code></a> in
which we specify how many values to gather from each process.</p><h3 id=all-to-all-everyone-talks-to-everyone>All to all: everyone talks to everyone
<a class=anchor href=#all-to-all-everyone-talks-to-everyone>#</a></h3><p>Generally, we would like to avoid parallelisation that requires that
we have data whose size scales with the total number of processes on
every rank, or where every process must communicate with every other,
sometimes this is unavoidable.</p><p>For example, multi-dimensional <a href=https://en.wikipedia.org/wiki/Fast_Fourier_transform>fast Fourier
transforms</a> are
among the fastest approaches for computing the inverse of the
Laplacian in periodic domains. They form, amongst other things, a core
computational component of numerical simulation of turbulent flow (see
for example the codes at <a href=https://github.com/spectralDNS)>https://github.com/spectralDNS)</a>.</p><p>To compute Fourier transforms in parallel, we need to do
one-dimensional transforms along each cartesian direction,
interspersed by a global transpose of the data. Depending on the data
distribution, this is a generalised &ldquo;all to all&rdquo; communication
pattern. See
<a href=https://mpi4py-fft.readthedocs.io/en/latest/introduction.html>mpi4py-fft</a>
for a high-performance Python library that does this.</p><p>MPI provides a number of routines that help with these including</p><ul><li><a href=https://rookiehpc.com/mpi/docs/mpi_allgather.php><code>MPI_Allgather</code></a>
(and its &ldquo;vector&rdquo; sibling
<a href=https://rookiehpc.com/mpi/docs/mpi_allgatherv.php><code>MPI_Allgatherv</code></a>).</li><li><a href=https://rookiehpc.com/mpi/docs/mpi_alltoall.php><code>MPI_Alltoall</code></a></li></ul><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Allgather</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>sendcount</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>sendtype</span><span style=color:#111>,</span>
                  <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>recvcount</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>recvtype</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Alltoall</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>sendcount</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>sendtype</span><span style=color:#111>,</span>
                 <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>recvbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>recvcount</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>recvtype</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
</code></pre></div><p>Note how neither of these calls has a <code>root</code> argument: there is no
&ldquo;special&rdquo; process in these collectives.</p><p>The pictorial version of the routines is shown below</p><figure><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-allgather.svg alt="MPI_Allgather acts like MPI_Gather but no process is special."><figcaption><p><code>MPI_Allgather</code> acts like <code>MPI_Gather</code> but no process is special.</p></figcaption></figure><p>Note how the <code>MPI_Alltoall</code> call ends up transposing the data.<figure><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-alltoall.svg alt="MPI_Alltoall sends data from all process to all processes, transposing along the way."><figcaption><p><code>MPI_Alltoall</code> sends data from all process to all processes, transposing along the way.</p></figcaption></figure></p><h2 id=summary>Summary
<a class=anchor href=#summary>#</a></h2><p>MPI has a rich array of <em>collective</em> operations which can be used to
implement common communication patterns much more efficiently than we
are likely to write by hand using only point-to-point messaging.</p><p>Unlike point-to-point messaging, there are no tags in collective
operations, so collectives are matched &ldquo;in-order&rdquo; on the communicator
they are used with. We briefly look at how to distinguish collective
operations, and how to restrict the operation to a subset of all the
processes in <code>MPI_COMM_WORLD</code> when considering <a href=https://teaching.wence.uk/phys52015/notes/mpi/advanced/#communicators>communicator
manipulation</a></p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>All processes simultaneously accessing the same file can be very
slow on some systems. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>In real-world applications you should use a more scalable,
parallel, approach. For example, directly using MPI-IO (see <a href=https://wgropp.cs.illinois.edu>Bill
Gropp&rsquo;s</a>
<a href=https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture32.pdf>slides</a>
for a nice intro and motivation), or a higher-level alternative
like <a href=https://www.hdfgroup.org>HDF5</a>. These additional libraries
are beyond the scope of this course. <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/notes/mpi/collectives.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#what-about-implementation>What about implementation?</a></li><li><a href=#types-of-collectives>Types of collectives</a><ul><li><a href=#barrier-mpi_barrier>Barrier: <code>MPI_Barrier</code></a></li><li><a href=#reductions>Reductions</a></li><li><a href=#bcast>One to all: scatters and broadcasts</a></li><li><a href=#all-to-one-gathers>All to one: gathers</a></li><li><a href=#all-to-all-everyone-talks-to-everyone>All to all: everyone talks to everyone</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></aside></main></body></html>