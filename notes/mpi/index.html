<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="MPI: a messaging API #  OpenMP level parallelism is possible when using a single computer, with a shared memory space.
If we want to solve really big problems, we need to move beyond this to a situation where we have distributed memory. In this situation, we have multiple computers which are connected with some form of network. The dominant parallel programming paradigm for this situation is that of message passing."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="MPI"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://teaching.wence.uk/phys52015/notes/mpi/"><title>MPI | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/ class=active>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>MPI</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#concepts>Concepts</a></li><li><a href=#single-program-multiple-data-spmd>Single program, multiple data (SPMD)</a></li><li><a href=#communicators>Communicators</a></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=mpi-a-messaging-api>MPI: a messaging API
<a class=anchor href=#mpi-a-messaging-api>#</a></h1><p>OpenMP level parallelism is possible when
using a single computer, with a shared memory space.</p><p>If we want to solve really big problems, we need to move beyond this
to a situation where we have <em>distributed memory</em>. In this situation,
we have multiple computers which are connected with some form of
network. The dominant parallel programming paradigm for this situation
is that of <em>message passing</em>.</p><p>Where in the OpenMP/shared memory world we can pass information
between threads by writing to some shared memory, in distributed
memory settings we need to explicitly package up and send data between
the different computers involved in the computation.</p><p>In the high performance computing world, the dominant library for
message passing is <a href=https://www.mpi-forum.org/>MPI</a>, the Message
Passing Interface.</p><p>Let&rsquo;s look at a simple <a href=https://teaching.wence.uk/phys52015/exercises/hello/#mpi>&ldquo;Hello, World&rdquo;</a>
MPI program, and contrast it with the equivalent OpenMP program.</p><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner"><div class=book-include><div class=book-include-heading><tt>hello/mpi.c</tt></div><div class=book-include-download><a href=https://teaching.wence.uk/phys52015/code/hello/mpi.c>Download</a></div><div class=book-include-content><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;mpi.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>main</span><span style=color:#111>(</span><span style=color:#00a8c8>void</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>rank</span><span style=color:#111>,</span> <span style=color:#111>size</span><span style=color:#111>,</span> <span style=color:#111>len</span><span style=color:#111>;</span>
  <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>char</span> <span style=color:#111>name</span><span style=color:#111>[</span><span style=color:#111>MPI_MAX_PROCESSOR_NAME</span><span style=color:#111>];</span>
  <span style=color:#111>MPI_Init</span><span style=color:#111>(</span><span style=color:#111>NULL</span><span style=color:#111>,</span> <span style=color:#111>NULL</span><span style=color:#111>);</span>
  <span style=color:#111>comm</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>;</span>
  <span style=color:#111>MPI_Comm_rank</span><span style=color:#111>(</span><span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>rank</span><span style=color:#111>);</span>
  <span style=color:#111>MPI_Comm_size</span><span style=color:#111>(</span><span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>size</span><span style=color:#111>);</span>
  <span style=color:#111>MPI_Get_processor_name</span><span style=color:#111>(</span><span style=color:#111>name</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>len</span><span style=color:#111>);</span>
  <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;Hello, World! I am rank %d of %d. Running on node %s</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span>
         <span style=color:#111>rank</span><span style=color:#111>,</span> <span style=color:#111>size</span><span style=color:#111>,</span> <span style=color:#111>name</span><span style=color:#111>);</span>
  <span style=color:#111>MPI_Finalize</span><span style=color:#111>();</span>
  <span style=color:#00a8c8>return</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
</code></pre></div></div></div></div><div class="flex-even markdown-inner"><div class=book-include><div class=book-include-heading><tt>hello/openmp.c</tt></div><div class=book-include-download><a href=https://teaching.wence.uk/phys52015/code/hello/openmp.c>Download</a></div><div class=book-include-content><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;omp.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>main</span><span style=color:#111>(</span><span style=color:#00a8c8>void</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>nthread</span> <span style=color:#f92672>=</span> <span style=color:#111>omp_get_max_threads</span><span style=color:#111>();</span>
  <span style=color:#00a8c8>int</span> <span style=color:#00a8c8>thread</span><span style=color:#111>;</span>
<span style=color:#75715e>#pragma omp parallel private(thread) shared(nthread)
</span><span style=color:#75715e></span>  <span style=color:#111>{</span>
    <span style=color:#00a8c8>thread</span> <span style=color:#f92672>=</span> <span style=color:#111>omp_get_thread_num</span><span style=color:#111>();</span>
    <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;Hello, World! I am thread %d of %d</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#00a8c8>thread</span><span style=color:#111>,</span> <span style=color:#111>nthread</span><span style=color:#111>);</span>
  <span style=color:#111>}</span>
  <span style=color:#00a8c8>return</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
</code></pre></div></div></div></div></div><p>Our MPI programs always start by initialising MPI with <code>MPI_Init</code>.
They must finish by shutting down, with <code>MPI_Finalize</code>. Inside, it
seems like there is no explicit parallelism written anywhere.</p><h2 id=concepts>Concepts
<a class=anchor href=#concepts>#</a></h2><p>The message passing model is based on the notion of processes (rather
than the threads in OpenMP). Processes are very similar, but do not
share an address space (and therefore do not share memory).</p><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner"><figure style=width:100%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/processes-private-memory-sketch.svg alt="Processes do not share memory."><figcaption><p>Processes do not share memory.</p></figcaption></figure></div><div class="flex-even markdown-inner"><figure style=width:100%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/shared-memory-sketch.svg alt="Threads can share memory."><figcaption><p>Threads can share memory.</p></figcaption></figure></div></div><p>Like in OpenMP programming, we achieve parallelism by having many
processes cooperate to solve the same task. We must come up with some
way of dividing the data and computation between the processes.</p><p>Since processes do not share memory, they must instead send messages
to communicate information. This is implemented in MPI through library
calls that we can make from our sequential programming language.
This is in contrast to OpenMP which defines pragma-based extensions to
the language.</p><p>The core difficulty in writing message-passing programs is the
conceptual model. This is a very different model to that required for
sequential programs. Becoming comfortable with this model is key to
understanding MPI programs. A key idea, which is different from the
examples we&rsquo;ve seen with OpenMP, is that there is much more focus on
the <em>decomposition of the data and work</em>. That is, we must think about
how we divide the data (and hence work) in our parallel program. I
will endeavour to emphasise this through the examples and exposition
when we encounter MPI functions.</p><p>Although at first MPI parallelism seems more complicated than OpenMP
(we can&rsquo;t just annotate a few loops with a <code>#pragma</code>), it is, in my
experience, a much more <em>powerful</em> programming model, and better
suited to the implementation of reusable software.</p><h2 id=single-program-multiple-data-spmd>Single program, multiple data (SPMD)
<a class=anchor href=#single-program-multiple-data-spmd>#</a></h2><p>Most MPI programs are written using the single-program, multiple data
(SPMD) paradigm. All processes are launched and run their <em>own</em> copy
of the same program. You saw this with the <a href=https://teaching.wence.uk/phys52015/exercises/hello/>Hello World</a> example.</p><p>Although each process is running the same program, they each have a
separate copy of data (there is no sharing like in OpenMP).</p><p>So that this is useful, processes have a unique identifier, their
<em>rank</em>. We can then write code that sends different ranks down
different paths in the control flow.</p><p>The way to think about this is as if we had written a number of
different copies of a program and each process gets its own copy. They
then execute at the same time and can pass messages to each other.</p><p>Suppose we have a function</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>void</span> <span style=color:#75af00>print_hello</span><span style=color:#111>(</span><span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>rank</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>size</span><span style=color:#111>;</span>

  <span style=color:#111>MPI_Comm_rank</span><span style=color:#111>(</span><span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>rank</span><span style=color:#111>);</span>
  <span style=color:#111>MPI_Comm_size</span><span style=color:#111>(</span><span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>size</span><span style=color:#111>);</span>

  <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;Hello, I am rank %d of %d</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>rank</span><span style=color:#111>,</span> <span style=color:#111>size</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div><p>Then if we execute it with two processes we have</p><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner"><p>Process 0</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>void</span> <span style=color:#75af00>print_hello</span><span style=color:#111>(</span><span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>rank</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>size</span><span style=color:#111>;</span>

  <span style=color:#111>rank</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span>
  <span style=color:#111>size</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span><span style=color:#111>;</span>
  <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;Hello, I am rank %d of %d</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>rank</span><span style=color:#111>,</span> <span style=color:#111>size</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div></div><div class="flex-even markdown-inner"><p>Process 1</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>void</span> <span style=color:#75af00>print_hello</span><span style=color:#111>(</span><span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>rank</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>size</span><span style=color:#111>;</span>

  <span style=color:#111>rank</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#111>;</span>
  <span style=color:#111>size</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span><span style=color:#111>;</span>
  <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;Hello, I am rank %d of %d</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>rank</span><span style=color:#111>,</span> <span style=color:#111>size</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div></div></div><p>Of course, on its own, this is not that useful. So the real power in
MPI comes through the ability to send messages between the processes.
These are facilitated by communicators.</p><h2 id=communicators>Communicators
<a class=anchor href=#communicators>#</a></h2><p>The powerful abstraction that MPI is built around is a notion of a
communicator. This logically groups some set of processes in the MPI
program. All communication happens via communicators. That is, when
sending and receiving messages we do so by providing a
communicator and a source/destination to be interpreted with reference
to that communicator.</p><p>When MPI launches a program, it pre-initialises two communicators</p><dl><dt><code>MPI_COMM_WORLD</code></dt><dd>A communicator consisting of all the processes in the current run.</dd><dt><code>MPI_COMM_SELF</code></dt><dd>A communicator consisting of each process individually.</dd></dl><p>The figure below shows an example of eight processes and draws the
world and self communicators.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/comm-world-comm-self.svg alt="An MPI program with eight processes and their ranks in MPI_COMM_WORLD (left) and MPI_COMM_SELF (right). In the right-hand figure the corresponding world rank is shown in parentheses"><figcaption><p>An MPI program with eight processes and their ranks in <code>MPI_COMM_WORLD</code> (left) and <code>MPI_COMM_SELF</code> (right). In the right-hand figure the corresponding world rank is shown in parentheses</p></figcaption></figure><p>A key thing to note is that the processes are the <em>same</em> in the left
and right figures. It is just their identifier that changes depending
on which communicator we view them through.</p><blockquote class=exercise><h3>Exercise</h3><p>This concept is illustrated by <a href=https://teaching.wence.uk/phys52015/code/mpi-snippets/comm-world-self.c><code>mpi-snippets/comm-world-self.c</code></a>.</p><p>Compile it with</p><pre><code>$ mpicc -o comm-world-self comm-world-self.c
</code></pre><details><summary>Hint</summary><div class=markdown-inner>Don&rsquo;t forget than in addition to loading the normal compiler modules
you also need to load the <code>intelmpi/intel/2018.2</code> module.</div></details><p>And run with</p><pre><code>$ mpirun -n 4 ./comm-world-self
</code></pre><p>Do you understand the output?</p></blockquote><p>An important thing about communicators is that they are always
explicit when we send messages: to send a message, we need a
communicator. So communicators, and the group of processes they
represent, are at the core of MPI programming. This is in contrast to
OpenMP where we generally don&rsquo;t think about which threads are in
involved in a parallel region.</p><p>We&rsquo;ll revisit these ideas as we learn more MPI functions. Next, we&rsquo;ll
look at <a href=https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/>point-to-point</a> messaging.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/notes/mpi/_index.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#concepts>Concepts</a></li><li><a href=#single-program-multiple-data-spmd>Single program, multiple data (SPMD)</a></li><li><a href=#communicators>Communicators</a></li></ul></nav></aside></main></body></html>