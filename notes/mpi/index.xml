<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MPI on PHYS52015 – Introduction to HPC</title><link>https://teaching.wence.uk/phys52015/notes/mpi/</link><description>Recent content in MPI on PHYS52015 – Introduction to HPC</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://teaching.wence.uk/phys52015/notes/mpi/index.xml" rel="self" type="application/rss+xml"/><item><title>Point-to-point messaging in MPI</title><link>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/</guid><description>Pairwise message exchange # The simplest form of communication in MPI is a pairwise exchange of a message between two processes.
In MPI, communication via messages is two-sided1. That is, for every message one process sends, there must be a matching receive call by another process.
Cartoon of sending a message between two processes
We need to fill in some details
How will we describe &amp;ldquo;data&amp;rdquo; How will we identify processes How will the receiver know which message to put where?</description></item><item><title>Non-blocking point-to-point messaging</title><link>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point-nb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point-nb/</guid><description>Non-blocking messages # As well as the blocking point to point messaging we saw last time, MPI also offers non-blocking versions.
These functions all return immediately, and provide a &amp;ldquo;request&amp;rdquo; object that we can then either wait for completion with or inspect to check if the message has been sent/received.
The function signatures for MPI_Isend and MPI_Irecv are:
int MPI_Isend(const void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); int MPI_Irecv(void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); Notice how the send gets an extra output argument (the request), and the receive loses the MPI_Status output argument and gains a request output argument.</description></item><item><title>Collectives</title><link>https://teaching.wence.uk/phys52015/notes/mpi/collectives/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/collectives/</guid><description>Collective communication # Point-to-point messages are sufficient to write all the parallel algorithms we might want. However, they might not necessarily be the most efficient.
As motivation, let&amp;rsquo;s think about the time we would expect the ring reduction you implemented to take as a function of the number of processes.
Recall from the ping-pong exercise that our model for the length of time it takes to send a message with $B$ bytes is</description></item><item><title>Advanced topics</title><link>https://teaching.wence.uk/phys52015/notes/mpi/advanced/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/advanced/</guid><description>Some pointers to more advanced features of MPI # Communicator manipulation # We saw that we can distinguish point-to-point messages by providing different tags, but that there was no such facility for collective operations. Moreover, a collective operation (by definition) involves all the processes in a communicator.
This raises two questions:
How can we have multiple collective operations without them interfering with each other; What if we want a collective operation, but using only a subset of the processes (e.</description></item></channel></rss>