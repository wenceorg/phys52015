<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Non-blocking messages #  As well as the blocking point to point messaging we saw last time, MPI also offers non-blocking versions.
These functions all return immediately, and provide a &ldquo;request&rdquo; object that we can then either wait for completion with or inspect to check if the message has been sent/received.
The function signatures for MPI_Isend and MPI_Irecv are:
int MPI_Isend(const void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); int MPI_Irecv(void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); Notice how the send gets an extra output argument (the request), and the receive loses the MPI_Status output argument and gains a request output argument."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Non-blocking point-to-point messaging"><meta property="og:description" content="Non-blocking messages #  As well as the blocking point to point messaging we saw last time, MPI also offers non-blocking versions.
These functions all return immediately, and provide a &ldquo;request&rdquo; object that we can then either wait for completion with or inspect to check if the message has been sent/received.
The function signatures for MPI_Isend and MPI_Irecv are:
int MPI_Isend(const void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); int MPI_Irecv(void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); Notice how the send gets an extra output argument (the request), and the receive loses the MPI_Status output argument and gains a request output argument."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/notes/mpi/point-to-point-nb/"><meta property="article:modified_time" content="2022-04-07T18:13:40+01:00"><title>Non-blocking point-to-point messaging | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/ class=active>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Non-blocking point-to-point messaging</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#why-would-you-do-this>Why would you do this?</a></li><li><a href=#waiting-for-multiple-messages>Waiting for multiple messages</a></li><li><a href=#wildcards>Wildcard matching</a></li><li><a href=#summary>Summary</a></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=non-blocking-messages>Non-blocking messages
<a class=anchor href=#non-blocking-messages>#</a></h1><p>As well as the blocking point to point messaging we saw <a href=https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/>last
time</a>, MPI also offers <em>non-blocking</em>
versions.</p><p>These functions all return immediately, and provide a &ldquo;request&rdquo; object
that we can then either wait for completion with or inspect to check
if the message has been sent/received.</p><p>The function signatures for
<a href=https://rookiehpc.com/mpi/docs/mpi_isend.php><code>MPI_Isend</code></a> and
<a href=https://rookiehpc.com/mpi/docs/mpi_irecv.php><code>MPI_Irecv</code></a> are:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Isend</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>dtype</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#111>MPI_Request</span> <span style=color:#f92672>*</span><span style=color:#111>request</span><span style=color:#111>);</span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Irecv</span><span style=color:#111>(</span><span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>dtype</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#111>MPI_Request</span> <span style=color:#f92672>*</span><span style=color:#111>request</span><span style=color:#111>);</span>
</code></pre></div><p>Notice how the send gets an extra output argument (the request), and
the receive loses the <code>MPI_Status</code> output argument and gains a request
output argument.</p><blockquote class="book-hint warning"><p>With the <a href=https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/>blocking versions</a>
(<code>MPI_Send</code>, <code>MPI_Ssend</code>, <code>MPI_Bsend</code>), the buffer argument is safe to
reuse <em>as soon as the function returns</em>. Equally, as soon as
<code>MPI_Recv</code> returns, we know the message has been received and we can
inspect the contents.</p><p><strong>This is not the case</strong> for non-blocking calls.</p><p>We are not allowed to reuse the buffer (or rely on its contents being
ready) until we have &ldquo;waited&rdquo; on the <code>request</code> handle.</p><p>See below for details on how to do this.</p></blockquote><p>If we have a request, we can check whether the message it corresponds
to has been completed with <a href=https://rookiehpc.com/mpi/docs/mpi_test.php><code>MPI_Test</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Test</span><span style=color:#111>(</span><span style=color:#111>MPI_Request</span> <span style=color:#f92672>*</span><span style=color:#111>request</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#f92672>*</span><span style=color:#111>flag</span><span style=color:#111>,</span> <span style=color:#111>MPI_Status</span> <span style=color:#f92672>*</span><span style=color:#111>status</span><span style=color:#111>);</span>
</code></pre></div><p><code>flag</code> will be true if the provided request has been completed, and
false otherwise.</p><p>If instead we want to wait for completion, we can use
<a href=https://rookiehpc.com/mpi/docs/mpi_wait.php><code>MPI_Wait</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Wait</span><span style=color:#111>(</span><span style=color:#111>MPI_Request</span> <span style=color:#f92672>*</span><span style=color:#111>request</span><span style=color:#111>,</span> <span style=color:#111>MPI_Status</span> <span style=color:#f92672>*</span><span style=color:#111>status</span><span style=color:#111>);</span>
</code></pre></div><p>Which waits until the message corresponding to <code>request</code> has been
completed.</p><p>Both of these calls can <em>complete</em> the message exchange. If <code>MPI_Test</code>
returns true in its flag argument, the message has been sent/received
and the user-provided send/receive buffer is safe to be used again.</p><p>Here&rsquo;s a picture of a non-blocking <code>MPI_Issend</code> matching with a
blocking <code>MPI_Recv</code>. Note how the data transfer does not start
(because this is a synchronous send) until the matching receive has
been posted (set up). So the first <code>MPI_Test</code> returns false. The
<code>MPI_Wait</code> will return immediately because the message has now been
transferred.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/mpi-issend-cartoon.svg alt="A non-blocking synchronous send returns immediately, and the data transfer begins as soon as the matching receive appears."><figcaption><p>A non-blocking synchronous send returns immediately, and the data transfer begins as soon as the matching receive appears.</p></figcaption></figure><h2 id=why-would-you-do-this>Why would you do this?
<a class=anchor href=#why-would-you-do-this>#</a></h2><p>Non-blocking messages allow us to separate &ldquo;posting&rdquo; messages
from when we check if they are completed. One reason to do this is
that MPI libraries often have optimisations to complete sends quickly
if the matching receive already exists.</p><p>If I am receiving messages from 10 different processes, if I use
a blocking <code>MPI_Recv</code>, then there is only ever one receive ready at
any one time. Conversely if I use <code>MPI_Irecv</code>, then all receives will
be ready, and the MPI library can complete them as the matching send
arrives.</p><p>It also allows us to simplify programs that exchange many messages if
we&rsquo;re trying to avoid deadlocks. We can just post all sends/receives
at once and then wait, rather than having to arrange that we have a
single send/receive ready at the right time.</p><p>Finally, non-blocking communication allows us to (in theory) <em>overlap</em>
communication with computation. This can help to improve scaling
performance in some cases.</p><p>As you probably saw when doing the <a href=https://teaching.wence.uk/phys52015/exercises/mpi-ping-pong/>ping-pong</a> exercise, all MPI messages have a non-zero
latency. That means that no matter how small it is, it takes some
time for a message to cross the network. If we use blocking messages,
the best case total time for our simulation is going to be</p><p>$$
T_{\text{compute}} + T_{\text{communicate}}
$$</p><p>Many scientific computing simulations have compute and communication
parts that can <em>overlap</em>. For example, when domain decomposing a mesh
for a parallel PDE solver, most of the computation can be done
without communicating with our neighbours: we only need information
when we&rsquo;re near the edge of our local domain. We can therefore often
split the simulation into phases:</p><ol><li>Send data to neighbours</li><li>Compute on local data that doesn&rsquo;t depend on neighbours</li><li>Receive data from neighbours</li><li>Compute on remaining local data</li></ol><p>If we use non-blocking messages, we can sometimes hide the latency in
steps (1) and (3), so that the total simulation time is now</p><p>$$
\max(T_{\text{compute}}, T_{\text{communicate}}) &lt; T_{\text{compute}} + T_{\text{communicate}}
$$</p><p>We will look at a concrete implementation of this idea when doing the
<a href=https://teaching.wence.uk/phys52015/exercises/mpi-stencil/>halo exchange</a> exercise.</p><h2 id=waiting-for-multiple-messages>Waiting for multiple messages
<a class=anchor href=#waiting-for-multiple-messages>#</a></h2><p>The advantage of the non-blocking communication mode becomes more
apparent when we look at waiting or testing for completion of multiple
messages simultaneously.</p><p>A typical pseudo-code with non-blocking communication might look
something like this</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#111>MPI_Request</span> <span style=color:#f92672>*</span><span style=color:#111>requests</span><span style=color:#111>;</span>

<span style=color:#111>nsend</span> <span style=color:#f92672>=</span> <span style=color:#111>...;</span>
<span style=color:#111>nrecv</span> <span style=color:#f92672>=</span> <span style=color:#111>...;</span>

<span style=color:#111>requests</span> <span style=color:#f92672>=</span> <span style=color:#111>malloc</span><span style=color:#111>((</span><span style=color:#111>nsend</span><span style=color:#f92672>+</span><span style=color:#111>nrecv</span><span style=color:#111>)</span><span style=color:#f92672>*</span><span style=color:#00a8c8>sizeof</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>requests</span><span style=color:#111>));</span>

<span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>nrecv</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Irecv</span><span style=color:#111>(...,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>requests</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]);</span>
<span style=color:#111>}</span>

<span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>nsend</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Isend</span><span style=color:#111>(...,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>requests</span><span style=color:#111>[</span><span style=color:#111>i</span> <span style=color:#f92672>+</span> <span style=color:#111>nrecv</span><span style=color:#111>]);</span>
<span style=color:#111>}</span>

<span style=color:#75715e>/* Some work that doesn&#39;t depend on the messages */</span>
</code></pre></div><p>Having done the work that doesn&rsquo;t depend on messages, we now need to
wait for message completion.</p><p>Perhaps we need all the messages to complete, in which case we can use
<a href=https://rookiehpc.com/mpi/docs/mpi_waitall.php><code>MPI_Waitall</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#111>MPI_Waitall</span><span style=color:#111>(</span><span style=color:#111>nsend</span><span style=color:#f92672>+</span><span style=color:#111>nrecv</span><span style=color:#111>,</span> <span style=color:#111>requests</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUSES_IGNORE</span><span style=color:#111>);</span>
</code></pre></div><p>This approach is preferred over a loop calling <code>MPI_Wait</code> on each
request, since the MPI implementation is free to process the arriving
messages in any order when we call <code>MPI_Waitall</code> which might speed
things up.</p><p>Perhaps we just want a message to have arrived, in which case we can
use <a href=https://rookiehpc.com/mpi/docs/mpi_waitany.php><code>MPI_Waitany</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#111>which</span><span style=color:#111>;</span>
<span style=color:#111>MPI_Waitany</span><span style=color:#111>(</span><span style=color:#111>nsend</span><span style=color:#f92672>+</span><span style=color:#111>nrecv</span><span style=color:#111>,</span> <span style=color:#111>requests</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>which</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUSES_IGNORE</span><span style=color:#111>);</span>
</code></pre></div><p>Now the <code>which</code> variable tells us which of the requests completed.</p><p>Finally, suppose we want to wait until <em>at least one</em> message has
completed, we can use
<a href=https://rookiehpc.com/mpi/docs/mpi_waitsome.php><code>MPI_Waitsome</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#f92672>*</span><span style=color:#111>indices</span> <span style=color:#f92672>=</span> <span style=color:#111>malloc</span><span style=color:#111>((</span><span style=color:#111>nsend</span><span style=color:#f92672>+</span><span style=color:#111>nrecv</span><span style=color:#111>)</span><span style=color:#f92672>*</span><span style=color:#00a8c8>sizeof</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>indices</span><span style=color:#111>));</span>
<span style=color:#00a8c8>int</span> <span style=color:#111>nfinished</span><span style=color:#111>;</span>
<span style=color:#111>MPI_Waitsome</span><span style=color:#111>(</span><span style=color:#111>nsend</span><span style=color:#f92672>+</span><span style=color:#111>nrecv</span><span style=color:#111>,</span> <span style=color:#111>requests</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>nfinished</span><span style=color:#111>,</span> <span style=color:#111>indices</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUSES_IGNORE</span><span style=color:#111>);</span>
<span style=color:#75715e>/* Now nfinished tells us how many requests are completed,
</span><span style=color:#75715e> * and indices[0..nfinished-1] tells us which requests they are */</span>
</code></pre></div><p>There are also matching
<a href=https://rookiehpc.com/mpi/docs/mpi_testall.php><code>MPI_Testall</code></a>,
<a href=https://rookiehpc.com/mpi/docs/mpi_testany.php><code>MPI_Testany</code></a>, and
<a href=https://rookiehpc.com/mpi/docs/mpi_testsome.php><code>MPI_Testsome</code></a>
calls which don&rsquo;t block for completion of the messages.</p><p>A high quality MPI implementation will provide optimised code for
these routines that is more efficient than a loop with
<code>MPI_Test</code>/<code>MPI_Wait</code> pairs.</p><blockquote class=exercise><h3>Exercise</h3><h3 id=gathering-data-from-every-process>Gathering data from every process</h3><p>Write an MPI code in which rank-0 gathers a message from every process
and places it in an array at a position corresponding to the rank of
the sender.</p><p>So if running with $P$ processes, rank-0 should allocate an array with
space for $P$ entries, and after collecting the messages.</p><p>Compare the performance of two versions.</p><ol><li>rank-0 uses a blocking <code>MPI_Recv</code> for all receives</li><li>rank-0 uses non-blocking <code>MPI_Irecv</code> followed by <code>MPI_Waitall</code>.</li></ol><p>Which performs better as a function of the total number of messages, $P$?</p><details><summary>Solution</summary><div class=markdown-inner><p>I&rsquo;ll sketch the core message exchange here, the full code is
implemented in <a href=https://teaching.wence.uk/phys52015/code/mpi-snippets/gather-from-all.c><code>mpi-snippets/gather-from-all.c</code></a>.</p><p>Suppose we want to gather a single <code>int</code> from each process.</p><p>Rank 0 should allocate space for <code>size</code> ints.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#f92672>*</span><span style=color:#111>recvbuf</span> <span style=color:#f92672>=</span> <span style=color:#111>NULL</span><span style=color:#111>;</span>
<span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>recvbuf</span> <span style=color:#f92672>=</span> <span style=color:#111>malloc</span><span style=color:#111>(</span><span style=color:#111>size</span> <span style=color:#f92672>*</span> <span style=color:#00a8c8>sizeof</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>recvbuf</span><span style=color:#111>));</span>
<span style=color:#111>}</span>
</code></pre></div><p>The blocking gather is straightforward</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>recvbuf</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>sendbuf</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>];</span>
  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>size</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#75715e>/* Receive from all ranks other than myself */</span>
    <span style=color:#111>MPI_Recv</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>(</span><span style=color:#111>recvbuf</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]),</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#111>i</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUS_IGNORE</span><span style=color:#111>);</span>
  <span style=color:#111>}</span>
<span style=color:#111>}</span> <span style=color:#00a8c8>else</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Ssend</span><span style=color:#111>(</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div><p>For the nonblocking gather, we just need to modify the receive side to
allocate some requests and then wait on them.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>if</span> <span style=color:#111>(</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Requests</span> <span style=color:#f92672>*</span><span style=color:#111>requests</span> <span style=color:#f92672>=</span> <span style=color:#111>malloc</span><span style=color:#111>((</span><span style=color:#111>size</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#f92672>*</span> <span style=color:#00a8c8>sizeof</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>requests</span><span style=color:#111>));</span>
  <span style=color:#111>recvbuf</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>sendbuf</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>];</span>
  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>size</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
    <span style=color:#75715e>/* Receive from all ranks other than myself */</span>
    <span style=color:#111>MPI_Irecv</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>(</span><span style=color:#111>recvbuf</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]),</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#111>i</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>(</span><span style=color:#111>requests</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>]));</span>
  <span style=color:#111>}</span>
  <span style=color:#75715e>/* After posting all receives, wait for completion. */</span>
  <span style=color:#111>MPI_Waitall</span><span style=color:#111>(</span><span style=color:#111>size</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>requests</span><span style=color:#111>,</span> <span style=color:#111>MPI_STATUSES_IGNORE</span><span style=color:#111>);</span>
  <span style=color:#111>free</span><span style=color:#111>(</span><span style=color:#111>requests</span><span style=color:#111>);</span>
<span style=color:#111>}</span> <span style=color:#00a8c8>else</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Ssend</span><span style=color:#111>(</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI_INT</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div><p>I leave it to you to compare the performance.</p></div></details></blockquote><h2 id=wildcards>Wildcard matching
<a class=anchor href=#wildcards>#</a></h2><p>So far, we&rsquo;ve always specified specific <code>source</code> and <code>tag</code> arguments in
the arguments to <code>MPI_Recv</code> and <code>MPI_Irecv</code>. MPI also provides us with
the option to say &ldquo;receive a message, I don&rsquo;t care who its from, or
what the tag is&rdquo;.</p><p>We do that by providing <code>MPI_ANY_SOURCE</code> and/or <code>MPI_ANY_TAG</code> as the
source and tag arguments respectively.</p><p>We can subsequently, find out where we got the message from, and what
its tag was, by inspecting the <code>status</code> object that <code>MPI_Recv</code>
returns.</p><p>Up to now, we&rsquo;ve just said <code>MPI_STATUS_IGNORE</code>, but we can also do</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#111>MPI_Status</span> <span style=color:#111>status</span><span style=color:#111>;</span>
<span style=color:#111>MPI_Recv</span><span style=color:#111>(...,</span> <span style=color:#111>MPI_ANY_SOURCE</span><span style=color:#111>,</span> <span style=color:#111>MPI_ANY_TAG</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>status</span><span style=color:#111>);</span>

<span style=color:#111>status</span><span style=color:#111>.</span><span style=color:#111>MPI_SOURCE</span><span style=color:#111>;</span> <span style=color:#75715e>/* The source rank */</span>
<span style=color:#111>status</span><span style=color:#111>.</span><span style=color:#111>MPI_TAG</span><span style=color:#111>;</span> <span style=color:#75715e>/* The tag */</span>
</code></pre></div><p>There actually aren&rsquo;t that many reasons you would use wildcards in
receives. They can be useful when implementing <a href="http://htor.inf.ethz.ch/publications/index.php?pub=99">dynamic sparse data
exchange</a>.</p><blockquote class="book-hint info">Typically, the implementation of &ldquo;wildcard&rdquo; matching is less efficient
than message matching with given source and tag arguments.</blockquote><h2 id=summary>Summary
<a class=anchor href=#summary>#</a></h2><p>As well as providing blocking send/receive options, MPI provides
non-blocking versions.</p><p>These allow us to potentially improve performance of message exchange,
and simplify writing algorithms that need to match many pairs of
messages, without thinking as hard about potential deadlocks.</p><p>The critical thing to recall is that <strong>we are not allowed</strong> to look at
the buffers we pass into non-blocking sends/receives until after
calling a blocking <code>MPI_Wait</code>-like call, or a non-blocking
<code>MPI_Test</code>-like call has returned true.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/notes/mpi/point-to-point-nb.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#why-would-you-do-this>Why would you do this?</a></li><li><a href=#waiting-for-multiple-messages>Waiting for multiple messages</a></li><li><a href=#wildcards>Wildcard matching</a></li><li><a href=#summary>Summary</a></li></ul></nav></aside></main></body></html>