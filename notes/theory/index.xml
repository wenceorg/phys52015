<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Theory &amp; concepts on PHYS52015 – Introduction to HPC</title><link>https://teaching.wence.uk/phys52015/notes/theory/</link><description>Recent content in Theory &amp; concepts on PHYS52015 – Introduction to HPC</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://teaching.wence.uk/phys52015/notes/theory/index.xml" rel="self" type="application/rss+xml"/><item><title>Parallel scaling laws</title><link>https://teaching.wence.uk/phys52015/notes/theory/scaling-laws/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/theory/scaling-laws/</guid><description>Scaling laws (models) # Suppose we have a simulation that has been parallelised. What should we expect of its performance when we run it in parallel? Answering this question will allow us to determine an appropriate level of parallelism to use when running the simulation. It can also help us determine how much effort to put into parallelising a serial code, or improving an existing parallel code.
Some simple examples1 # The type of parallelism we will cover in this course is that where we, as programmers, are explicitly in control of what is going on.</description></item><item><title>Parallelism in hardware: an overview</title><link>https://teaching.wence.uk/phys52015/notes/theory/hardware-parallelism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/theory/hardware-parallelism/</guid><description>Supercomputer architecture # When thinking about the implementation of parallel programming, it is helpful to have at least a high-level idea of what kinds of parallelism are available in the hardware. The different levels of parallelism available in the hardware then map onto (sometimes) different programming models.
Distributed memory parallelism # Modern supercomputers are all massively parallel, distributed memory, systems. That is, they consist of many processors linked by some network, but do not share a single memory system.</description></item><item><title>Parallel patterns</title><link>https://teaching.wence.uk/phys52015/notes/theory/concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/theory/concepts/</guid><description>Overview of parallel patterns # So far, we&amp;rsquo;ve seen that to run large simulations, and exploit all of the hardware available to us on supercomputers (but even on laptops and phones), we will need to use parallelism of some kind.
We&amp;rsquo;ve also looked at the levels of parallelism exposed by modern hardware, and noted that there are effectively three levels.
Now we&amp;rsquo;re going to look at the types of parallelism (or parallel patterns) that we might encounter in software.</description></item></channel></rss>