<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Supercomputer architecture #  When thinking about the implementation of parallel programming, it is helpful to have at least a high-level idea of what kinds of parallelism are available in the hardware. The different levels of parallelism available in the hardware then map onto (sometimes) different programming models.
Distributed memory parallelism #  Modern supercomputers are all massively parallel, distributed memory, systems. That is, they consist of many processors linked by some network, but do not share a single memory system."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Parallelism in hardware: an overview"><meta property="og:description" content="Supercomputer architecture #  When thinking about the implementation of parallel programming, it is helpful to have at least a high-level idea of what kinds of parallelism are available in the hardware. The different levels of parallelism available in the hardware then map onto (sometimes) different programming models.
Distributed memory parallelism #  Modern supercomputers are all massively parallel, distributed memory, systems. That is, they consist of many processors linked by some network, but do not share a single memory system."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/notes/theory/hardware-parallelism/"><meta property="article:modified_time" content="2022-04-07T18:13:40+01:00"><title>Parallelism in hardware: an overview | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/ class=active>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Parallelism in hardware: an overview</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#distributed-memory-parallelism>Distributed memory parallelism</a></li><li><a href=#shared-memory>Shared memory parallelism</a></li><li><a href=#instruction-parallelism>Instruction parallelism</a></li><li><a href=#summary>Summary</a></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=supercomputer-architecture>Supercomputer architecture
<a class=anchor href=#supercomputer-architecture>#</a></h1><p>When thinking about the implementation of parallel programming, it is
helpful to have at least a high-level idea of what kinds of
parallelism are available in the hardware. The different levels of
parallelism available in the hardware then map onto (sometimes)
different programming models.</p><h2 id=distributed-memory-parallelism>Distributed memory parallelism
<a class=anchor href=#distributed-memory-parallelism>#</a></h2><p>Modern supercomputers are all massively parallel, distributed memory,
systems. That is, they consist of many processors linked by some
network, but do not share a single memory system.</p><p>That is, we have a (large) number of computers, often called <em>compute
nodes</em> linked by some network.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/networked-compute-nodes.svg alt="A small number of compute nodes in a 2D network."><figcaption><p>A small number of compute nodes in a 2D network.</p></figcaption></figure><p>Of course, real supercomputers have many more nodes. For example the
present (October 2020) fastest super computer in the world,
<a href=https://blog.global.fujitsu.com/fgb/2020-06-22/supercomputer-fugaku-named-world-fastest/>Fugaku</a>
consists of around 400 racks, each containing 384 compute nodes.</p><figure style=width:75%><a href=https://blog.global.fujitsu.com/fgb/2020-06-22/supercomputer-fugaku-named-world-fastest/><img class=scaled src="https://images2.newscred.com/Zz1iYjIwMDk4NGI0OTYxMWVhYTdlZjBhZGYxMWZiZjY5NQ==" alt="Fugaku machine room."></a><figcaption><p>Fugaku machine room.</p></figcaption></figure><p>These different compute nodes do not share a memory space, and so we
must use a <em>distributed memory</em> programming model to address the
parallelism they offer.</p><h2 id=shared-memory>Shared memory parallelism
<a class=anchor href=#shared-memory>#</a></h2><p>Zooming in to a single compute node, we still find multiple levels of
parallelism here. As we saw when introducing <a href=https://teaching.wence.uk/phys52015/notes/introduction/#moores-law>Moore&rsquo;s law</a>, despite single-core performance
flatlining, transistor counts on chips still do double every 18-24
months.</p><p>This exhibits firstly by an increase in the number of CPU cores on any
single chip. Rather than having a single CPU attached to some main
memory, we might have between four and 64. This is an example of a
<em>shared memory</em> system: the different compute units (CPUs) have access
to the same piece of shared memory. Sometimes, as sketched in the
figure below, there are actually multiple chips all with their
attached memory. This is an example of a shared memory compute node
with four <em>sockets</em>.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/numa-socket-sketch.svg alt="Sketch of a shared memory chip with four sockets."><figcaption><p>Sketch of a shared memory chip with four sockets.</p></figcaption></figure><p>We can usually program such systems without thinking too hard about
the sockets, but we sometimes have to pay attention to where in the
memory the data we are computing with resides.</p><h2 id=instruction-parallelism>Instruction parallelism
<a class=anchor href=#instruction-parallelism>#</a></h2><p>We&rsquo;re not quite done in our tour of hardware parallelism. If we now
zoom in further to an individual CPU, there is parallelism available
here in the instructions that the CPU executes when running our
programs. Although there are multiple tricks hardware designers utilise
to get the most out of all the transistors on offer, we&rsquo;ll focus just
one, namely <em>vector instructions</em>.</p><blockquote class="book-hint info">There are other forms of parallelism available on individual CPUs,
such as pipelining, superscalar, and out-of-order execution. But these
are typically best left to the compiler and hardware, and harder to
express in high level code, so we will gloss over them in this course.</blockquote><p>Vectorisation is an example of <a href=https://en.wikipedia.org/wiki/SIMD><abbr title="Single Instruction
Multiple Data">SIMD</abbr></a>
parallelism. It is the application of the <em>same</em> operation
simultaneously to a number of data items. A canonical example of this
is operations on arrays of data.</p><p>For example, the loop</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#111>size_t</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>n</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span>
  <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>b</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>+</span> <span style=color:#111>c</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
</code></pre></div><p>might be implemented by \(n\) scalar addition operations, or
\(\frac{n}{4}\) vector addition operations (each performing four
scalar additions).</p><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner"><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/scalaradd.svg alt="Scalar addition, one element at a time"><figcaption><p>Scalar addition, one element at a time</p></figcaption></figure></div><div class="flex-even markdown-inner"><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/vectoradd.svg alt="Vector addition, four elements at a time"><figcaption><p>Vector addition, four elements at a time</p></figcaption></figure></div></div><p>The rationale for this choice is that it costs about the same
power to decode and emit a vector instruction as it does a scalar
instruction, but we get four times the throughput! Since we can no
longer increase the clock frequency of chips, this is a useful
workaround.</p><h2 id=summary>Summary
<a class=anchor href=#summary>#</a></h2><p>Parallelism in modern supercomputers spans many scales, from SIMD
instruction-level parallelism in which each CPU core might operate on
4-16 floating point values in lockstep, through shared memory
parallelism where we might have around 64 CPU cores sharing memory, up
to large distributed memory systems where the only limiting factors are
the capital budget and ongoing power and cooling costs.</p><p>At the present time, there is no one established programming model
that covers all of these levels, in this course we will look at how to
program for <a href=https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/>shared memory</a>, and <a href=https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/>distributed memory</a> parallelism.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/notes/theory/hardware-parallelism.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#distributed-memory-parallelism>Distributed memory parallelism</a></li><li><a href=#shared-memory>Shared memory parallelism</a></li><li><a href=#instruction-parallelism>Instruction parallelism</a></li><li><a href=#summary>Summary</a></li></ul></nav></aside></main></body></html>