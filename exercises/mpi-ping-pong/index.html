<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Measuring point-to-point message latency with ping-pong #  In this exercise we will write a simple code that does a message ping-pong: sending a message back and forth between two processes.
We can use this to measure both the latency and bandwidth of the network on our supercomputer. Which are both important measurements when we&rsquo;re looking at potential parallel performance: they help us to decide if our code is running slowly because of our bad choices, or limitations in the hardware."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="MPI: ping-pong latency"><meta property="og:description" content="Measuring point-to-point message latency with ping-pong #  In this exercise we will write a simple code that does a message ping-pong: sending a message back and forth between two processes.
We can use this to measure both the latency and bandwidth of the network on our supercomputer. Which are both important measurements when we&rsquo;re looking at potential parallel performance: they help us to decide if our code is running slowly because of our bad choices, or limitations in the hardware."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/exercises/mpi-ping-pong/"><meta property="article:modified_time" content="2022-04-07T18:13:40+01:00"><title>MPI: ping-pong latency | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/ class=active>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>MPI: ping-pong latency</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#a-model-for-the-time-to-send-a-message>A model for the time to send a message</a></li><li><a href=#implementation>Implementation</a></li><li><a href=#experiment>Experiment</a></li><li><a href=#advanced-variability>Advanced: variability</a><ul><li><a href=#network-contention>Network contention</a></li></ul></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=measuring-point-to-point-message-latency-with-ping-pong>Measuring point-to-point message latency with ping-pong
<a class=anchor href=#measuring-point-to-point-message-latency-with-ping-pong>#</a></h1><p>In this exercise we will write a simple code that does a message
ping-pong: sending a message back and forth between two processes.</p><p>We can use this to measure both the <em>latency</em> and <em>bandwidth</em> of the
network on our supercomputer. Which are both important measurements
when we&rsquo;re looking at potential parallel performance: they help us to
decide if our code is running slowly because of our bad choices, or
limitations in the hardware.</p><h2 id=a-model-for-the-time-to-send-a-message>A model for the time to send a message
<a class=anchor href=#a-model-for-the-time-to-send-a-message>#</a></h2><p>We care about the total time it takes to send a message, our model is
a linear model which has two free parameters:</p><ol><li>$\alpha$, the message latency, measured in seconds;</li><li>$\beta$, the inverse network bandwidth, measured in seconds/byte
(so that the bandwidth is $\beta^{-1}$.</li></ol><p>With this model, the time to send a message with $b$ bytes is</p><p>$$
T(b) = \alpha + \beta b
$$</p><h2 id=implementation>Implementation
<a class=anchor href=#implementation>#</a></h2><p>I provide a template in <a href=https://teaching.wence.uk/phys52015/code/mpi/ping-pong/ping-pong.c><code>mpi/ping-pong/ping-pong.c</code></a> in the <code>code/mpi/ping-pong</code>
subdirectory of the <a href=https://github.com/wenceorg/phys52015>repository</a> that you can compile
with <code>mpicc</code>. It takes one argument, the size of the message (in
bytes) to exchange.</p><p>You should implement the <code>ping_pong</code> function which should send a
message of the given size from rank 0 to rank 1, after which rank 1
should send the same message back to rank 0. Ensure that the code also
works with more than two processes (all other ranks should just do
nothing).</p><p>Add timing around the <code>ping_pong</code> call to determine how long it takes
to send these messages.</p><details><summary>Hint</summary><div class=markdown-inner><p>Use <a href=https://rookiehpc.com/mpi/docs/mpi_wtime.php><code>MPI_Wtime()</code></a> for
timing.</p><p>For small messages you will probably need to do many ping-pong
iterations in a loop to get accurate timings.</p></div></details><h2 id=experiment>Experiment
<a class=anchor href=#experiment>#</a></h2><blockquote class=exercise><h3>Exercise</h3><p>Run your code on the Hamilton compute nodes for a range of messages
sizes from one byte to 64MB.</p><p>Produce a plot of the time to send a message as a function of message
size.</p><p>Using
<a href=https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html><code>numpy.polyfit</code></a>
(or your favourite linear regression scheme), fit our proposed model
to your data.</p><p>What values of $\alpha$ and $\beta$ do you get?</p><details><summary>Solution</summary><div class=markdown-inner><p>I provide a sample solution in
<a href=https://teaching.wence.uk/phys52015/code/mpi/ping-pong/ping-pong-solution.c><code>mpi/ping-pong/ping-pong-solution.c</code></a>. When I run it on one node
using messages ranging from 1 Byte to 16MB (in powers of 2), I get
something like the below.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/auto/ping-pong-timing.svg alt="Ping pong time and model fit on Hamilton v6 (par6.q queue), this is the older hardware"><figcaption><p>Ping pong time and model fit on Hamilton v6 (<code>par6.q</code> queue), this is the older hardware</p></figcaption></figure><p>It looks like a piecewise linear model is right for this MPI
implementation. Between 512KB and 1MB, the time jumps up. This is
probably when the implementation is switching protocols.</p><p>I get a latency of $\alpha_\text{intra} \approx 500\text{ns}$, and an inverse
bandwidth of $\beta \approx 4.5\times 10^{-10}\text{s/byte}$ or a
bandwidth of around $2\text{GB/s}$.</p></div></details></blockquote><blockquote class=question><h3>Question</h3><span><p>Perform the same experiment, but this time, place the two processes on
<em>different</em> Hamilton compute nodes. Do you observe a difference in the
performance?</p><p>To do this, you&rsquo;ll need to write a SLURM batch script that specifies</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh><span style=color:#75715e># Two nodes</span>
<span style=color:#75715e>#SBATCH --nodes=2</span>
<span style=color:#75715e># One process per node</span>
<span style=color:#75715e>#SBTACH --ntasks-per-node=1</span>
</code></pre></div><details><summary>Solution</summary><div class=markdown-inner><p>If I do this, I see that the inter-node latency on Hamilton 6 is
pretty bad, although asymptotically it seems like the bandwidth is the
same as for inter-node. This time I ran out to messages of 64MB. The
slow message at 32MB appears to be repeatable, but I don&rsquo;t understand
what is going on.</p><p>Notice that when going across nodes, the switch in protocol happens at
a lower size (it looks like 256KB, rather than 1MB).</p><p>The plot also has results for Hamilton 7 which performs somewhat
better.</p><figure style=width:60%><img class=scaled src=https://teaching.wence.uk/phys52015/images/auto/ping-pong-timing-lots.svg alt="Ping pong latency on Hamilton 6 and Hamilton 7."><figcaption><p>Ping pong latency on Hamilton 6 and Hamilton 7.</p></figcaption></figure><p>If I fit our linear model to the inter-node data, I get
$\alpha_\text{inter} \approx 6\mu\text{s} \approx 10
\alpha_\text{intra}$. The inverse bandwidth is about the same,
resulting in a network bandwidth of around $2\text{GB/s}$.</p><p>Fitting the model to the Hamilton 7 data, the intra-node latency is
still around $500\text{ns}$, but now the asymptotic bandwidth is
around $6.5\text{GB/s}$, so the network is much better inside a node.
However, we see that between nodes, it performs in a similar manner to
the Hamilton 6 case.</p></div></details></span></blockquote><h2 id=advanced-variability>Advanced: variability
<a class=anchor href=#advanced-variability>#</a></h2><blockquote class="book-hint info"><p>This section is optional, but possibly interesting.</p><details><summary>Solution</summary><div class=markdown-inner>Solutions for this section are left as an exercise.</div></details></blockquote><p>One thing that can affect performance of real MPI codes is the message
latency, and particularly if there is any variability. This might be
affected by other processes that happen to be using the network, or
our own code, or operating system level variability. We&rsquo;ll see if we
can observe any on Hamilton.</p><p>Modify your code so that rather than just timing many ping-pong
iterations, it records the time for each of the many iterations
separately.</p><p>Use this information to compute the mean ping-pong time, along with
the standard deviation and the minimum and maximum times.</p><details><summary>Hint</summary><div class=markdown-inner><p>You can allocate an array for your timing data with</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#111>nrepeats</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span><span style=color:#111>;</span> <span style=color:#75715e>/* Or some approriate number */</span>
<span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#111>timing</span> <span style=color:#f92672>=</span> <span style=color:#111>malloc</span><span style=color:#111>(</span><span style=color:#111>nrepeats</span> <span style=color:#f92672>*</span> <span style=color:#00a8c8>sizeof</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#00a8c8>double</span><span style=color:#111>));</span>

<span style=color:#111>...;</span>
<span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>nrepeats</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
   <span style=color:#111>start</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI_Wtime</span><span style=color:#111>();</span>
   <span style=color:#111>ping_pong</span><span style=color:#111>(...);</span>
   <span style=color:#111>end</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI_Wtime</span><span style=color:#111>();</span>
   <span style=color:#111>timing</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>end</span> <span style=color:#f92672>-</span> <span style=color:#111>start</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
<span style=color:#111>...;</span> <span style=color:#75715e>/* Compute statistics */</span>

<span style=color:#111>free</span><span style=color:#111>(</span><span style=color:#111>timing</span><span style=color:#111>);</span> <span style=color:#75715e>/* Don&#39;t forget to release the memory! */</span>
</code></pre></div></div></details><p>Produce a plot of these data, using the standard deviation as error
bars and additionally showing the minimum and maximum times as
outliers.</p><blockquote class=question><h3>Question</h3><span><p>What, if any, variability do you observe?</p><p>Does it change if you move from a single compute node to two nodes?</p></span></blockquote><h3 id=network-contention>Network contention
<a class=anchor href=#network-contention>#</a></h3><p>Finally, we&rsquo;ll look at whether having more messages &ldquo;in flight&rdquo; at
once effects performance.</p><p>Rather than running with two processes, you should run with full
compute nodes (24 processes per node).</p><p>Modify your ping-pong code so that all ranks participate in pairwise
messaging.</p><p>Divide the processes into a &ldquo;bottom&rdquo; and &ldquo;top&rdquo; half. Suppose we are
using <code>size</code> processes in total. Processes with <code>rank &lt; size/2</code> are in
the &ldquo;bottom&rdquo; half, the remainder are in the &ldquo;top&rdquo; half.</p><p>A process in the bottom half should send a message to its matching
pair in the top half (<code>rank + size/2</code>), that process should then
return the message (to <code>rank - size/2</code>).</p><p>Again measure time and variability, and produce a plot.</p><blockquote class=question><h3>Question</h3><span><p>Do the results change from previously?</p><ol><li>When using one compute node?</li><li>When using two?</li><li>When using four?</li></ol></span></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/exercises/mpi-ping-pong.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#a-model-for-the-time-to-send-a-message>A model for the time to send a message</a></li><li><a href=#implementation>Implementation</a></li><li><a href=#experiment>Experiment</a></li><li><a href=#advanced-variability>Advanced: variability</a><ul><li><a href=#network-contention>Network contention</a></li></ul></li></ul></nav></aside></main></body></html>