<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Exercises on PHYS52015 – Introduction to HPC</title><link>https://teaching.wence.uk/phys52015/exercises/</link><description>Recent content in Exercises on PHYS52015 – Introduction to HPC</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://teaching.wence.uk/phys52015/exercises/index.xml" rel="self" type="application/rss+xml"/><item><title>Parallel Hello World</title><link>https://teaching.wence.uk/phys52015/exercises/hello/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/hello/</guid><description>Hello, World! # As with every programming course, the first thing we will do is compile and run a &amp;ldquo;Hello world&amp;rdquo; program. Actually we&amp;rsquo;ll do three. The goal of this is to familiarise you with the module system on Hamilton, as well as how to compile code. So take a look at the quickstart guide if you haven&amp;rsquo;t already.
A serial version # Log in to Hamilton/COSMA load the relevant compiler modules</description></item><item><title>OpenMP: parallel loops</title><link>https://teaching.wence.uk/phys52015/exercises/openmp-loop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/openmp-loop/</guid><description>Parallelisation of a simple loop # As usual, we&amp;rsquo;ll be running these exercises on Hamilton or COSMA, so remind yourself of how to log in and transfer code if you need to.
Obtaining the code # The code for this exercise is in the code/add_numbers subdirectory of the repository.
We&amp;rsquo;ll be working in the openmp subdirectory.
Working from the repository If you have committed your changes on branches for the previous exercises, just checkout the main branch again and create a new branch for this exercise.</description></item><item><title>OpenMP: stencils</title><link>https://teaching.wence.uk/phys52015/exercises/openmp-stencil/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/openmp-stencil/</guid><description>Parallelisation of a simple stencil # We&amp;rsquo;ll be running these exercises on Hamilton or COSMA, so remind yourself of how to log in and transfer code if you need to.
Blurring an image # One can blur or smooth the edges of an image by convolving the image with a normalised box kernel. Every output pixel \( g_{k, l} \) is created from the mean of the input image pixel \(f _{k, l}\) and its eight neighbours.</description></item><item><title>OpenMP: synchronisation</title><link>https://teaching.wence.uk/phys52015/exercises/openmp-reduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/openmp-reduction/</guid><description>Avoiding data-races in updates to shared memory # In this exercise, we&amp;rsquo;ll use the synchronisation constructs we encountered when looking at OpenMP collectives to implement different approaches to combining the partial sums in a reduction.
We&amp;rsquo;ll then also benchmark the performance of the different approaches to see if there are any differences
Template code and benchmarking # As usual, the starter code lives in the repository, in the code/openmp-snippets subdirectory.</description></item><item><title>MPI: messages round a ring</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-ring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-ring/</guid><description>Sending messages around a ring # In this exercise we&amp;rsquo;ll write a simple form of global reduction. We will set up the processes in a ring (so each process has a left and right neighbour) and each process should initialise a buffer to its rank.
To compute a global summation a simple method is to rotate each piece of data all the way round the ring: at each step, a process receives from the left and sends to the right.</description></item><item><title>MPI: Calculating π</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-pi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-pi/</guid><description>Simple MPI parallelism # In this exercise we&amp;rsquo;re going to compute an approximation to the value of π using a simple Monte Carlo method. We do this by noticing that if we randomly throw darts at a square, the fraction of the time they will fall within the incircle approaches π.
Consider a square with side-length \(2r\) and an inscribed circle with radius \(r\).
Square with inscribed circle</description></item><item><title>MPI: ping-pong latency</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-ping-pong/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-ping-pong/</guid><description>Measuring point-to-point message latency with ping-pong # In this exercise we will write a simple code that does a message ping-pong: sending a message back and forth between two processes.
We can use this to measure both the latency and bandwidth of the network on our supercomputer. Which are both important measurements when we&amp;rsquo;re looking at potential parallel performance: they help us to decide if our code is running slowly because of our bad choices, or limitations in the hardware.</description></item><item><title>MPI: simple collectives</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-collectives/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-collectives/</guid><description>Manipulating matrices with collectives # In this exercise, we&amp;rsquo;ll look at some simple manipulations of matrices with collective operations.
We&amp;rsquo;ll do this with square matrices, where the number of rows (and columns) is equal to the number of processes.
We&amp;rsquo;ll implement three routines:
printing a matrix by gathering it to a single process and printing there; transposing a matrix; multiplying a matrix against a distributed vector. For a data distribution, we will use a one-dimensional distribution where each process holds one row.</description></item><item><title>MPI: domain decomposition and halo exchanges</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-stencil/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-stencil/</guid><description>Data parallel stencil computations # In this exercise we&amp;rsquo;re going to explore an MPI-parallelisation of a simple image processing problem.
In particular, we are going to reconstruct an image from its edges.
Introduction and background # A particularly simple way of detecting the edges in an image is to convolve it with a Laplacian kernel. That is, given some image $I$, we can obtain its edges with
$$ E = \nabla^2 I.</description></item></channel></rss>