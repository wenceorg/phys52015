<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Avoiding data-races in updates to shared memory #  In this exercise, we&rsquo;ll use the synchronisation constructs we encountered when looking at OpenMP collectives to implement different approaches to combining the partial sums in a reduction.
We&rsquo;ll then also benchmark the performance of the different approaches to see if there are any differences
Template code and benchmarking #  As usual, the starter code lives in the repository, in the code/openmp-snippets subdirectory."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="OpenMP: synchronisation"><meta property="og:description" content="Avoiding data-races in updates to shared memory #  In this exercise, we&rsquo;ll use the synchronisation constructs we encountered when looking at OpenMP collectives to implement different approaches to combining the partial sums in a reduction.
We&rsquo;ll then also benchmark the performance of the different approaches to see if there are any differences
Template code and benchmarking #  As usual, the starter code lives in the repository, in the code/openmp-snippets subdirectory."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/exercises/openmp-reduction/"><meta property="article:modified_time" content="2022-04-07T18:13:40+01:00"><title>OpenMP: synchronisation | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/ class=active>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>OpenMP: synchronisation</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#template-code-and-benchmarking>Template code and benchmarking</a></li><li><a href=#more-details-thread-placement>More details: thread placement</a></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=avoiding-data-races-in-updates-to-shared-memory>Avoiding data-races in updates to shared memory
<a class=anchor href=#avoiding-data-races-in-updates-to-shared-memory>#</a></h1><p>In this exercise, we&rsquo;ll use the synchronisation constructs we
encountered when looking at <a href=https://teaching.wence.uk/phys52015/notes/openmp/collectives/>OpenMP collectives</a> to implement different approaches
to combining the partial sums in a reduction.</p><p>We&rsquo;ll then also benchmark the performance of the different approaches
to see if there are any differences</p><h2 id=template-code-and-benchmarking>Template code and benchmarking
<a class=anchor href=#template-code-and-benchmarking>#</a></h2><p>As usual, the starter code lives in the <a href=https://github.com/wenceorg/phys52015>repository</a>, in
the <code>code/openmp-snippets</code> subdirectory.
In <a href=https://teaching.wence.uk/phys52015/code/openmp-snippets/reduction-template.c><code>reduction-template.c</code></a> is some code that times
how long it takes to run the reduction.</p><p>You can select the length of the vector to compute the dot product of
by passing a size on the commandline. For example, after compiling with</p><pre><code>$ icc -qopenmp -o reduction-template reduction-template.c
</code></pre><p>You can run, for example, with</p><pre><code>$ OMP_NUM_THREADS=2 ./reduction-template 1000000
</code></pre><p>The implementation of the parallel reduction is left up to you.</p><p>You should implement a correct reduction using the four different
approaches listed in the code:</p><ol><li>&ldquo;By hand&rdquo;, using the same kind of approach as in
<code>openmp-snippets/reduction-hand.c</code>;</li><li>Using an atomic directive to protect the shared updates;</li><li>Using a critical section to protect the shared updates;</li><li>Using the reduction clause on a parallel loop.</li></ol><details><summary>Solution</summary><div class=markdown-inner><p>I implemented these approaches in
<a href=https://teaching.wence.uk/phys52015/code/code/openmp-snippets/reduction-different-approaches.c><code>code/openmp-snippets/reduction-different-approaches.c</code></a>. The
crucial thing to note with the critical section and atomics is to
compute partial dotproducts in private variables and then protect the
single shared update.</p><p>That is, do this</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp for
</span><span style=color:#75715e></span><span style=color:#00a8c8>for</span> <span style=color:#111>(</span> <span style=color:#111>...</span> <span style=color:#111>)</span> <span style=color:#111>{</span>
  <span style=color:#111>mydot</span> <span style=color:#f92672>+=</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span><span style=color:#f92672>*</span><span style=color:#111>b</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
<span style=color:#111>}</span>
<span style=color:#75715e>#pragma omp atomic
</span><span style=color:#75715e></span><span style=color:#111>dotab</span> <span style=color:#f92672>+=</span> <span style=color:#111>mydot</span><span style=color:#111>;</span>
</code></pre></div><p>Rather than this</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp for
</span><span style=color:#75715e></span><span style=color:#00a8c8>for</span> <span style=color:#111>(</span> <span style=color:#111>...</span> <span style=color:#111>)</span> <span style=color:#111>{</span>
<span style=color:#75715e>#pragma omp atomic
</span><span style=color:#75715e></span>  <span style=color:#111>dotab</span> <span style=color:#f92672>+=</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span><span style=color:#f92672>*</span><span style=color:#111>b</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
<span style=color:#111>}</span>
</code></pre></div><p>Since the latter does far more locking and synchronisation than
necessary.</p></div></details><blockquote class=exercise><h3>Exercise</h3><p>For each of the approaches, benchmark the time it takes to run the
reduction for a vector with 1 billion entries across a range of
threads, from one up to 48 threads.</p><p>Produce plots of the <a href=https://teaching.wence.uk/phys52015/notes/theory/scaling-laws/>parallel scaling</a>
and parallel efficiency of the different approaches.</p><details><summary>Solution</summary><div class=markdown-inner><p>I did this with a vector with 500 million entries, and only ran up to
24 threads, because I didn&rsquo;t want to wait that long, but the results
will be broadly the same.</p><p>This time I plot time to solution against number of threads on a
<a href=https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.semilogy.html><code>semilogy</code></a>
axis.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/auto/openmp-reduction-approaches.svg alt="Time to solution (lower is better) for a vector dot product as a function of number of threads."><figcaption><p>Time to solution (lower is better) for a vector dot product as a function of number of threads.</p></figcaption></figure><p>It looks like all the solutions are broadly identical in performance.</p><p>We can also plot the efficiency which tells a similar story</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/auto/openmp-reduction-approaches-efficiency.sv alt="Parallel efficiency (for strong scaling) for a vector dot product."><figcaption><p>Parallel efficiency (for strong scaling) for a vector dot product.</p></figcaption></figure><p>It appears for these benchmarks that the &ldquo;by hand&rdquo; approach is
<em>marginally</em> better, but we would have to do more detailed
benchmarking to be confident.</p></div></details></blockquote><blockquote class=question><h3>Question</h3><span><p>Which approach works best?</p><p>Which approach works worst?</p><p>Do you observe perfect scalability? That is, is the speedup linear in
the number of threads?</p><details><summary>Solution</summary><div class=markdown-inner>As we see in the plots above, it looks like all the approaches are
basically equally good for this test. The speedup is far from perfect,
this is because the amount of memory bandwidth that the Hamilton
compute node delivers does not scale linearly in the number of cores.
So once we&rsquo;ve got to around 8 cores, we&rsquo;re already using all of the
memory bandwidth, and adding more cores doesn&rsquo;t get us the answer
faster.</div></details></span></blockquote><h2 id=more-details-thread-placement>More details: thread placement
<a class=anchor href=#more-details-thread-placement>#</a></h2><p>The Hamilton compute nodes are dual-socket. That is, they have two
chips in a single motherboard, each with its own memory attached.
Although OpenMP treats this logically as a single piece of shared
memory, the performance of the code depends on where the memory
accessed is relative to where the threads are.</p><p>We will now briefly investigate this.</p><p>OpenMP exposes some extra environment variables that control where
threads are physically placed on the hardware. We&rsquo;ll look at how these
affect the performance of our reduction.</p><p>Use the implementation with the reduction clause, we hope it is the
most efficient!</p><p>The relevant environment variables for controlling placement are
<code>OMP_PROC_BIND</code> and <code>OMP_PLACES</code>. We need to set <code>OMP_PLACES=cores</code></p><pre><code>$ export OMP_PLACES=cores
</code></pre><details><summary>Hint</summary><div class=markdown-inner>Don&rsquo;t forget to do this in your submission script.</div></details><blockquote class=exercise><h3>Exercise</h3><p>We&rsquo;ll now look at the difference in scaling performance when using
different values for <code>OMP_PROC_BIND</code>.</p><p>As before, run with a vector with 1 billion entries, from one to 48
threads, and produce scaling plots.</p><p>First, use</p><pre><code>OMP_PROC_BIND=close
</code></pre><p>This places threads on cores that are physically close to one another,
first filling up the first socket, and then the second.</p><p>Compare this to results obtained with</p><pre><code>OMP_PROC_BIND=spread
</code></pre><p>This spreads threads out between cores, thread 0 to the first socket,
thread 1 to the second, and so on. I think this is the default setting
when using the Intel OpenMP library.</p><p>Which approach works better? Is there a difference at all?</p><details><summary>Solution</summary><div class=markdown-inner><p>I just do this for the reduction clause case, since we determined that
everything is basically the same for the other versions.</p><p>I do this (again running with a vector size of 500 million, but
the results will be broadly the same for 1 billion entries I suspect),
and find that when I fill up all the cores, the speedup looks pretty
much identical.</p><p>However, at interim thread counts, <code>OMP_PROC_BIND=spread</code> produces
better performance.</p><p>This is because it places threads on both sockets on the compute node,
maximising the amount of memory bandwidth that the program can obtain.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/auto/openmp-proc-bind-reduction.svg alt="The reduction performs better when using an intermediate number of threads if we specify a spread distribution."><figcaption><p>The reduction performs better when using an intermediate number of threads if we specify a <code>spread</code> distribution.</p></figcaption></figure></div></details></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/exercises/openmp-reduction.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#template-code-and-benchmarking>Template code and benchmarking</a></li><li><a href=#more-details-thread-placement>More details: thread placement</a></li></ul></nav></aside></main></body></html>