<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Parallelisation of a simple loop #  As usual, we&rsquo;ll be running these exercises on Hamilton or COSMA, so remind yourself of how to log in and transfer code if you need to.
Obtaining the code #  The code for this exercise is in the code/add_numbers subdirectory of the repository.
We&rsquo;ll be working in the openmp subdirectory.
Working from the repository If you have committed your changes on branches for the previous exercises, just checkout the main branch again and create a new branch for this exercise."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="OpenMP: parallel loops"><meta property="og:description" content="Parallelisation of a simple loop #  As usual, we&rsquo;ll be running these exercises on Hamilton or COSMA, so remind yourself of how to log in and transfer code if you need to.
Obtaining the code #  The code for this exercise is in the code/add_numbers subdirectory of the repository.
We&rsquo;ll be working in the openmp subdirectory.
Working from the repository If you have committed your changes on branches for the previous exercises, just checkout the main branch again and create a new branch for this exercise."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/exercises/openmp-loop/"><meta property="article:modified_time" content="2022-04-07T18:13:40+01:00"><title>OpenMP: parallel loops | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/ class=active>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>OpenMP: parallel loops</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#obtaining-the-code>Obtaining the code</a></li><li><a href=#parallelising-the-loop>Parallelising the loop</a><ul><li><a href=#different-schedules>Different schedules</a></li></ul></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=parallelisation-of-a-simple-loop>Parallelisation of a simple loop
<a class=anchor href=#parallelisation-of-a-simple-loop>#</a></h1><p>As usual, we&rsquo;ll be running these exercises on Hamilton or COSMA, so
remind yourself of how to log in and transfer code <a href=https://teaching.wence.uk/phys52015/setup/hamilton-quickstart/>if you need
to</a>.</p><h2 id=obtaining-the-code>Obtaining the code
<a class=anchor href=#obtaining-the-code>#</a></h2><p>The code for this exercise is in the <code>code/add_numbers</code> subdirectory
of the <a href=https://github.com/wenceorg/phys52015>repository</a>.</p><p>We&rsquo;ll be working in the <code>openmp</code> subdirectory.</p><details><summary>Working from the repository</summary><div class=markdown-inner>If you have committed your changes on branches for the previous
exercises, just checkout the <code>main</code> branch again and create a new
branch for this exercise.</div></details><h2 id=parallelising-the-loop>Parallelising the loop
<a class=anchor href=#parallelising-the-loop>#</a></h2><blockquote class=exercise><h3>Exercise</h3><p>Compile and run the code with OpenMP enabled.</p><p>Try running with different numbers of threads. Does the runtime
change?</p><p>You should use a reasonably large value for <code>N</code>.</p></blockquote><p>Check the <code>add_numbers</code> routine in <code>add_numbers.c</code>. Annotate it with
appropriate OpenMP pragmas to parallelise the loop.</p><blockquote class=question><h3>Question</h3><span><p>Does the code now have different runtimes when using different numbers
of threads?</p><details><summary>Solution</summary><div class=markdown-inner><p>This code can be parallelised using a simple parallel for.</p><p>In <code>add_numbers.c</code> we annotate the for loop with</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#75715e>#pragma omp parallel for default(none) shared(n_numbers, numbers) reduction(+:result) schedule(static)
</span></code></pre></div><p>If I do this, I see that the code now takes less time with fewer
threads.</p></div></details></span></blockquote><h3 id=different-schedules>Different schedules
<a class=anchor href=#different-schedules>#</a></h3><p>Experiment with different <a href=https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/#loop-schedules>loop schedules</a>. Which work best? Which work
worst?</p><blockquote class=exercise><h3>Exercise</h3><p>Produce a <a href=https://teaching.wence.uk/phys52015/notes/theory/scaling-laws/#amdahl>strong scaling</a> plot
for the computation as a function of the number of threads using the
different schedules you investigated.</p><p>Don&rsquo;t forget to do this on a <a href=https://teaching.wence.uk/phys52015/setup/hamilton-quickstart/#running-code>compute node</a> (submit a job script
with <code>sbatch</code>) to avoid timing variability.</p><p>What do you observe?</p><details><summary>Solution</summary><div class=markdown-inner><p>This is what I get for some different schedules when computing on a
vector of one million numbers, I did not run multiple times to avoid
timing variability.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/auto/add-numbers-scaling.svg alt="Strong scaling of OpenMP parallelisation of add_numbers with different schedule choices."><figcaption><p>Strong scaling of OpenMP parallelisation of add_numbers with different schedule choices.</p></figcaption></figure></div></details></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/exercises/openmp-loop.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#obtaining-the-code>Obtaining the code</a></li><li><a href=#parallelising-the-loop>Parallelising the loop</a><ul><li><a href=#different-schedules>Different schedules</a></li></ul></li></ul></nav></aside></main></body></html>