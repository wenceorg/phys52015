<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Simple MPI parallelism #  In this exercise we&rsquo;re going to compute an approximation to the value of π using a simple Monte Carlo method. We do this by noticing that if we randomly throw darts at a square, the fraction of the time they will fall within the incircle approaches π.
Consider a square with side-length \(2r\) and an inscribed circle with radius \(r\).
 Square with inscribed circle"><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="MPI: Calculating π"><meta property="og:description" content="Simple MPI parallelism #  In this exercise we&rsquo;re going to compute an approximation to the value of π using a simple Monte Carlo method. We do this by noticing that if we randomly throw darts at a square, the fraction of the time they will fall within the incircle approaches π.
Consider a square with side-length \(2r\) and an inscribed circle with radius \(r\).
 Square with inscribed circle"><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/exercises/mpi-pi/"><meta property="article:modified_time" content="2022-04-07T18:13:40+01:00"><title>MPI: Calculating π | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/ class=active>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>MPI: Calculating π</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#obtaining-the-code>Obtaining the code</a></li><li><a href=#convergence>Convergence</a></li><li><a href=#parallelisation-with-mpi>Parallelisation with MPI</a><ul><li><a href=#initialising-mpi>Initialising MPI</a></li><li><a href=#parallelising-the-random-number-generation>Parallelising the random number generation</a></li><li><a href=#dividing-the-work>Dividing the work</a></li><li><a href=#advice-when-writing-mpi-programs>Advice when writing MPI programs</a></li></ul></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=simple-mpi-parallelism>Simple MPI parallelism
<a class=anchor href=#simple-mpi-parallelism>#</a></h1><p>In this exercise we&rsquo;re going to compute an approximation to the value
of π using a simple Monte Carlo method. We do this by noticing that if
we randomly throw darts at a square, the fraction of the time they
will fall within the incircle approaches π.</p><p>Consider a square with side-length \(2r\) and an inscribed circle
with radius \(r\).</p><figure style=width:40%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/square-circle.svg alt="Square with inscribed circle"><figcaption><p>Square with inscribed circle</p></figcaption></figure><p>The ratio of areas is</p><p>$$
\frac{A_\text{circle}}{A_\text{square}} = \frac{\pi r^2}{4 r^2} = \frac{\pi}{4}.
$$</p><p>If we therefore draw \(X\) uniformly at random from the distribution
\( \mathcal{U}(0, r) \times \mathcal{U}(0, r)\), then the
probability that \(X\) is in the circle is</p><p>$$
p_\text{in} = \frac{\pi}{4}.
$$</p><p>We can therefore approximate π by picking \(N_\text{total}\) random
points and counting the number, \(N_\text{circle}\), that fall within the
circle</p><p>$$
\pi_\text{numerical} = 4 \frac{N_\text{circle}}{N_\text{total}}
$$</p><h2 id=obtaining-the-code>Obtaining the code
<a class=anchor href=#obtaining-the-code>#</a></h2><p>The code for this exercise is in the <a href=https://github.com/wenceorg/phys52015>repository</a> in the
<code>code/calculate_pi/</code> subdirectory.</p><details><summary>Working from the repository</summary><div class=markdown-inner>If you have committed your changes on branches for the previous
exercises, just checkout the <code>main</code> branch again and create a new
branch for this exercise.</div></details><p>The code contains two subdirectories. We&rsquo;ll be working in the <code>serial</code>
subdirectory.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>$ <span style=color:#111>cd</span> code/calculate_pi/serial
$ ls
Makefile       calculate_pi.c main.c         proto.h
</code></pre></div><p>Load the relevant Intel compiler modules and then build the code with
<code>make</code>. It can be run with <code>./calc_pi N</code> where <code>N</code> is the total number
of random points.</p><h2 id=convergence>Convergence
<a class=anchor href=#convergence>#</a></h2><blockquote class=exercise><h3>Exercise</h3><p>Run the code for different choices of <code>N</code> and plot the error as a
function of <code>N</code>.</p><p>What relationship do you observe between the accuracy of the
approximate result and <code>N</code>?</p><details><summary>Hint</summary><div class=markdown-inner>You can execute a sequence of commands in your batch script (just make
sure to allocate enough time in the queue for them all), rather than
running each one in its own job.</div></details><details><summary>Solution</summary><div class=markdown-inner><p>I get, <a href=https://en.wikipedia.org/wiki/Monte_Carlo_integration>as
expected</a>,
approximately $\sqrt{N}$ convergence.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/auto/calcpi-convergence.svg alt="Convergence of the Monte-Carlo estimate of $\pi$"><figcaption><p>Convergence of the Monte-Carlo estimate of $\pi$</p></figcaption></figure></div></details></blockquote><h2 id=parallelisation-with-mpi>Parallelisation with MPI
<a class=anchor href=#parallelisation-with-mpi>#</a></h2><p>We&rsquo;re now going to parallelise this computation with MPI. The first
thing to do is to load the appropriate module to gain access to the
MPI compilers.</p><div class=book-tabs><input type=radio class=toggle name=tabs-MPI-modules id=tabs-MPI-modules-0 checked>
<label for=tabs-MPI-modules-0>Hamilton</label><div class="book-tabs-content markdown-inner"><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>$ module load intelmpi/intel/2018.2
</code></pre></div></div><input type=radio class=toggle name=tabs-MPI-modules id=tabs-MPI-modules-1>
<label for=tabs-MPI-modules-1>COSMA</label><div class="book-tabs-content markdown-inner"><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>$ module load intel_mpi/2018
</code></pre></div></div></div><h3 id=initialising-mpi>Initialising MPI
<a class=anchor href=#initialising-mpi>#</a></h3><p>Remember that the first thing every MPI program should do is to
<em>initialise</em> MPI with
<a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node225.htm><code>MPI_Init</code></a>
and the final thing they should do is <em>finalise</em> MPI with
<a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node225.htm><code>MPI_Finalize</code></a>.</p><p>So do that in the <code>main</code> function (in <code>main.c</code>).</p><blockquote class=exercise><h3>Exercise</h3><p>Add code in <code>main</code> to determine the
<a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node155.htm>size</a>
of <code>MPI_COMM_WORLD</code> as well as the
<a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node155.htm>rank</a>
of the current process, and then print the results out on every
process.</p><p>Now compile and then run the code with two processes using <code>mpirun</code>.
Does what you observe make sense?</p><details><summary>Solution</summary><div class=markdown-inner>At this point, your code just runs the same calculations on multiple
processes (with the same random numbers). So you should have seen
multiple prints of the same output.</div></details></blockquote><h3 id=parallelising-the-random-number-generation>Parallelising the random number generation
<a class=anchor href=#parallelising-the-random-number-generation>#</a></h3><p>So far all we&rsquo;ve done is take a serial program and parallelise it by
running the same calculations multiple times. This is not very useful.</p><p>The first thing to do is to ensure that the different processes use
<em>different</em> random numbers. These are generated using the C standard
library&rsquo;s pseudo-random number generator. The initial state is
<em>seeded</em> in <code>calculate_pi</code>.</p><blockquote class=exercise><h3>Exercise</h3><p>Modify the code so that the seed depends on which process is
generating the random numbers.</p><details><summary>Hint</summary><div class=markdown-inner>The <code>rank</code> of a process is a unique identifier.</div></details><p>Run again on two processes, do you now see that the results are
different depending on the process?</p><details><summary>Solution</summary><div class=markdown-inner>If you change the call <code>srand(42)</code> to <code>srand(rank)</code> then different
process will produce different random numbers. Now when running in
parallel you should see (slightly) different results on the different
processes.</div></details></blockquote><details><summary>Note: parallel random numbers</summary><div class=markdown-inner><p>The approach we use here does not produce statistically uncorrelated
random number streams. This does not really matter for our didactic
purposes, it just means that the effective number of Monte Carlo
samples is lower than what we specify.</p><p>If you need truly independent random number streams, then the
different approaches <a href=https://bashtage.github.io/randomgen/parallel.html>described
here</a> give more
information on how to achieve it.</p></div></details><h3 id=dividing-the-work>Dividing the work
<a class=anchor href=#dividing-the-work>#</a></h3><p>Now we have different processes using differents seeds we need to
divide up the work such that we take <code>N</code> samples in total (rather than
<code>N</code> samples on each process).</p><p>Modify the <code>calculate_pi</code> function such that the samples are
(reasonably) evenly divided between all the processes. After this
you&rsquo;re producing a partial result on every process.</p><p>Finally, combine these partial results to produce the full result by
summing the number of points found to be in the circle across all
processes.</p><details><summary>Hint</summary><div class=markdown-inner><p>Remember that you wrote a function in the <a href=https://teaching.wence.uk/phys52015/exercises/mpi-ring/>ring reduction</a> exercise to add up partial values from all the
processes.</p><p>Alternately, you may find the function
<a href=https://rookiehpc.com/mpi/docs/mpi_allreduce.php><code>MPI_Allreduce</code></a> useful.</p></div></details><blockquote class=question><h3>Question</h3><span><p>Test your code running on 1, 2, 4, 8, and 16 cores. Produce a plot of
the runtime as a function of the number of cores.</p><p>What observations can you make?</p><details><summary>Solution</summary><div class=markdown-inner><p>If you did not manage, or you want to compare with a different
implementation, the directory <code>code/calculate_pi/mpi</code> contains a
parallel implementation.</p><p>You should see that the runtime decreases almost linearly with the
number of additional processes.</p><p>When I do this, I observe the following plot</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/auto/calcpi-scaling.svg alt="Scaling of parallel calculation of $\pi$"><figcaption><p>Scaling of parallel calculation of $\pi$</p></figcaption></figure><p>This is sort of expected because there&rsquo;s no communication and the
major bottleneck is how fast the random points can be generated.</p></div></details></span></blockquote><h3 id=advice-when-writing-mpi-programs>Advice when writing MPI programs
<a class=anchor href=#advice-when-writing-mpi-programs>#</a></h3><p>It is good practice when using MPI that every function
which is <em>collective</em> explicitly receives as an argument the
communicator.</p><p>If you find yourself explicitly referring to one of the default
communicators (e.g. <code>MPI_COMM_WORLD</code>) in a function, think about how
to redesign the code so that you pass the communicator as an argument.</p><p>For example, rather than writing something like:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>void</span> <span style=color:#75af00>divide_work</span><span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>N</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#f92672>*</span><span style=color:#111>nlocal</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>size</span><span style=color:#111>,</span> <span style=color:#111>rank</span><span style=color:#111>;</span>
  <span style=color:#111>MPI_Comm_size</span><span style=color:#111>(</span><span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>size</span><span style=color:#111>);</span>
  <span style=color:#111>MPI_Comm_rank</span><span style=color:#111>(</span><span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>rank</span><span style=color:#111>);</span>
  <span style=color:#75715e>/* Hand out evenly sized chunks with the leftover 
</span><span style=color:#75715e>   * (N - (N/size)*size) being evenly split between higher-numbered
</span><span style=color:#75715e>   * processes */</span>
  <span style=color:#f92672>*</span><span style=color:#111>nlocal</span> <span style=color:#f92672>=</span> <span style=color:#111>N</span> <span style=color:#f92672>/</span> <span style=color:#111>size</span> <span style=color:#f92672>+</span> <span style=color:#111>((</span><span style=color:#111>N</span> <span style=color:#f92672>%</span> <span style=color:#111>size</span><span style=color:#111>)</span> <span style=color:#f92672>&gt;</span> <span style=color:#111>rank</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div><p>Instead prefer</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>void</span> <span style=color:#75af00>divide_work</span><span style=color:#111>(</span><span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>N</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#f92672>*</span><span style=color:#111>nlocal</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>size</span><span style=color:#111>,</span> <span style=color:#111>rank</span><span style=color:#111>;</span>
  <span style=color:#111>MPI_Comm_size</span><span style=color:#111>(</span><span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>size</span><span style=color:#111>);</span>
  <span style=color:#111>MPI_Comm_rank</span><span style=color:#111>(</span><span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>rank</span><span style=color:#111>);</span>
  <span style=color:#75715e>/* Hand out evenly sized chunks with the leftover 
</span><span style=color:#75715e>   * (N - (N/size)*size) being evenly split between higher-numbered
</span><span style=color:#75715e>   * processes */</span>
  <span style=color:#f92672>*</span><span style=color:#111>nlocal</span> <span style=color:#f92672>=</span> <span style=color:#111>N</span> <span style=color:#f92672>/</span> <span style=color:#111>size</span> <span style=color:#f92672>+</span> <span style=color:#111>((</span><span style=color:#111>N</span> <span style=color:#f92672>%</span> <span style=color:#111>size</span><span style=color:#111>)</span> <span style=color:#f92672>&gt;</span> <span style=color:#111>rank</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div><p>This way, if your code is changed to use different communicators, it
will work transparently.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/exercises/mpi-pi.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#obtaining-the-code>Obtaining the code</a></li><li><a href=#convergence>Convergence</a></li><li><a href=#parallelisation-with-mpi>Parallelisation with MPI</a><ul><li><a href=#initialising-mpi>Initialising MPI</a></li><li><a href=#parallelising-the-random-number-generation>Parallelising the random number generation</a></li><li><a href=#dividing-the-work>Dividing the work</a></li><li><a href=#advice-when-writing-mpi-programs>Advice when writing MPI programs</a></li></ul></li></ul></nav></aside></main></body></html>