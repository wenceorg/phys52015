<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Dense linear algebra in parallel #  You should submit your work for this coursework via DUO. The submission deadline is 2021/01/11 at 14:00UTC.  Updates 2021/01/05 To avoid OpenBLAS parallelising with threads (and therefore destroying scaling behaviour), you need to do
export OMP_NUM_THREADS=1 export OPENBLAS_NUM_THREADS=1 In your submission scripts (before running the benchmark). Apologies for not noticing this earlier.
Updates 2021/01/04 Textual benchmark output advertised the wrong order of timing data."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Coursework: parallel dense linear algebra"><meta property="og:description" content="Dense linear algebra in parallel #  You should submit your work for this coursework via DUO. The submission deadline is 2021/01/11 at 14:00UTC.  Updates 2021/01/05 To avoid OpenBLAS parallelising with threads (and therefore destroying scaling behaviour), you need to do
export OMP_NUM_THREADS=1 export OPENBLAS_NUM_THREADS=1 In your submission scripts (before running the benchmark). Apologies for not noticing this earlier.
Updates 2021/01/04 Textual benchmark output advertised the wrong order of timing data."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/past-editions/2020-21/coursework/"><meta property="article:modified_time" content="2022-04-07T18:13:40+01:00"><title>Coursework: parallel dense linear algebra | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><a href=/phys52015/coursework/>Coursework: stencils and collectives</a></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/phys52015/past-editions/2020-21/coursework/ class=active>Coursework: parallel dense linear algebra</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Coursework: parallel dense linear algebra</strong>
<label for=toc-control><img src=/phys52015/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#data-layout>Data layout</a></li><li><a href=#skeleton-code>Skeleton code</a><ul><li><a href=#code-details>Code details</a></li></ul></li><li><a href=#task>Task</a><ul><li><a href=#matrix-vector-multiplication>Matrix-vector multiplication</a></li><li><a href=#matrix-matrix-multiplication>Matrix-matrix multiplication</a></li></ul></li><li><a href=#mark-scheme>Mark scheme and submission</a></li><li><a href=#hints>Hints</a><ul><li><a href=#valgrind>Using valgrind for memory correctness</a></li></ul></li></ul></nav></aside></header><article class=markdown><blockquote class="book-hint warning"><span>This course page was updated until March 2022 when I left Durham University.
For future updates, please visit
the <a href=https://durham-phys52015.github.io/>new version
of the course pages</a>.</span></blockquote><h1 id=dense-linear-algebra-in-parallel>Dense linear algebra in parallel
<a class=anchor href=#dense-linear-algebra-in-parallel>#</a></h1><blockquote class="book-hint warning">You should submit your work for this coursework via DUO. The
submission deadline is 2021/01/11 at 14:00UTC.</blockquote><blockquote class="book-hint info"><h3 id=updates-20210105>Updates 2021/01/05</h3><p>To avoid OpenBLAS parallelising with threads (and therefore destroying
scaling behaviour), you need to do</p><pre><code>export OMP_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=1
</code></pre><p>In your submission scripts (before running the benchmark). Apologies
for not noticing this earlier.</p><h3 id=updates-20210104>Updates 2021/01/04</h3><p>Textual benchmark output advertised the wrong order of timing data. It
is <code>Min, Max, Mean, Standard deviation</code>. The JSON output is correct.
Update the <code>bench.c</code> file to correct this error.</p><h3 id=updates-20201211>Updates 2020/12/11</h3><p>Added link to descriptors for written work in <a href=#mark-scheme>mark scheme</a>.</p><h3 id=updates-20201208>Updates 2020/12/08</h3><p>The notation
$$
\left\lfloor \frac{N}{p} \right \rfloor = \frac{N}{p}
$$
means that the floor of the division (i.e. rounding down to the
nearest integer) should be equal to the original division. That is $N$
should be divisible by $p$ with no remainder.</p><p>Changed instances of <code>mpiexec</code> in the examples to <code>mpirun</code> for
consistency with the rest of the notes.</p></blockquote><p>In this coursework you are to write parallel implementations of some
dense linear algebra operations using MPI, and benchmark the scaling
performance. You should not use other forms of parallelism.</p><p>I provide template code that contains input/output routines and
skeleton data structures. There are a number of functions that you
need to provide an implementation for.</p><p>You will implement a (simplified) case of a small number of the
parallel <a href=https://www.netlib.org/blas/index.html>BLAS</a> routines using
MPI.</p><p>In particular, you should implement the matrix-vector product</p><p>$$
y \gets A x
$$</p><p>and two algorithms for the matrix-matrix product</p><p>$$
C \gets A B + C
$$</p><p>For square matrices \(A, B, C \in \mathbb{R}^{N \times N}\) and
vectors \(x, y \in \mathbb{R}^N\).</p><h2 id=data-layout>Data layout
<a class=anchor href=#data-layout>#</a></h2><p>To simplify things, the setup is restricted to square matrices, that
are distributed using a 2D block layout, where each process stores a
contiguous block of the matrix.</p><p>This is shown below for a \(8 \times 8 \) matrix and four processes.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/matrix-block-distribution.svg alt="Blocked data distribution of a matrix over four processes."><figcaption><p>Blocked data distribution of a matrix over four processes.</p></figcaption></figure><p>The process grid must evenly divide the matrix rows and columns, so
every local piece is the same size. That is, we require, for \(p\)
processes and an \( N \times N \) matrix that
$$
\lfloor \sqrt{p}\rfloor = \sqrt{p}
$$
and
$$
\left\lfloor \frac{N}{\sqrt{p}} \right\rfloor = \frac{N}{\sqrt{p}}.
$$
This ensures that all processes have the same local matrix size, which
simplifies some implementation.</p><p>Furthermore, for matrix-vector products, we also require that
$$
\left \lfloor\frac{N}{p}\right\rfloor = \frac{N}{p}.
$$
The code checks these conditions and produces an error message if they
are not satisfied.</p><p>On each process, the local part of
the matrix is stored in <em>row major</em> format.</p><details><summary>Row major indexing</summary><div class=markdown-inner><p>For a local matrix of size \( n \times n \), the \( (i, j) \)
entry can be obtained with</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#111>Aij</span> <span style=color:#f92672>=</span> <span style=color:#111>A</span><span style=color:#f92672>-&gt;</span><span style=color:#111>data</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#f92672>*</span><span style=color:#111>A</span><span style=color:#f92672>-&gt;</span><span style=color:#111>n</span> <span style=color:#f92672>+</span> <span style=color:#111>j</span><span style=color:#111>];</span>
</code></pre></div></div></details><p>Vectors are distributed by allocating a contiguous chunk of entries to
each process. Again, the number of process much evenly divide the
number entries in the vector.</p><p>This is shown below for an 8-element vector and four processes.</p><figure style=width:25%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/vector-block-distribution.svg alt="Blocked data distribution of a vector over four processes."><figcaption><p>Blocked data distribution of a vector over four processes.</p></figcaption></figure><h2 id=skeleton-code>Skeleton code
<a class=anchor href=#skeleton-code>#</a></h2><p>I provide a skeleton code in C which provides datatypes for vectors
and matrices, some viewing facilities (for debugging), and has various
options for benchmarking performance and testing correctness.</p><p>Find the code in the <code>/code/coursework</code> subdirectory of the <a href=https://github.com/wenceorg/phys52015>course
repository</a>.</p><p>You should do your implementation in the <code>solution.c</code> file, which
contains three functions that you need to implement. They are correct
when run in serial, but do not yet work correctly in parallel.</p><blockquote class="book-hint info"><h3 id=required-modules>Required modules</h3><p>Building on Hamilton or COSMA needs a few modules. On your own system
you may need to edit the <code>Makefile</code>.</p><div class=book-tabs><input type=radio class=toggle name=tabs-Modules id=tabs-Modules-0 checked>
<label for=tabs-Modules-0>Hamilton</label><div class="book-tabs-content markdown-inner"><pre><code>$ module load intel/2019.5
$ module load gcc/8.2.0
$ module load openblas
$ module load intelmpi/intel/2019.6
</code></pre></div><input type=radio class=toggle name=tabs-Modules id=tabs-Modules-1>
<label for=tabs-Modules-1>COSMA</label><div class="book-tabs-content markdown-inner"><pre><code>$ module load intel_comp/2020
$ module load intel_mpi/2020
$ module load openblas
</code></pre></div></div></blockquote><blockquote class="book-hint warning"><p>The local matrix-vector and matrix-matrix multiplications are done
with OpenBLAS (hence needing to load the <code>openblas</code> module). On
Hamilton at least this library automatically uses thread-parallelism
unless you tell it not to. This will get in the way of the MPI
parallelism we&rsquo;re using here.</p><p>To turn it off, ensure that you do</p><pre><code>export OMP_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=1
</code></pre><p>In your batch submission scripts.</p></blockquote><p>Having set up the environment by loading the required modules, build
the code with <code>make</code> and run the executable with <code>./main</code>, with no
arguments, it reports usage information.</p><pre><code>$ cd coursework
$ make
$ ./main
Matrix size is a required argument please specify with -N.

Usage:
./main -N N [-a ALGORITHM] [-t MODE] [-f FILE] [-h]
Run benchmarking or checking of matrix-vector or matrix-matrix multiplication.

Options:
 -N N
    Set matrix size (required).

 -a CANNON | SUMMA
    Select algorithm for matrix-matrix multiplication (default SUMMA).

 -t CHECK_MAT_MULT | BENCH_MAT_MULT | CHECK_MAT_MAT_MULT | BENCH_MAT_MAT_MULT
    Select execution mode (default CHECK_MAT_MAT_MULT).
    CHECK_MAT_MULT: check correctness of matrix-vector multiplication.
    BENCH_MAT_MULT: print timing data for matrix-vector multiplication.
    CHECK_MAT_MAT_MULT: check correctness of matrix-matrix multiplication.
    BENCH_MAT_MAT_MULT: print timing data for matrix-matrix multiplication.

 -f FILE
    In benchmarking mode, print timing data to FILE in JSON format.
    WARNING: overwrites output file if it exists.
    Use &quot;-f -&quot; to dump to standard output.

 -h
    Print this help.
</code></pre><p>The executable has a checking mode, which checks that the
implementations of matrix-vector and matrix-matrix multiplication are
correct. Only the serial multiplication is implemented, so it only
reports a passed test when running with one process. It also prints a
warning that nothing is implemented yet.</p><pre><code>$ mpirun -n 1 ./main -t CHECK_MAT_MULT -N 8
[MatMult]: TODO, please implement me.
CheckMatMult succeeded.
</code></pre><pre><code>$ mpirun -n 4 ./main -t CHECK_MAT_MULT -N 8
[MatMult]: TODO, please implement me.
[1] CheckMatMult failed at local index 0, expected 26 got 12
[1] CheckMatMult failed at local index 1, expected 26 got 12
[MatMult]: TODO, please implement me.
[2] CheckMatMult failed at local index 0, expected 66 got 12
[2] CheckMatMult failed at local index 1, expected 66 got 12
[MatMult]: TODO, please implement me.
[3] CheckMatMult failed at local index 0, expected 66 got 8
[3] CheckMatMult failed at local index 1, expected 66 got 8
[MatMult]: TODO, please implement me.
[0] CheckMatMult failed at local index 0, expected 26 got 8
[0] CheckMatMult failed at local index 1, expected 26 got 8
CheckMatMult failed.
</code></pre><p>Now we see an error and reports of where a mismatch was observed.</p><p>In benchmark mode, the matrices and vectors are filled with random
numbers and then the relevant operation is performed. On successful
completion of the benchmark, the program prints a short summary of the
performance. It can also dump in machine-readable json format to a
specified file with <code>-f data.json</code>.</p><blockquote class="book-hint warning">When writing to a file, any existing contents are <strong>overwritten</strong>!</blockquote><blockquote class="book-hint info"><p>You can load json files in Python with something like</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-python data-lang=python><span style=color:#f92672>import</span> <span style=color:#111>json</span>
<span style=color:#00a8c8>with</span> <span style=color:#111>open</span><span style=color:#111>(</span><span style=color:#d88200>&#34;data.json&#34;</span><span style=color:#111>,</span> <span style=color:#d88200>&#34;r&#34;</span><span style=color:#111>)</span> <span style=color:#00a8c8>as</span> <span style=color:#111>f</span><span style=color:#111>:</span>
    <span style=color:#111>data</span> <span style=color:#f92672>=</span> <span style=color:#111>json</span><span style=color:#f92672>.</span><span style=color:#111>load</span><span style=color:#111>(</span><span style=color:#111>f</span><span style=color:#111>)</span>
</code></pre></div></blockquote><p>For example</p><pre><code>$ ./main -t BENCH_MAT_MAT_MULT -a CANNON -N 1000
[MatMatMultCannon]: TODO, please implement me.
Timing data for MatMatMult[CANNON] on 1 processes, matrix size 1000
All data in seconds. Min, Mean, Max, Standard deviation.
0.0186729 0.0186729 0.0186729 0
</code></pre><h3 id=code-details>Code details
<a class=anchor href=#code-details>#</a></h3><p>The data types are defined in <code>mat.h</code> and <code>vec.h</code> for the <code>Mat</code> and
<code>Vec</code> types respectively.</p><p>The <code>Mat</code> data type is a pointer to a struct</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>struct</span> <span style=color:#111>_p_Mat</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>;</span>                <span style=color:#75715e>/* communicator */</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>n</span><span style=color:#111>,</span> <span style=color:#111>N</span><span style=color:#111>;</span>                     <span style=color:#75715e>/* local and global size */</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>np</span><span style=color:#111>;</span>                       <span style=color:#75715e>/* process grid np x np */</span>
  <span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#111>data</span><span style=color:#111>;</span>                 <span style=color:#75715e>/* matrix entries in row-major format */</span>
<span style=color:#111>};</span>
</code></pre></div><p>So if you have a <code>Mat A</code>, its local size is <code>A->n</code>.</p><p>Similarly, the <code>Vec</code> data type is a pointer to a struct</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>struct</span> <span style=color:#111>_p_Vec</span> <span style=color:#111>{</span>
  <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>;</span>                <span style=color:#75715e>/* communicator */</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>n</span><span style=color:#111>,</span> <span style=color:#111>N</span><span style=color:#111>;</span>                     <span style=color:#75715e>/* local and global size */</span>
  <span style=color:#00a8c8>int</span> <span style=color:#111>np</span><span style=color:#111>;</span>                       <span style=color:#75715e>/* number of processes */</span>
  <span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#111>data</span><span style=color:#111>;</span>                 <span style=color:#75715e>/* vector entries */</span>
<span style=color:#111>};</span>
</code></pre></div><p>All routines return an error code (<code>return 0</code> for no error) can be
checked with the <code>CHKERR</code> macro as shown in the template code.</p><p>For debugging purposes, I provide routines</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MatView</span><span style=color:#111>(</span><span style=color:#111>Mat</span> <span style=color:#111>A</span><span style=color:#111>,</span> <span style=color:#111>FILE</span> <span style=color:#f92672>*</span><span style=color:#111>output</span><span style=color:#111>);</span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>VecView</span><span style=color:#111>(</span><span style=color:#111>Vec</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>FILE</span> <span style=color:#f92672>*</span><span style=color:#111>output</span><span style=color:#111>);</span>
</code></pre></div><p>which can be used to print the provided matrix or vector. You can pass
<code>NULL</code> to print to standard output.</p><h2 id=task>Task
<a class=anchor href=#task>#</a></h2><p>You should implement matrix-vector multiplication, and two algorithms
for parallel matrix-matrix multiplication in parallel. You should use
MPI for parallelisation.</p><p>Having checked the correctness of your implementation, you should
perform a weak and strong scaling study of the three routines. Up to
around 256 processes on Hamilton (or COSMA).</p><p>Finally, you should write a short (maximum four pages) report
describing the algorithms as you implemented them and any choices you
made, and discussing your findings from the scaling study.</p><h3 id=matrix-vector-multiplication>Matrix-vector multiplication
<a class=anchor href=#matrix-vector-multiplication>#</a></h3><p>For this routine, you are free to choose your own algorithm (which you
should describe in the report). You should aim for an approach which
is memory scalable, in the sense that the amount of memory needed by
each process is related to \(N/P\) rather than \(N\) (for an \(N
\times N \) matrix on \( P \) processes).</p><h3 id=matrix-matrix-multiplication>Matrix-matrix multiplication
<a class=anchor href=#matrix-matrix-multiplication>#</a></h3><p>For this routine, you should implement two approaches (which you
should describe in the report):</p><ol><li><a href=https://en.wikipedia.org/wiki/Cannon%27s_algorithm>Cannon&rsquo;s
algorithm</a>,
which is <a href=https://people.eecs.berkeley.edu/~demmel/cs267/lecture11/lecture11.html#link_5>described
nicely</a>
by Jim Demmel.</li><li><a href=http://netlib.org/lapack/lawnspdf/lawn96.pdf>SUMMA</a>, which is a
more general method (although we do not implement the full
generality here). David Bindel has a nice description in <a href=https://www.cs.cornell.edu/courses/cs5220/2017fa/lec/2017-10-19-slides.pdf>these
slides</a>
[pages 2-8]. Since we are restricting ourselves to a square grid of
processes, the data transfer steps can always operate on the full
local matrix (rather than the extra blocking as described in the
SUMMA paper).</li></ol><h2 id=mark-scheme>Mark scheme and submission
<a class=anchor href=#mark-scheme>#</a></h2><p>You should submit, via DUO, a <strong>PDF</strong> of your report (max 4 pages),
and a zip-file of your implementation (<code>solution.c</code>).</p><p>I will build and test your code using the coursework skeleton code, so
you should make sure that your code compiles and runs correctly with
an <em>unmodified</em> version of the skeleton.</p><table><thead><tr><th align=right>Artifact</th><th align=left>Descriptor</th><th>Marks</th></tr></thead><tbody><tr><td align=right>Code</td><td align=left>Correct implementation of MatMult</td><td>10</td></tr><tr><td align=right>Code</td><td align=left>Correct implementation of CANNON</td><td>15</td></tr><tr><td align=right>Code</td><td align=left>Correct implementation of SUMMA</td><td>15</td></tr><tr><td align=right>Code</td><td align=left>Code compiles with no warnings and does not leak memory</td><td>15</td></tr><tr><td align=right>Report</td><td align=left>Description of algorithms</td><td>25</td></tr><tr><td align=right>Report</td><td align=left>Analysis and presentation of benchmarking data</td><td>25</td></tr></tbody></table><p>The report will be marked with reference to the descriptors for
written work (non-dissertation) found on
<a href="https://duo.dur.ac.uk/webapps/blackboard/content/listContent.jsp?course_id=_91410_1&content_id=_5831035_1">DUO</a>,
see <em>MSc in Scientific Computing and Data Analysis Programme →
Assessments and Procedures → Marking criteria for written work
(non-dissertation)</em>.</p><p>You can check memory leaks with <a href=https://www.valgrind.org>Valgrind</a>.
I provide some instructions <a href=#valgrind>below</a>.</p><h2 id=hints>Hints
<a class=anchor href=#hints>#</a></h2><p>I find it very helpful to draw things out for a small matrix and small
number of processes. Then I can see what is going on, and figure out
how to generalise things.</p><p>For debugging purposes, feel free to add <code>printf</code> and other statements
to your code, but your final submission should not contain any
<code>printf</code> output.</p><h3 id=valgrind>Using valgrind for memory correctness
<a class=anchor href=#valgrind>#</a></h3><p><a href=https://www.valgrind.org>Valgrind</a> is an excellent memory debugger.
On Hamilton, you can use it like this:</p><pre><code>module load valgrind
</code></pre><p>You then run your program with</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>$ valgrind --leak-check<span style=color:#f92672>=</span>full ./main ANY ARGUMENTS HERE
</code></pre></div><p>When running in parallel you should run one valgrind process per MPI
task. In that case, it is useful to send the output to a log file per
process.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>$ mpirun -n <span style=color:#ae81ff>4</span> valgrind --log-file<span style=color:#f92672>=</span><span style=color:#d88200>&#34;log-file.%p&#34;</span> --leak-check<span style=color:#f92672>=</span>full ./main ANY ARGUMENTS HERE
</code></pre></div><p>Unfortunately, the Intel MPI library on Hamilton has a number of
memory leaks. I therefore recommend the following recipe for checking
memory leaks. This is what I will do to check things.</p><blockquote class="book-hint warning"><strong>DO NOT</strong> use the MPICH module for any performance testing, since it
does not use the high-speed interconnect!</blockquote><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>$ module load intel/2019.5
$ module load mpich2
$ module load valgrind
$ module load gcc/8.2.0
$ module load openblas
$ make clean
$ make all
$ mpirun -n <span style=color:#ae81ff>4</span> valgrind --log-file<span style=color:#f92672>=</span><span style=color:#d88200>&#34;valgrind-log.%p&#34;</span> --leak-check<span style=color:#f92672>=</span>full --suppressions<span style=color:#f92672>=</span>valgrind.supp ./main ANY ARGUMENTS HERE
</code></pre></div><p>Where the file <code>valgrind.supp</code> is included in the coursework tarball.</p><p>A successful run will look something like</p><pre><code>$ mpirun -n 1 valgrind --leak-check=full --suppressions=valgrind.supp ./main -t CHECK_MAT_MULT -N 8
==28983== Memcheck, a memory error detector
==28983== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==28983== Using Valgrind-3.14.0 and LibVEX; rerun with -h for copyright info
==28983== Command: ./main -t CHECK_MAT_MULT -N 8
==28983== 
CheckMatMult succeeded.
==28983== 
==28983== HEAP SUMMARY:
==28983==     in use at exit: 0 bytes in 0 blocks
==28983==   total heap usage: 241 allocs, 241 frees, 4,310,004 bytes allocated
==28983== 
==28983== All heap blocks were freed -- no leaks are possible
==28983== 
==28983== For counts of detected and suppressed errors, rerun with: -v
==28983== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 2 from 2)
</code></pre><p>Whereas if we failed to deallocate some memory, we see something like</p><pre><code>==30038== Memcheck, a memory error detector
==30038== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==30038== Using Valgrind-3.14.0 and LibVEX; rerun with -h for copyright info
==30038== Command: ./main -t CHECK_MAT_MULT -N 8
==30038== 
CheckMatMult succeeded.
==30038== 
==30038== HEAP SUMMARY:
==30038==     in use at exit: 64 bytes in 1 blocks
==30038==   total heap usage: 241 allocs, 240 frees, 4,310,004 bytes allocated
==30038== 
==30038== 64 bytes in 1 blocks are definitely lost in loss record 1 of 1
==30038==    at 0x4C29E83: malloc (vg_replace_malloc.c:299)
==30038==    by 0x406474: MatMult (solution.c:23)
==30038==    by 0x404F88: CheckMatMult (check.c:24)
==30038==    by 0x403AF9: main (main.c:148)
==30038== 
==30038== LEAK SUMMARY:
==30038==    definitely lost: 64 bytes in 1 blocks
==30038==    indirectly lost: 0 bytes in 0 blocks
==30038==      possibly lost: 0 bytes in 0 blocks
==30038==    still reachable: 0 bytes in 0 blocks
==30038==         suppressed: 0 bytes in 0 blocks
==30038== 
==30038== For counts of detected and suppressed errors, rerun with: -v
==30038== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 2 from 2)
</code></pre><p>This tells us that memory allocated on line 23 of <code>solution.c</code> was
never <code>free</code>d.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/d488ecdf20e1aba21512c0e81135546a0c9a77d7 title="Last modified by Lawrence Mitchell | April 7, 2022" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>April 7, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/past-editions/2020-21/coursework.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#data-layout>Data layout</a></li><li><a href=#skeleton-code>Skeleton code</a><ul><li><a href=#code-details>Code details</a></li></ul></li><li><a href=#task>Task</a><ul><li><a href=#matrix-vector-multiplication>Matrix-vector multiplication</a></li><li><a href=#matrix-matrix-multiplication>Matrix-matrix multiplication</a></li></ul></li><li><a href=#mark-scheme>Mark scheme and submission</a></li><li><a href=#hints>Hints</a><ul><li><a href=#valgrind>Using valgrind for memory correctness</a></li></ul></li></ul></nav></aside></main></body></html>