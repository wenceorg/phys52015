<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction on PHYS52015 – Introduction to HPC</title><link>https://teaching.wence.uk/phys52015/</link><description>Recent content in Introduction on PHYS52015 – Introduction to HPC</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://teaching.wence.uk/phys52015/index.xml" rel="self" type="application/rss+xml"/><item><title>Coursework: parallel dense linear algebra</title><link>https://teaching.wence.uk/phys52015/past-editions/2020-21/coursework/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/past-editions/2020-21/coursework/</guid><description>Dense linear algebra in parallel # You should submit your work for this coursework via DUO. The submission deadline is 2021/01/11 at 14:00UTC. Updates 2021/01/05 To avoid OpenBLAS parallelising with threads (and therefore destroying scaling behaviour), you need to do
export OMP_NUM_THREADS=1 export OPENBLAS_NUM_THREADS=1 In your submission scripts (before running the benchmark). Apologies for not noticing this earlier.
Updates 2021/01/04 Textual benchmark output advertised the wrong order of timing data.</description></item><item><title>Introduction and motivation</title><link>https://teaching.wence.uk/phys52015/notes/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/introduction/</guid><description>Introduction # This course provides a brief introduction to parallel computing. It focuses on those paradigms that are prevalent in scientific computing in academia. There are other parallel programming models which we will mention in passing where appropriate.
Task I think I have arranged for you to have a Hamilton account. Follow the instructions in that guide to log in and check that you can access Hamilton.
If you can&amp;rsquo;t access Hamilton, get in touch with me!</description></item><item><title>Parallel Hello World</title><link>https://teaching.wence.uk/phys52015/exercises/hello/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/hello/</guid><description>Hello, World! # As with every programming course, the first thing we will do is compile and run a &amp;ldquo;Hello world&amp;rdquo; program. Actually we&amp;rsquo;ll do three. The goal of this is to familiarise you with the module system on Hamilton, as well as how to compile code. So take a look at the quickstart guide if you haven&amp;rsquo;t already.
A serial version # Log in to Hamilton/COSMA load the relevant compiler modules</description></item><item><title>Point-to-point messaging in MPI</title><link>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/</guid><description>Pairwise message exchange # The simplest form of communication in MPI is a pairwise exchange of a message between two processes.
In MPI, communication via messages is two-sided1. That is, for every message one process sends, there must be a matching receive call by another process.
Cartoon of sending a message between two processes
We need to fill in some details
How will we describe &amp;ldquo;data&amp;rdquo; How will we identify processes How will the receiver know which message to put where?</description></item><item><title>What is OpenMP?</title><link>https://teaching.wence.uk/phys52015/notes/openmp/intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/openmp/intro/</guid><description>What is OpenMP # OpenMP is a standardised API for programming shared memory computers (and more recently GPUs) using threading as the programming paradigm. It supports both data-parallel shared memory programming (typically for parallelising loops) and task parallelism. We&amp;rsquo;ll see some examples later.
In recent years, it has also gained support for some vector-based parallelism.
Using OpenMP # OpenMP is implemented as a set of extensions for C, C++, and Fortran.</description></item><item><title>Loop parallelism</title><link>https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/</guid><description>OpenMP loop parallelism # With a parallel region and identification of individual threads, we can actually parallelise loops &amp;ldquo;by hand&amp;rdquo;.
Suppose we wish to divide a loop approximately equally between all the threads, by assigning consecutive blocks of the loop iterations to consecutive threads.
Distribution of 16 loop iterations across five threads.
Notice here that the number of iterations is not evenly divisible by the number of threads, so we&amp;rsquo;ve assigned one extra iteration to thread0.</description></item><item><title>Non-blocking point-to-point messaging</title><link>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point-nb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point-nb/</guid><description>Non-blocking messages # As well as the blocking point to point messaging we saw last time, MPI also offers non-blocking versions.
These functions all return immediately, and provide a &amp;ldquo;request&amp;rdquo; object that we can then either wait for completion with or inspect to check if the message has been sent/received.
The function signatures for MPI_Isend and MPI_Irecv are:
int MPI_Isend(const void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); int MPI_Irecv(void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); Notice how the send gets an extra output argument (the request), and the receive loses the MPI_Status output argument and gains a request output argument.</description></item><item><title>Parallel scaling laws</title><link>https://teaching.wence.uk/phys52015/notes/theory/scaling-laws/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/theory/scaling-laws/</guid><description>Scaling laws (models) # Suppose we have a simulation that has been parallelised. What should we expect of its performance when we run it in parallel? Answering this question will allow us to determine an appropriate level of parallelism to use when running the simulation. It can also help us determine how much effort to put into parallelising a serial code, or improving an existing parallel code.
Some simple examples1 # The type of parallelism we will cover in this course is that where we, as programmers, are explicitly in control of what is going on.</description></item><item><title>Collectives</title><link>https://teaching.wence.uk/phys52015/notes/mpi/collectives/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/collectives/</guid><description>Collective communication # Point-to-point messages are sufficient to write all the parallel algorithms we might want. However, they might not necessarily be the most efficient.
As motivation, let&amp;rsquo;s think about the time we would expect the ring reduction you implemented to take as a function of the number of processes.
Recall from the ping-pong exercise that our model for the length of time it takes to send a message with $B$ bytes is</description></item><item><title>Coursework: stencils and collectives</title><link>https://teaching.wence.uk/phys52015/coursework/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/coursework/</guid><description>Overview # The submission deadline for this work is 2022-01-10 at 14:00UTC. This coursework has two parts. In the first part, you will design and implement OpenMP parallelisation of a simple stencil code. In the second part, you will develop a tree-based implementation of MPI_Allreduce, and then perform some benchmarking of your implementation against both the vendor-provided version, and the non-scalable version we saw in the exercises.
You will be assessed by submitting both your code, and brief writeups of your findings through GitHub classroom.</description></item><item><title>Parallelism in hardware: an overview</title><link>https://teaching.wence.uk/phys52015/notes/theory/hardware-parallelism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/theory/hardware-parallelism/</guid><description>Supercomputer architecture # When thinking about the implementation of parallel programming, it is helpful to have at least a high-level idea of what kinds of parallelism are available in the hardware. The different levels of parallelism available in the hardware then map onto (sometimes) different programming models.
Distributed memory parallelism # Modern supercomputers are all massively parallel, distributed memory, systems. That is, they consist of many processors linked by some network, but do not share a single memory system.</description></item><item><title>Advanced topics</title><link>https://teaching.wence.uk/phys52015/notes/mpi/advanced/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/advanced/</guid><description>Some pointers to more advanced features of MPI # Communicator manipulation # We saw that we can distinguish point-to-point messages by providing different tags, but that there was no such facility for collective operations. Moreover, a collective operation (by definition) involves all the processes in a communicator.
This raises two questions:
How can we have multiple collective operations without them interfering with each other; What if we want a collective operation, but using only a subset of the processes (e.</description></item><item><title>OpenMP: parallel loops</title><link>https://teaching.wence.uk/phys52015/exercises/openmp-loop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/openmp-loop/</guid><description>Parallelisation of a simple loop # As usual, we&amp;rsquo;ll be running these exercises on Hamilton or COSMA, so remind yourself of how to log in and transfer code if you need to.
Obtaining the code # The code for this exercise is in the code/add_numbers subdirectory of the repository.
We&amp;rsquo;ll be working in the openmp subdirectory.
Working from the repository If you have committed your changes on branches for the previous exercises, just checkout the main branch again and create a new branch for this exercise.</description></item><item><title>Parallel patterns</title><link>https://teaching.wence.uk/phys52015/notes/theory/concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/theory/concepts/</guid><description>Overview of parallel patterns # So far, we&amp;rsquo;ve seen that to run large simulations, and exploit all of the hardware available to us on supercomputers (but even on laptops and phones), we will need to use parallelism of some kind.
We&amp;rsquo;ve also looked at the levels of parallelism exposed by modern hardware, and noted that there are effectively three levels.
Now we&amp;rsquo;re going to look at the types of parallelism (or parallel patterns) that we might encounter in software.</description></item><item><title>Collectives</title><link>https://teaching.wence.uk/phys52015/notes/openmp/collectives/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/openmp/collectives/</guid><description>OpenMP collectives # So far we&amp;rsquo;ve seen how we can create thread teams using #pragma omp parallel and distribute work in loops between members of the team by using #pragma omp for.
Now we&amp;rsquo;ll look at what we need to do if we need to communicate between threads.
Reductions # Remember that the OpenMP programming model allows communication between threads by using shared memory. If some piece of memory is shared in a parallel region then every thread in the team can both read and write to it.</description></item><item><title>Further resources</title><link>https://teaching.wence.uk/phys52015/resources/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/resources/</guid><description>Other reading/resources # There are many excellent textbooks and tutorials on parallel programming. Here are some I like, with brief commentary.
Victor Eijkhout, Introduction to High Performance Computing. This is a gentle, and quite comprehensive, introduction to parallel computing. The author has a separate book which contains more details on OpenMP and MPI programming models.
Georg Hager &amp;amp; Gerhard Wellein, Introduction to High Performance Computing for Scientists and Engineers, also available through the Durham library.</description></item><item><title>OpenMP: stencils</title><link>https://teaching.wence.uk/phys52015/exercises/openmp-stencil/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/openmp-stencil/</guid><description>Parallelisation of a simple stencil # We&amp;rsquo;ll be running these exercises on Hamilton or COSMA, so remind yourself of how to log in and transfer code if you need to.
Blurring an image # One can blur or smooth the edges of an image by convolving the image with a normalised box kernel. Every output pixel \( g_{k, l} \) is created from the mean of the input image pixel \(f _{k, l}\) and its eight neighbours.</description></item><item><title>Remote editing/development</title><link>https://teaching.wence.uk/phys52015/setup/remote/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/setup/remote/</guid><description>Remote development and managing changes # Remote editing # Editing in a terminal is somewhat painful, and laboriously copying files back and forth between your local machine and Hamilton is error-prone.
A better approach is to use a combination of version control and remote editing. I provide brief instructions for VS Code which is a popular editor, and very brief instructions for Emacs.
VS Code # VS Code has an extension for remote editing which allows you to edit files on remote systems as if you&amp;rsquo;re running locally.</description></item><item><title>OpenMP: synchronisation</title><link>https://teaching.wence.uk/phys52015/exercises/openmp-reduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/openmp-reduction/</guid><description>Avoiding data-races in updates to shared memory # In this exercise, we&amp;rsquo;ll use the synchronisation constructs we encountered when looking at OpenMP collectives to implement different approaches to combining the partial sums in a reduction.
We&amp;rsquo;ll then also benchmark the performance of the different approaches to see if there are any differences
Template code and benchmarking # As usual, the starter code lives in the repository, in the code/openmp-snippets subdirectory.</description></item><item><title>MPI: messages round a ring</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-ring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-ring/</guid><description>Sending messages around a ring # In this exercise we&amp;rsquo;ll write a simple form of global reduction. We will set up the processes in a ring (so each process has a left and right neighbour) and each process should initialise a buffer to its rank.
To compute a global summation a simple method is to rotate each piece of data all the way round the ring: at each step, a process receives from the left and sends to the right.</description></item><item><title>MPI: Calculating π</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-pi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-pi/</guid><description>Simple MPI parallelism # In this exercise we&amp;rsquo;re going to compute an approximation to the value of π using a simple Monte Carlo method. We do this by noticing that if we randomly throw darts at a square, the fraction of the time they will fall within the incircle approaches π.
Consider a square with side-length \(2r\) and an inscribed circle with radius \(r\).
Square with inscribed circle</description></item><item><title>MPI: ping-pong latency</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-ping-pong/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-ping-pong/</guid><description>Measuring point-to-point message latency with ping-pong # In this exercise we will write a simple code that does a message ping-pong: sending a message back and forth between two processes.
We can use this to measure both the latency and bandwidth of the network on our supercomputer. Which are both important measurements when we&amp;rsquo;re looking at potential parallel performance: they help us to decide if our code is running slowly because of our bad choices, or limitations in the hardware.</description></item><item><title>Acknowledgements</title><link>https://teaching.wence.uk/phys52015/acknowledgements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/acknowledgements/</guid><description>Acknowledgements # Some of the exercises and examples, where noted, are adapted from Victor Eijkhout&amp;rsquo;s introductory high performance computing textbook.
Some of the material is adapted from a previous version of the course taught by Tobias Weinzierl at Durham.</description></item><item><title>MPI: simple collectives</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-collectives/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-collectives/</guid><description>Manipulating matrices with collectives # In this exercise, we&amp;rsquo;ll look at some simple manipulations of matrices with collective operations.
We&amp;rsquo;ll do this with square matrices, where the number of rows (and columns) is equal to the number of processes.
We&amp;rsquo;ll implement three routines:
printing a matrix by gathering it to a single process and printing there; transposing a matrix; multiplying a matrix against a distributed vector. For a data distribution, we will use a one-dimensional distribution where each process holds one row.</description></item><item><title>MPI: domain decomposition and halo exchanges</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-stencil/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-stencil/</guid><description>Data parallel stencil computations # In this exercise we&amp;rsquo;re going to explore an MPI-parallelisation of a simple image processing problem.
In particular, we are going to reconstruct an image from its edges.
Introduction and background # A particularly simple way of detecting the edges in an image is to convolve it with a Laplacian kernel. That is, given some image $I$, we can obtain its edges with
$$ E = \nabla^2 I.</description></item><item><title>Hamilton access &amp; quickstart</title><link>https://teaching.wence.uk/phys52015/setup/hamilton-quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/setup/hamilton-quickstart/</guid><description>Accessing Durham Supercomputing facilities # Most of the exercises in this course will require that you use one of Durham&amp;rsquo;s supercomputing systems. This will either be Hamilton, or (for some Physics students) COSMA. If you&amp;rsquo;re signed up on the course, you should have been given access to Hamilton semi-automatically (I signed everyone up).
Access to Hamilton # For many of the exercises in the course, we will be using the Hamilton supercomputer.</description></item><item><title>Local setup</title><link>https://teaching.wence.uk/phys52015/setup/byod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/setup/byod/</guid><description>Running the exercises on your own hardware # Although you don&amp;rsquo;t need to, you might find it useful or convenient to run the exercises on your own computer. To do so, you&amp;rsquo;ll need to obtain appropriate compilers and MPI implementations.
For more general information on getting a development toolchain setup, the Faculty of Natural Sciences at Imperial College have prepared some useful guides
MacOS # Open source tools # I recommend homebrew for installation of packages on MacOS.</description></item><item><title>ssh configuration</title><link>https://teaching.wence.uk/phys52015/setup/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/setup/configuration/</guid><description>ssh tips &amp;amp; tricks # Setting up simpler logins # It can be tedious to remember to type long login commands every time when logging in via ssh to hamilton. I therefore recommend that you set up an ssh config file.
Additionally, you might also want to set up ssh keys for passwordless login.
The ssh-config configuration file # Mac/Linux When you run ssh it reads a configuration file at $HOME/.</description></item><item><title>Unix resources</title><link>https://teaching.wence.uk/phys52015/setup/unix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/setup/unix/</guid><description>Using Unix-like systems for computational science # This course presupposes some level of familiarity with commandline interfaces. In case you need a quick refresher, I recommend the material produced by the Software Carpentry project.
They have a number of useful lessons and materials providing introductory training on how to do things like use the Unix shell, version control with git, and some introductory programming and plotting in Python. They are good place to start for a gentle introduction to these topics.</description></item></channel></rss>